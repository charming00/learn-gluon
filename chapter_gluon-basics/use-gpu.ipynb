{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU计算\n",
    "\n",
    "目前为止，我们一直在使用CPU计算。的确，绝大部分的计算设备都有CPU。然而，CPU的设计目的是处理通用的计算。对于复杂的神经网络和大规模的数据来说，使用单块CPU计算可能不够高效。\n",
    "\n",
    "本节中，我们将介绍如何使用单块Nvidia GPU来计算。\n",
    "\n",
    "首先，需要确保至少有一块Nvidia显卡已经安装好了。然后，下载安装显卡驱动和[CUDA](https://developer.nvidia.com/cuda-downloads)（推荐下载8.0，CUDA自带了驱动）。Windows用户还需要设一下PATH：\n",
    "\n",
    "> `set PATH=C:\\Program Files\\NVIDIA Corporation\\NVSMI;%PATH%`\n",
    "\n",
    "这些准备工作都完成后，下面就可以通过`nvidia-smi`来查看显卡信息了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 24 17:00:07 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:0B:00.0  On |                  N/A |\r\n",
      "| 47%   35C    P5    23W / 250W |  11146MiB / 11171MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1330      G   /usr/lib/xorg/Xorg                           192MiB |\r\n",
      "|    0      2406      G   compiz                                       102MiB |\r\n",
      "|    0      6311      C   ...arming/miniconda3/envs/gluon/bin/python   391MiB |\r\n",
      "|    0     11707      C   ...arming/miniconda3/envs/gluon/bin/python  2699MiB |\r\n",
      "|    0     16137      C   ...arming/miniconda3/envs/gluon/bin/python  7733MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们需要确认安装了MXNet的GPU版本。如果装了MXNet的CPU版本，我们需要卸载它。例如\n",
    "\n",
    "> `pip uninstall mxnet`\n",
    "\n",
    "为了使用MXNet的GPU版本，我们需要根据CUDA版本安装`mxnet-cu75`、`mxnet-cu80`或者`mxnet-cu90`。例如\n",
    "\n",
    "> `pip install --pre mxnet-cu80`\n",
    "\n",
    "## 处理器\n",
    "\n",
    "使用MXNet的GPU版本和之前没什么不同。下面导入本节中实验所需的包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon, nd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MXNet使用`context`来指定用来存储和计算的设备。默认情况下，MXNet会将数据开在主内存，然后利用CPU来计算。在MXNet中，CPU和GPU可分别由`mx.cpu()`和`mx.gpu()`来表示。需要注意的是，`mx.cpu()`表示所有的物理CPU和内存。这意味着计算上会尽量使用所有的CPU核。但`mx.gpu()`只代表一块显卡和相应的显卡内存。如果有多块GPU，我们用`mx.gpu(i)`来表示第$i$块GPU（$i$从0开始）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[cpu(0), gpu(0), gpu(1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[mx.cpu(), mx.gpu(), mx.gpu(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NDArray的GPU计算\n",
    "\n",
    "每个NDArray都有一个`context`属性来表示它存在哪个设备上。默认情况下，NDArray存在CPU上。因此，之前我们每次打印NDArray的时候都会看到`@cpu(0)`这个标识。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  \n",
      "[ 1.  2.  3.]\n",
      "<NDArray 3 @cpu(0)> \n",
      "context of x:  cpu(0)\n"
     ]
    }
   ],
   "source": [
    "x = nd.array([1,2,3])\n",
    "print('x: ', x, '\\ncontext of x: ', x.context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU上的存储\n",
    "\n",
    "我们可以在创建NDArray的时候通过`ctx`指定存储设备。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "ename": "MXNetError",
     "evalue": "[08:55:55] src/engine/threaded_engine.cc:320: Check failed: exec_ctx.dev_id < device_count_ (1 vs. 1) Invalid GPU Id: 1, Valid device id should be less than device_count: 1\n\nStack trace returned 10 entries:\n[bt] (0) /home/charming/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x32173a) [0x7f058c40873a]\n[bt] (1) /home/charming/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x321d61) [0x7f058c408d61]\n[bt] (2) /home/charming/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x24f34dc) [0x7f058e5da4dc]\n[bt] (3) /home/charming/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x24f4868) [0x7f058e5db868]\n[bt] (4) /home/charming/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x29848d5) [0x7f058ea6b8d5]\n[bt] (5) /home/charming/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2984ac7) [0x7f058ea6bac7]\n[bt] (6) /home/charming/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x298d0ff) [0x7f058ea740ff]\n[bt] (7) /home/charming/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x298d7db) [0x7f058ea747db]\n[bt] (8) /home/charming/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2576afd) [0x7f058e65dafd]\n[bt] (9) /home/charming/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x257767a) [0x7f058e65e67a]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-adcfeaa3fcdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 假设至少存在2块GPU。如果不存在则会报错。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nb: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nc: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/ndarray/random.py\u001b[0m in \u001b[0;36muniform\u001b[0;34m(low, high, shape, dtype, ctx, out, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \"\"\"\n\u001b[1;32m    100\u001b[0m     return _random_helper(_internal._random_uniform, _internal._sample_uniform,\n\u001b[0;32m--> 101\u001b[0;31m                           [low, high], shape, dtype, ctx, out, kwargs)\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/ndarray/random.py\u001b[0m in \u001b[0;36m_random_helper\u001b[0;34m(random, sampler, params, shape, dtype, ctx, out, kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;34m\"Distribution parameters must all have the same type, but got \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;34m\"both %s and %s.\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     raise ValueError(\"Distribution parameters must be either NDArray or numbers, \"\n",
      "\u001b[0;32m~/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/ndarray/register.py\u001b[0m in \u001b[0;36m_random_uniform\u001b[0;34m(low, high, shape, ctx, dtype, out, name, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/_ctypes/ndarray.py\u001b[0m in \u001b[0;36m_imperative_invoke\u001b[0;34m(handle, ndargs, keys, vals, out)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         ctypes.byref(out_stypes)))\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moriginal_output\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \"\"\"\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMXNetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMXGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMXNetError\u001b[0m: [08:55:55] src/engine/threaded_engine.cc:320: Check failed: exec_ctx.dev_id < device_count_ (1 vs. 1) Invalid GPU Id: 1, Valid device id should be less than device_count: 1\n\nStack trace returned 10 entries:\n[bt] (0) /home/charming/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x32173a) [0x7f058c40873a]\n[bt] (1) /home/charming/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x321d61) [0x7f058c408d61]\n[bt] (2) /home/charming/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x24f34dc) [0x7f058e5da4dc]\n[bt] (3) /home/charming/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x24f4868) [0x7f058e5db868]\n[bt] (4) /home/charming/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x29848d5) [0x7f058ea6b8d5]\n[bt] (5) /home/charming/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2984ac7) [0x7f058ea6bac7]\n[bt] (6) /home/charming/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x298d0ff) [0x7f058ea740ff]\n[bt] (7) /home/charming/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x298d7db) [0x7f058ea747db]\n[bt] (8) /home/charming/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2576afd) [0x7f058e65dafd]\n[bt] (9) /home/charming/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x257767a) [0x7f058e65e67a]\n\n"
     ]
    }
   ],
   "source": [
    "a = nd.array([1, 2, 3], ctx=mx.gpu())\n",
    "b = nd.zeros((3, 2), ctx=mx.gpu())\n",
    "# 假设至少存在2块GPU。如果不存在则会报错。\n",
    "c = nd.random.uniform(shape=(2, 3), ctx=mx.gpu(1)) \n",
    "print('a: ', a, '\\nb: ', b, '\\nc: ', c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过`copyto`和`as_in_context`函数在设备之间传输数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  \n",
      "[ 1.  2.  3.]\n",
      "<NDArray 3 @cpu(0)> \n",
      "y:  \n",
      "[ 1.  2.  3.]\n",
      "<NDArray 3 @gpu(0)> \n",
      "z:  \n",
      "[ 1.  2.  3.]\n",
      "<NDArray 3 @gpu(0)>\n"
     ]
    }
   ],
   "source": [
    "y = x.copyto(mx.gpu())\n",
    "z = x.as_in_context(mx.gpu())\n",
    "print('x: ', x, '\\ny: ', y, '\\nz: ', z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要区分的是，如果源变量和目标变量的`context`一致，`as_in_context`使目标变量和源变量共享源变量的内存，而`copyto`总是为目标变量新创建内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  \n",
      "[ 1.  2.  3.]\n",
      "<NDArray 3 @gpu(0)> \n",
      "y_target:  \n",
      "[ 1.  2.  3.]\n",
      "<NDArray 3 @gpu(0)>\n",
      "z:  \n",
      "[ 1.  2.  3.]\n",
      "<NDArray 3 @gpu(0)> \n",
      "z_target:  \n",
      "[ 1.  2.  3.]\n",
      "<NDArray 3 @gpu(0)>\n",
      "y_target and y share memory?  True\n",
      "z_target and z share memory?  False\n"
     ]
    }
   ],
   "source": [
    "y_target = y.as_in_context(mx.gpu())\n",
    "z_target = z.copyto(mx.gpu())\n",
    "print('y: ', y, '\\ny_target: ', y_target)\n",
    "print('z: ', z, '\\nz_target: ', z_target)\n",
    "print('y_target and y share memory? ', y_target is y)\n",
    "print('z_target and z share memory? ', z_target is z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU上的计算\n",
    "\n",
    "MXNet的计算会在数据的`context`上执行。为了使用GPU计算，我们只需要事先将数据放在GPU上面。而计算结果会自动保存在相同的设备上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[  20.08553696  109.19629669  445.23950195]\n",
       "<NDArray 3 @gpu(0)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.exp(z + 2) * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，MXNet要求计算的所有输入数据都在同一个设备上。这个设计的原因是设备之间的数据交互通常比较耗时。因此，MXNet希望用户确切地指明计算的输入数据都在同一个设备上。例如，如果将CPU上的`x`和GPU上的`y`做运算，会出现错误信息。\n",
    "\n",
    "### 其他复制到主内存的操作\n",
    "\n",
    "当我们打印NDArray或将NDArray转换成NumPy格式时，MXNet会自动将数据复制到主内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 1.  2.  3.]\n",
      "<NDArray 3 @gpu(0)>\n",
      "[ 1.  2.  3.]\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(y.asnumpy())\n",
    "print(y.sum().asscalar())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gluon的GPU计算\n",
    "\n",
    "同NDArray类似，Gluon的大部分函数可以通过`ctx`指定设备。下面代码将模型参数初始化在GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = gluon.nn.Sequential()\n",
    "net.add(gluon.nn.Dense(1))\n",
    "net.initialize(ctx=mx.gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当输入是GPU上的NDArray时，Gluon会在相同的GPU上计算结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.00683238]\n",
       " [ 0.00583882]\n",
       " [ 0.01332516]]\n",
       "<NDArray 3x1 @gpu(0)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = nd.random.uniform(shape=[3, 2], ctx=mx.gpu())\n",
    "net(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确认一下模型参数存储在相同的GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.0068339   0.01299825]]\n",
       "<NDArray 1x2 @gpu(0)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 通过`context`，我们可以在不同的设备上存储数据和计算。\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 试试大一点的计算任务，例如大矩阵的乘法，看看CPU和GPU的速度区别。如果是计算量很小的任务呢？\n",
    "* GPU上应如何读写模型参数？\n",
    "\n",
    "## 扫码直达[讨论区](https://discuss.gluon.ai/t/topic/988)\n",
    "\n",
    "![](../img/qr_use-gpu.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
