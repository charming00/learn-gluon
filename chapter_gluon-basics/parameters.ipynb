{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型参数\n",
    "\n",
    "为了引出本节的话题，让我们先构造一个多层感知机。首先，导入本节中实验所需的包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import init, gluon, nd\n",
    "from mxnet.gluon import nn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面定义多层感知机。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.hidden = nn.Dense(4)\n",
    "            self.output = nn.Dense(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(nd.relu(self.hidden(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下面代码，系统抱怨说模型参数没有初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'mlp0_dense0_weight' has not been initialized. Note that you should initialize parameters and create Trainer with Block.collect_params() instead of Block.params because the later does not include Parameters of nested child Blocks"
     ]
    }
   ],
   "source": [
    "x = nd.random.uniform(shape=(3, 5))\n",
    "try:\n",
    "    net = MLP()\n",
    "    net(x)\n",
    "except RuntimeError as err:\n",
    "    sys.stderr.write(str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作如下修改之后，模型便计算成功。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.00212593  0.00365805]\n",
       " [ 0.00161272  0.00441845]\n",
       " [ 0.00204872  0.00352518]]\n",
       "<NDArray 3x2 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里添加的`net.initialize()`对模型参数做了初始化。模型参数是深度学习计算中的重要组成部分。本节中，我们将介绍如何访问、初始化和共享模型参数。\n",
    "\n",
    "## 访问模型参数\n",
    "\n",
    "在Gluon中，模型参数的类型是`Parameter`。下面让我们创建一个名字叫“good_param”、形状为$2 \\times 3$的模型参数。在默认的初始化中，模型参数中的每一个元素是一个在`[-0.07, 0.07]`之间均匀分布的随机数。相应地，该模型参数还有一个形状为$2 \\times 3$的梯度，初始值为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  \n",
      "[[ 0.0421275  -0.00539289  0.00286685]\n",
      " [ 0.03927409  0.02504314 -0.05344158]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "grad:  \n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "name:  good_param\n"
     ]
    }
   ],
   "source": [
    "my_param = gluon.Parameter(\"good_param\", shape=(2, 3))\n",
    "my_param.initialize()\n",
    "print('data: ', my_param.data(), '\\ngrad: ', my_param.grad(),\n",
    "      '\\nname: ', my_param.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，让我们访问本节开头定义的多层感知机`net`中隐藏层`hidden`的模型参数：权重`weight`和偏差`bias`。它们的类型也都是`Parameter`。我们可以看到它们的名字、形状和数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer name:  mlp0_dense0 \n",
      "weight:  Parameter mlp0_dense0_weight (shape=(4, 5), dtype=<class 'numpy.float32'>) \n",
      "bias:  Parameter mlp0_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n"
     ]
    }
   ],
   "source": [
    "w = net.hidden.weight\n",
    "b = net.hidden.bias\n",
    "print('hidden layer name: ', net.hidden.name, '\\nweight: ', w, '\\nbias: ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们同样可以访问这两个参数的值和梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: \n",
      "[[-0.06206018  0.06491279 -0.03182812 -0.01631819 -0.00312688]\n",
      " [ 0.0408415   0.04370362  0.00404529 -0.0028032   0.00952624]\n",
      " [-0.01501013  0.05958354  0.04705103 -0.06005495 -0.02276454]\n",
      " [-0.0578019   0.02074406 -0.06716943 -0.01844618  0.04656678]]\n",
      "<NDArray 4x5 @cpu(0)> \n",
      "weight grad: \n",
      "[[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "<NDArray 4x5 @cpu(0)> \n",
      "bias: \n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)> \n",
      "bias grad: \n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print('weight:', w.data(), '\\nweight grad:', w.grad(), '\\nbias:', b.data(),\n",
    "      '\\nbias grad:', b.grad())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，我们也可以通过`collect_params`来访问Block里的所有参数（包括所有的子Block）。它会返回一个名字到对应`Parameter`的字典。在这个字典中，我们既可以用`[]`（需要指定前缀），又可以用`get()`（不需要指定前缀）来访问模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp0_ (\n",
      "  Parameter mlp0_dense0_weight (shape=(4, 5), dtype=<class 'numpy.float32'>)\n",
      "  Parameter mlp0_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter mlp0_dense1_weight (shape=(2, 4), dtype=<class 'numpy.float32'>)\n",
      "  Parameter mlp0_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
      ")\n",
      "\n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "params = net.collect_params()\n",
    "print(params)\n",
    "print(params['mlp0_dense0_bias'].data())\n",
    "print(params.get('dense0_bias').data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型参数\n",
    "\n",
    "在Gluon中，模型的偏差参数总是默认初始化为0。当我们对整个模型所有参数做初始化时，默认下权重参数的所有元素为[-0.07, 0.07]之间均匀分布的随机数。我们也可以使用其他初始化方法。以下例子使用了均值为0，标准差为0.02的正态分布来随机初始化模型中所有层的权重参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden weight:  \n",
      "[[ 0.01725933 -0.02177767  0.01344385  0.00272668 -0.00392631]\n",
      " [-0.03435376  0.01124353 -0.00622001  0.00689361  0.02062465]\n",
      " [ 0.00675439  0.01104854  0.01147354  0.00579418 -0.04144352]\n",
      " [-0.02262641  0.00582818  0.00072618  0.02628598 -0.00958349]]\n",
      "<NDArray 4x5 @cpu(0)> \n",
      "hidden bias:  \n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)> \n",
      "output weight:  \n",
      "[[-0.0179193  -0.01632507 -0.03224728  0.01471114]\n",
      " [-0.00140731 -0.02293223 -0.02087744 -0.03070692]]\n",
      "<NDArray 2x4 @cpu(0)> \n",
      "output bias:  \n",
      "[ 0.  0.]\n",
      "<NDArray 2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "params = net.collect_params()\n",
    "params.initialize(init=init.Normal(sigma=0.02), force_reinit=True)\n",
    "print('hidden weight: ', net.hidden.weight.data(), '\\nhidden bias: ',\n",
    "      net.hidden.bias.data(), '\\noutput weight: ', net.output.weight.data(),\n",
    "      '\\noutput bias: ',net.output.bias.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以把模型中任意层任意参数初始化，例如把上面模型中隐藏层的偏差参数初始化为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 1.  1.  1.  1.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "net.hidden.bias.initialize(init=init.One(), force_reinit=True)\n",
    "print(net.hidden.bias.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义初始化方法\n",
    "\n",
    "下面我们自定义一个初始化方法。它通过重载`_init_weight`来实现自定义的初始化方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 14.14368629  14.66310787  14.74697495  12.44425583  16.2351017 ]\n",
       " [ 11.58969593  13.38007641  11.10375118  16.74752235  16.56329536]\n",
       " [ 13.1720171   11.38182926  17.7834549   11.96582413  19.49571037]\n",
       " [ 13.68725204  16.62526894  18.20993233  10.13571644  10.97101307]]\n",
       "<NDArray 4x5 @cpu(0)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyInit(init.Initializer):\n",
    "    def __init__(self):\n",
    "        super(MyInit, self).__init__()\n",
    "        self._verbose = True\n",
    "    def _init_weight(self, _, arr):\n",
    "        # 初始化权重，使用out=arr后我们不需指定形状。\n",
    "        nd.random.uniform(low=10, high=20, out=arr)\n",
    "\n",
    "net = MLP()\n",
    "net.initialize(MyInit())\n",
    "net(x)\n",
    "net.hidden.weight.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以通过`Parameter.set_data`来直接改写模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output layer default weight: \n",
      "[[ 0.05855297 -0.06101935 -0.0396449   0.0269461 ]\n",
      " [ 0.00912645  0.0093242   0.05111437 -0.03284547]]\n",
      "<NDArray 2x4 @cpu(0)>\n",
      "output layer modified weight: \n",
      "[[ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]]\n",
      "<NDArray 2x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "net = MLP()\n",
    "net.initialize()\n",
    "net(x)\n",
    "print('output layer default weight:', net.output.weight.data())\n",
    "\n",
    "w = net.output.weight\n",
    "w.set_data(nd.ones(w.shape))\n",
    "print('output layer modified weight:', net.output.weight.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 延后的初始化\n",
    "\n",
    "我们在本节开头定义的`MLP`模型的层`nn.Dense(4)`和`nn.Dense(2)`中无需指定它们的输入单元个数。定义`net = MLP()`和输入数据`x`。我们在[“模型构造”](block.md)一节中介绍过，执行`net(x)`将调用`net`的`forward`函数计算模型输出。在这次计算中，`net`也将从输入数据`x`的形状自动推断模型中每一层尚未指定的输入单元个数，得到模型中所有参数形状，并真正完成模型参数的初始化。因此，在上面两个例子中，我们总是在调用`net(x)`之后访问初始化的模型参数。\n",
    "\n",
    "这种延后的初始化带来的一大便利是，我们在构造模型时无需指定每一层的输入单元个数。\n",
    "\n",
    "\n",
    "下面，我们具体来看延后的初始化是怎么工作的。让我们新建一个网络并打印所有模型参数。这时，两个全连接层的权重的形状里都有0。它们代表尚未指定的输入单元个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp3_ (\n",
       "  Parameter mlp3_dense0_weight (shape=(4, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter mlp3_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter mlp3_dense1_weight (shape=(2, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter mlp3_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，调用`net.initialize()`并打印所有模型参数。这时模型参数依然没有被初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp3_ (\n",
       "  Parameter mlp3_dense0_weight (shape=(4, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter mlp3_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter mlp3_dense1_weight (shape=(2, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter mlp3_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize()\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，当模型见到输入数据`x`后（`shape=(3, 5)`），模型每一层参数的形状得以推断，参数的初始化最终完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.54881352  0.59284461  0.71518934  0.84426576  0.60276335]\n",
      " [ 0.85794562  0.54488319  0.84725171  0.42365479  0.62356371]\n",
      " [ 0.64589411  0.38438171  0.4375872   0.29753461  0.89177299]]\n",
      "<NDArray 3x5 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mlp3_ (\n",
       "  Parameter mlp3_dense0_weight (shape=(4, 5), dtype=<class 'numpy.float32'>)\n",
       "  Parameter mlp3_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter mlp3_dense1_weight (shape=(2, 4), dtype=<class 'numpy.float32'>)\n",
       "  Parameter mlp3_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x)\n",
    "net(x)\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共享模型参数\n",
    "\n",
    "在有些情况下，我们希望模型的多个层之间共享模型参数。这时，我们可以通过Block的`params`来指定模型参数。在下面使用`Sequential`类构造的多层感知机中，模型的第二隐藏层（`net[1]`）和第三隐藏层（`net[2]`）共享模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.02520778 -0.00740245 -0.00711232  0.04849721]\n",
      " [ 0.06699993  0.0279271  -0.05373173 -0.02835883]\n",
      " [ 0.03738332  0.0439317  -0.01234518 -0.0144892 ]\n",
      " [ 0.02456146  0.05335445 -0.03502852  0.01137821]]\n",
      "<NDArray 4x4 @cpu(0)>\n",
      "\n",
      "[[ 0.02520778 -0.00740245 -0.00711232  0.04849721]\n",
      " [ 0.06699993  0.0279271  -0.05373173 -0.02835883]\n",
      " [ 0.03738332  0.0439317  -0.01234518 -0.0144892 ]\n",
      " [ 0.02456146  0.05335445 -0.03502852  0.01137821]]\n",
      "<NDArray 4x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(nn.Dense(4, activation='relu'))\n",
    "    net.add(nn.Dense(4, activation='relu'))\n",
    "    # 通过params指定需要共享的模型参数。\n",
    "    net.add(nn.Dense(4, activation='relu', params=net[1].params))\n",
    "    net.add(nn.Dense(2))\n",
    "\n",
    "net.initialize()\n",
    "net(x)\n",
    "print(net[1].weight.data())\n",
    "print(net[2].weight.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样，我们也可以在使用Block构造的多层感知机中，让模型的第二隐藏层（`hidden2`）和第三隐藏层（`hidden3`）共享模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.05298331 -0.05103363 -0.05559913 -0.02824048]\n",
      " [-0.05706766  0.00979508 -0.02043347  0.01272219]\n",
      " [ 0.00725428  0.01040554 -0.06529249  0.02144811]\n",
      " [ 0.06565464  0.02129445 -0.02506039 -0.00960142]]\n",
      "<NDArray 4x4 @cpu(0)>\n",
      "\n",
      "[[ 0.05298331 -0.05103363 -0.05559913 -0.02824048]\n",
      " [-0.05706766  0.00979508 -0.02043347  0.01272219]\n",
      " [ 0.00725428  0.01040554 -0.06529249  0.02144811]\n",
      " [ 0.06565464  0.02129445 -0.02506039 -0.00960142]]\n",
      "<NDArray 4x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "class MLP_SHARE(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MLP_SHARE, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.hidden1 = nn.Dense(4, activation='relu')\n",
    "            self.hidden2 = nn.Dense(4, activation='relu')\n",
    "            # 通过params指定需要共享的模型参数。\n",
    "            self.hidden3 = nn.Dense(4, activation='relu',\n",
    "                                    params=self.hidden2.params)\n",
    "            self.output = nn.Dense(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(self.hidden3(self.hidden2(self.hidden1(x))))\n",
    "\n",
    "net = MLP_SHARE()\n",
    "net.initialize()\n",
    "net(x)\n",
    "print(net.hidden2.weight.data())\n",
    "print(net.hidden3.weight.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 我们可以很方便地访问、自定义和共享模型参数。\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 在本节任何一个例子中，`net.collect_params()`和`net.params`的返回有什么不同？\n",
    "* 查阅[MXNet文档](https://mxnet.incubator.apache.org/api/python/model.html#initializer-api-reference)，了解不同的参数初始化方式。\n",
    "* 构造一个含共享参数层的多层感知机并训练。观察每一层的模型参数。\n",
    "* 如果两个层共用一个参数，求梯度的时候会发生什么？\n",
    "\n",
    "## 扫码直达[讨论区](https://discuss.gluon.ai/t/topic/987)\n",
    "\n",
    "![](../img/qr_parameters.svg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}