{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "expand_size :  80  round :  10  acc1 :  0.01  acc2 :  0.01\n",
      "expand_size :  80  round :  20  acc1 :  0.01  acc2 :  0.03\n",
      "expand_size :  80  round :  30  acc1 :  0.02  acc2 :  0.06\n",
      "expand_size :  80  round :  40  acc1 :  0.01  acc2 :  0.01\n",
      "expand_size :  80  round :  50  acc1 :  0.01  acc2 :  0.02\n",
      "\n",
      "expand_size :  85  round :  10  acc1 :  0.01  acc2 :  0.02\n",
      "expand_size :  85  round :  20  acc1 :  0.01  acc2 :  0.03\n",
      "expand_size :  85  round :  30  acc1 :  0.01  acc2 :  0.01\n",
      "expand_size :  85  round :  40  acc1 :  0.01  acc2 :  0.01\n",
      "expand_size :  85  round :  50  acc1 :  0.02  acc2 :  0.02\n",
      "\n",
      "expand_size :  90  round :  10  acc1 :  0.04  acc2 :  0.03\n",
      "expand_size :  90  round :  20  acc1 :  0.02  acc2 :  0.02\n",
      "expand_size :  90  round :  30  acc1 :  0.04  acc2 :  0.04\n",
      "expand_size :  90  round :  40  acc1 :  0.04  acc2 :  0.04\n",
      "expand_size :  90  round :  50  acc1 :  0.03  acc2 :  0.03\n",
      "\n",
      "expand_size :  95  round :  10  acc1 :  0.01  acc2 :  0.02\n",
      "expand_size :  95  round :  20  acc1 :  0.02  acc2 :  0.05\n",
      "expand_size :  95  round :  30  acc1 :  0.01  acc2 :  0.04\n",
      "expand_size :  95  round :  40  acc1 :  0.01  acc2 :  0.01\n",
      "expand_size :  95  round :  50  acc1 :  0.01  acc2 :  0.04\n",
      "\n",
      "expand_size :  100  round :  10  acc1 :  0.03  acc2 :  0.03\n",
      "expand_size :  100  round :  20  acc1 :  0.05  acc2 :  0.03\n",
      "expand_size :  100  round :  30  acc1 :  0.03  acc2 :  0.03\n",
      "expand_size :  100  round :  40  acc1 :  0.02  acc2 :  0.02\n",
      "expand_size :  100  round :  50  acc1 :  0.04  acc2 :  0.04\n",
      "\n",
      "expand_size :  105  round :  10  acc1 :  0.03  acc2 :  0.03\n",
      "expand_size :  105  round :  20  acc1 :  0.04  acc2 :  0.03\n",
      "expand_size :  105  round :  30  acc1 :  0.01  acc2 :  0.03\n",
      "expand_size :  105  round :  40  acc1 :  0.04  acc2 :  0.09\n",
      "expand_size :  105  round :  50  acc1 :  0.01  acc2 :  0.02\n",
      "\n",
      "expand_size :  110  round :  10  acc1 :  0.01  acc2 :  0.01\n",
      "expand_size :  110  round :  20  acc1 :  0.02  acc2 :  0.03\n",
      "expand_size :  110  round :  30  acc1 :  0.0  acc2 :  0.0\n",
      "expand_size :  110  round :  40  acc1 :  0.0  acc2 :  0.01\n",
      "expand_size :  110  round :  50  acc1 :  0.0  acc2 :  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mxnet import nd\n",
    "from mxnet import image\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "from time import time\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "from mxnet import init\n",
    "\n",
    "classes = ['background', 'p1', 'p2', 'p3', 'p4',\n",
    "           'p5', 'p6', 'p7', 'p8', 'p9', 'p10',\n",
    "           'p11', 'p12', 'p13', 'p14', 'p15',\n",
    "           'p16', 'p17', 'p18', 'p19']\n",
    "colormap = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0], [0, 0, 128],\n",
    "            [128, 0, 128], [0, 64, 128], [128, 128, 128], [64, 0, 0], [192, 0, 0],\n",
    "            [64, 128, 0], [192, 128, 0], [64, 0, 128], [192, 0, 128],\n",
    "            [64, 128, 128], [192, 128, 128], [0, 64, 0], [128, 64, 0],\n",
    "            [0, 192, 0], [128, 192, 0]]\n",
    "min_coordinate = [[504, 697], [1063, 606], [948, 888], [286, 902], [1079, 1191], [1053, 1507], [1048, 1661],\n",
    "                  [993, 1729], [1025, 1706], [415, 1391], [1121, 1355], [1138, 1388], [1245, 1294],\n",
    "                  [1212, 1477], [1170, 1152], [1114, 1756], [635, 1090], [1081, 1100], [350, 1006]]\n",
    "# expand_size = 55\n",
    "landmark_index = 2\n",
    "data_root = '../data'\n",
    "image_root = data_root + '/CephalometricLandmark/CroppedImage'\n",
    "txt_root = data_root + '/CephalometricLandmark/AnnotationsByMD'\n",
    "rgb_mean = nd.array([0.485, 0.456, 0.406])\n",
    "rgb_std = nd.array([0.229, 0.224, 0.225])\n",
    "\n",
    "for _ in range(10):\n",
    "    for expand_size in range(80, 160, 5):\n",
    "        print()\n",
    "\n",
    "\n",
    "        def read_images(dataset_num=0):\n",
    "\n",
    "            if dataset_num == 0:\n",
    "                begin_index = 1\n",
    "                end_index = 151\n",
    "            elif dataset_num == 1:\n",
    "                begin_index = 151\n",
    "                end_index = 301\n",
    "            else:\n",
    "                begin_index = 301\n",
    "                end_index = 401\n",
    "\n",
    "            data, label = [None] * (end_index - begin_index), [None] * (end_index - begin_index)\n",
    "            index = 0\n",
    "            for i in range(begin_index, end_index):\n",
    "                image_filename = image_root + \"/%02d/%03d.bmp\" % (landmark_index + 1, i)\n",
    "                txt_filename1 = txt_root + '/400_senior' + \"/%03d.txt\" % i\n",
    "\n",
    "                with open(txt_filename1, 'r') as f:\n",
    "                    txts = f.read().split()\n",
    "                x = int(txts[landmark_index].split(',')[0]) - min_coordinate[landmark_index][0]\n",
    "                y = int(txts[landmark_index].split(',')[1]) - min_coordinate[landmark_index][1]\n",
    "\n",
    "                minx = x - expand_size\n",
    "                maxx = x + expand_size\n",
    "                if minx < 0:\n",
    "                    minx = 0\n",
    "                if maxx >= 640:\n",
    "                    maxx = 639\n",
    "\n",
    "                miny = y - expand_size\n",
    "                maxy = y + expand_size\n",
    "                if miny < 0:\n",
    "                    miny = 0\n",
    "                if maxy >= 640:\n",
    "                    maxy = 639\n",
    "\n",
    "                data[index] = image.imread(image_filename)\n",
    "                label[index] = nd.zeros((data[index].shape[0], data[index].shape[1]))\n",
    "                label[index][miny:maxy, minx:maxx] = 1\n",
    "                index += 1\n",
    "            return data, label\n",
    "\n",
    "\n",
    "        def normalize_image(data):\n",
    "            return (data.astype('float32') / 255 - rgb_mean) / rgb_std\n",
    "\n",
    "\n",
    "        class VOCSegDataset(gluon.data.Dataset):\n",
    "\n",
    "            def __init__(self, dataset_num, crop_size):\n",
    "                self.crop_size = crop_size\n",
    "                self.data, self.label = read_images(dataset_num=dataset_num)\n",
    "                self.data[:] = [normalize_image(im) for im in self.data]\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                return self.data[idx].transpose((2, 0, 1)), self.label[idx]\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.data)\n",
    "\n",
    "\n",
    "        input_shape = (640, 640)\n",
    "        voc_train = VOCSegDataset(0, input_shape)\n",
    "        voc_test1 = VOCSegDataset(1, input_shape)\n",
    "\n",
    "        batch_size = 16\n",
    "        train_data = gluon.data.DataLoader(\n",
    "            voc_train, batch_size, shuffle=True, last_batch='discard')\n",
    "        test_data = gluon.data.DataLoader(\n",
    "            voc_test1, batch_size, last_batch='discard')\n",
    "\n",
    "        conv = nn.Conv2D(10, kernel_size=4, padding=1, strides=2)\n",
    "        conv_trans = nn.Conv2DTranspose(3, kernel_size=4, padding=1, strides=2)\n",
    "\n",
    "        conv.initialize()\n",
    "        conv_trans.initialize()\n",
    "\n",
    "        pretrained_net = models.resnet18_v2(pretrained=True)\n",
    "\n",
    "        net = nn.HybridSequential()\n",
    "        for layer in pretrained_net.features[:-2]:\n",
    "            net.add(layer)\n",
    "\n",
    "        num_classes = len(classes)\n",
    "\n",
    "        with net.name_scope():\n",
    "            net.add(\n",
    "                nn.Conv2D(2, kernel_size=1),\n",
    "                nn.Conv2DTranspose(2, kernel_size=64, padding=16, strides=32)\n",
    "            )\n",
    "\n",
    "\n",
    "        def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
    "            factor = (kernel_size + 1) // 2\n",
    "            if kernel_size % 2 == 1:\n",
    "                center = factor - 1\n",
    "            else:\n",
    "                center = factor - 0.5\n",
    "            og = np.ogrid[:kernel_size, :kernel_size]\n",
    "            filt = (1 - abs(og[0] - center) / factor) * \\\n",
    "                   (1 - abs(og[1] - center) / factor)\n",
    "            weight = np.zeros(\n",
    "                (in_channels, out_channels, kernel_size, kernel_size),\n",
    "                dtype='float32')\n",
    "            weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "            return nd.array(weight)\n",
    "\n",
    "\n",
    "        conv_trans = net[-1]\n",
    "        conv_trans.initialize(init=init.Zero())\n",
    "        net[-2].initialize(init=init.Xavier())\n",
    "\n",
    "        x = nd.zeros((batch_size, 3, *input_shape))\n",
    "        net(x)\n",
    "\n",
    "        shape = conv_trans.weight.data().shape\n",
    "        conv_trans.weight.set_data(bilinear_kernel(*shape[0:3]))\n",
    "\n",
    "        for round_num in range(5):\n",
    "            loss = gluon.loss.SoftmaxCrossEntropyLoss(axis=1)\n",
    "\n",
    "            ctx = utils.try_all_gpus()\n",
    "            net.collect_params().reset_ctx(ctx)\n",
    "\n",
    "            trainer = gluon.Trainer(net.collect_params(),\n",
    "                                    'sgd', {'learning_rate': .1, 'wd': 1e-3})\n",
    "\n",
    "            utils.train(train_data, test_data, net, loss,\n",
    "                        trainer, ctx, num_epochs=10)\n",
    "\n",
    "\n",
    "            def predict(im):\n",
    "                data = normalize_image(im)\n",
    "                data = data.transpose((2, 0, 1)).expand_dims(axis=0)\n",
    "                yhat = net(data.as_in_context(ctx[0]))\n",
    "                pred = nd.argmax(yhat, axis=1)\n",
    "                return pred.reshape((pred.shape[1], pred.shape[2]))\n",
    "\n",
    "\n",
    "            def label2image(pred):\n",
    "                x = pred.astype('int32').asnumpy()\n",
    "                cm = nd.array(colormap).astype('uint8')\n",
    "                return nd.array(cm[x, :])\n",
    "\n",
    "\n",
    "            def evaluate_acc(result, label):\n",
    "                if (len(np.where(result.asnumpy() > 0)[1]) == 0):\n",
    "                    return False, False\n",
    "                result_maxx = np.max(np.where(result.asnumpy() > 0)[1])\n",
    "                result_minx = np.min(np.where(result.asnumpy() > 0)[1])\n",
    "                result_maxy = np.max(np.where(result.asnumpy() > 0)[0])\n",
    "                result_miny = np.min(np.where(result.asnumpy() > 0)[0])\n",
    "\n",
    "                result_centerx = int((result_maxx + result_minx) / 2)\n",
    "                result_centery = int((result_maxy + result_miny) / 2)\n",
    "\n",
    "                result_avgx = np.average(np.where(result.asnumpy() > 0)[1])\n",
    "                result_avgy = np.average(np.where(result.asnumpy() > 0)[0])\n",
    "\n",
    "                label_maxx = np.max(np.where(label.asnumpy() > 0)[1])\n",
    "                label_minx = np.min(np.where(label.asnumpy() > 0)[1])\n",
    "                label_maxy = np.max(np.where(label.asnumpy() > 0)[0])\n",
    "                label_miny = np.min(np.where(label.asnumpy() > 0)[0])\n",
    "\n",
    "                label_centerx = int((label_maxx + label_minx) / 2)\n",
    "                label_centery = int((label_maxy + label_miny) / 2)\n",
    "\n",
    "                # lable_avgx = np.average(np.where(label.asnumpy() > 0)[1])\n",
    "                # lable_avgy = np.average(np.where(label.asnumpy() > 0)[0])\n",
    "\n",
    "                d1 = pow((result_centerx - label_centerx), 2) + pow((result_centery - label_centery), 2)\n",
    "                d2 = pow((result_avgx - label_centerx), 2) + pow((result_avgy - label_centery), 2)\n",
    "\n",
    "                return (d1 < 400, d2 < 400)\n",
    "\n",
    "\n",
    "            test_images, test_labels = read_images(2)\n",
    "            n = len(test_images)\n",
    "            imgs = []\n",
    "            acc1 = 0\n",
    "            acc2 = 0\n",
    "            for i in range(n):\n",
    "                x = test_images[i]\n",
    "                result = predict(x)\n",
    "                f1, f2 = evaluate_acc(result, test_labels[i])\n",
    "                if f1:\n",
    "                    acc1 += 1\n",
    "                if f2:\n",
    "                    acc2 += 1\n",
    "\n",
    "            print(\"expand_size : \", expand_size, \" round : \", 10 + 10 * round_num, \" acc1 : \", acc1 / n, \" acc2 : \",\n",
    "                  acc2 / n)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
