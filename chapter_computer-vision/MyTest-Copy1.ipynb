{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "landmark_index :  0  round :  10  acc1 :  0.95  acc2 :  0.97\n",
      "landmark_index :  0  round :  20  acc1 :  0.95  acc2 :  0.96\n",
      "landmark_index :  0  round :  30  acc1 :  0.96  acc2 :  0.97\n",
      "landmark_index :  0  round :  40  acc1 :  0.95  acc2 :  0.96\n",
      "landmark_index :  0  round :  50  acc1 :  0.95  acc2 :  0.96\n",
      "\n",
      "landmark_index :  1  round :  10  acc1 :  0.72  acc2 :  0.75\n",
      "landmark_index :  1  round :  20  acc1 :  0.76  acc2 :  0.75\n",
      "landmark_index :  1  round :  30  acc1 :  0.77  acc2 :  0.79\n",
      "landmark_index :  1  round :  40  acc1 :  0.81  acc2 :  0.81\n",
      "landmark_index :  1  round :  50  acc1 :  0.8  acc2 :  0.78\n",
      "\n",
      "landmark_index :  2  round :  10  acc1 :  0.02  acc2 :  0.03\n",
      "landmark_index :  2  round :  20  acc1 :  0.02  acc2 :  0.03\n",
      "landmark_index :  2  round :  30  acc1 :  0.04  acc2 :  0.03\n",
      "landmark_index :  2  round :  40  acc1 :  0.05  acc2 :  0.05\n",
      "landmark_index :  2  round :  50  acc1 :  0.03  acc2 :  0.04\n",
      "\n",
      "landmark_index :  3  round :  10  acc1 :  0.35  acc2 :  0.35\n",
      "landmark_index :  3  round :  20  acc1 :  0.31  acc2 :  0.33\n",
      "landmark_index :  3  round :  30  acc1 :  0.4  acc2 :  0.43\n",
      "landmark_index :  3  round :  40  acc1 :  0.41  acc2 :  0.42\n",
      "landmark_index :  3  round :  50  acc1 :  0.38  acc2 :  0.42\n",
      "\n",
      "landmark_index :  4  round :  10  acc1 :  0.35  acc2 :  0.35\n",
      "landmark_index :  4  round :  20  acc1 :  0.46  acc2 :  0.46\n",
      "landmark_index :  4  round :  30  acc1 :  0.58  acc2 :  0.6\n",
      "landmark_index :  4  round :  40  acc1 :  0.46  acc2 :  0.46\n",
      "landmark_index :  4  round :  50  acc1 :  0.54  acc2 :  0.5\n",
      "\n",
      "landmark_index :  5  round :  10  acc1 :  0.05  acc2 :  0.07\n",
      "landmark_index :  5  round :  20  acc1 :  0.05  acc2 :  0.05\n",
      "landmark_index :  5  round :  30  acc1 :  0.04  acc2 :  0.05\n",
      "landmark_index :  5  round :  40  acc1 :  0.05  acc2 :  0.06\n",
      "landmark_index :  5  round :  50  acc1 :  0.05  acc2 :  0.06\n",
      "\n",
      "landmark_index :  6  round :  10  acc1 :  0.87  acc2 :  0.89\n",
      "landmark_index :  6  round :  20  acc1 :  0.92  acc2 :  0.91\n",
      "landmark_index :  6  round :  30  acc1 :  0.91  acc2 :  0.92\n",
      "landmark_index :  6  round :  40  acc1 :  0.92  acc2 :  0.92\n",
      "landmark_index :  6  round :  50  acc1 :  0.9  acc2 :  0.89\n",
      "\n",
      "landmark_index :  7  round :  10  acc1 :  0.93  acc2 :  0.94\n",
      "landmark_index :  7  round :  20  acc1 :  0.94  acc2 :  0.94\n",
      "landmark_index :  7  round :  30  acc1 :  0.92  acc2 :  0.94\n",
      "landmark_index :  7  round :  40  acc1 :  0.93  acc2 :  0.94\n",
      "landmark_index :  7  round :  50  acc1 :  0.92  acc2 :  0.92\n",
      "\n",
      "landmark_index :  8  round :  10  acc1 :  0.95  acc2 :  0.95\n",
      "landmark_index :  8  round :  20  acc1 :  0.95  acc2 :  0.96\n",
      "landmark_index :  8  round :  30  acc1 :  0.95  acc2 :  0.95\n",
      "landmark_index :  8  round :  40  acc1 :  0.95  acc2 :  0.96\n",
      "landmark_index :  8  round :  50  acc1 :  0.94  acc2 :  0.96\n",
      "\n",
      "landmark_index :  9  round :  10  acc1 :  0.17  acc2 :  0.2\n",
      "landmark_index :  9  round :  20  acc1 :  0.31  acc2 :  0.31\n",
      "landmark_index :  9  round :  30  acc1 :  0.29  acc2 :  0.3\n",
      "landmark_index :  9  round :  40  acc1 :  0.43  acc2 :  0.44\n",
      "landmark_index :  9  round :  50  acc1 :  0.34  acc2 :  0.35\n",
      "\n",
      "landmark_index :  10  round :  10  acc1 :  0.65  acc2 :  0.68\n",
      "landmark_index :  10  round :  20  acc1 :  0.69  acc2 :  0.74\n",
      "landmark_index :  10  round :  30  acc1 :  0.76  acc2 :  0.76\n",
      "landmark_index :  10  round :  40  acc1 :  0.76  acc2 :  0.76\n",
      "landmark_index :  10  round :  50  acc1 :  0.79  acc2 :  0.77\n",
      "\n",
      "landmark_index :  11  round :  10  acc1 :  0.74  acc2 :  0.75\n",
      "landmark_index :  11  round :  20  acc1 :  0.86  acc2 :  0.87\n",
      "landmark_index :  11  round :  30  acc1 :  0.87  acc2 :  0.87\n",
      "landmark_index :  11  round :  40  acc1 :  0.9  acc2 :  0.89\n",
      "landmark_index :  11  round :  50  acc1 :  0.89  acc2 :  0.9\n",
      "\n",
      "landmark_index :  12  round :  10  acc1 :  0.21  acc2 :  0.2\n",
      "landmark_index :  12  round :  20  acc1 :  0.12  acc2 :  0.15\n",
      "landmark_index :  12  round :  30  acc1 :  0.15  acc2 :  0.15\n",
      "landmark_index :  12  round :  40  acc1 :  0.09  acc2 :  0.09\n",
      "landmark_index :  12  round :  50  acc1 :  0.12  acc2 :  0.14\n",
      "\n",
      "landmark_index :  13  round :  10  acc1 :  0.42  acc2 :  0.41\n",
      "landmark_index :  13  round :  20  acc1 :  0.44  acc2 :  0.47\n",
      "landmark_index :  13  round :  30  acc1 :  0.4  acc2 :  0.4\n",
      "landmark_index :  13  round :  40  acc1 :  0.48  acc2 :  0.5\n",
      "landmark_index :  13  round :  50  acc1 :  0.5  acc2 :  0.5\n",
      "\n",
      "landmark_index :  14  round :  10  acc1 :  0.68  acc2 :  0.7\n",
      "landmark_index :  14  round :  20  acc1 :  0.73  acc2 :  0.75\n",
      "landmark_index :  14  round :  30  acc1 :  0.8  acc2 :  0.81\n",
      "landmark_index :  14  round :  40  acc1 :  0.78  acc2 :  0.78\n",
      "landmark_index :  14  round :  50  acc1 :  0.82  acc2 :  0.84\n",
      "\n",
      "landmark_index :  15  round :  10  acc1 :  0.0  acc2 :  0.0\n",
      "landmark_index :  15  round :  20  acc1 :  0.0  acc2 :  0.0\n",
      "landmark_index :  15  round :  30  acc1 :  0.0  acc2 :  0.0\n",
      "landmark_index :  15  round :  40  acc1 :  0.0  acc2 :  0.0\n",
      "landmark_index :  15  round :  50  acc1 :  0.0  acc2 :  0.0\n",
      "\n",
      "landmark_index :  16  round :  10  acc1 :  0.57  acc2 :  0.61\n",
      "landmark_index :  16  round :  20  acc1 :  0.59  acc2 :  0.59\n",
      "landmark_index :  16  round :  30  acc1 :  0.61  acc2 :  0.6\n",
      "landmark_index :  16  round :  40  acc1 :  0.61  acc2 :  0.64\n",
      "landmark_index :  16  round :  50  acc1 :  0.6  acc2 :  0.61\n",
      "\n",
      "landmark_index :  17  round :  10  acc1 :  0.41  acc2 :  0.43\n",
      "landmark_index :  17  round :  20  acc1 :  0.54  acc2 :  0.52\n",
      "landmark_index :  17  round :  30  acc1 :  0.42  acc2 :  0.44\n",
      "landmark_index :  17  round :  40  acc1 :  0.61  acc2 :  0.63\n",
      "landmark_index :  17  round :  50  acc1 :  0.6  acc2 :  0.61\n",
      "\n",
      "landmark_index :  18  round :  10  acc1 :  0.48  acc2 :  0.47\n",
      "landmark_index :  18  round :  20  acc1 :  0.52  acc2 :  0.5\n",
      "landmark_index :  18  round :  30  acc1 :  0.58  acc2 :  0.49\n",
      "landmark_index :  18  round :  40  acc1 :  0.53  acc2 :  0.47\n",
      "landmark_index :  18  round :  50  acc1 :  0.51  acc2 :  0.5\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3db0174ea4dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             utils.train(train_data, test_data, net, loss,\n\u001b[0;32m--> 166\u001b[0;31m                         trainer, ctx, num_epochs=10)\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/gluon-tutorials/utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data, test_data, net, loss, trainer, ctx, num_epochs, print_batches)\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             train_acc += sum([(yhat.argmax(axis=1)==y).sum().asscalar()\n\u001b[0;32m--> 147\u001b[0;31m                               for yhat, y in zip(outputs, label)])\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/gluon-tutorials/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             train_acc += sum([(yhat.argmax(axis=1)==y).sum().asscalar()\n\u001b[0;32m--> 147\u001b[0;31m                               for yhat, y in zip(outputs, label)])\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1894\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1896\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gluon/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1874\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1876\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1877\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mxnet import nd\n",
    "from mxnet import image\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "from time import time\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "from mxnet import init\n",
    "\n",
    "classes = ['background', 'p1', 'p2', 'p3', 'p4',\n",
    "           'p5', 'p6', 'p7', 'p8', 'p9', 'p10',\n",
    "           'p11', 'p12', 'p13', 'p14', 'p15',\n",
    "           'p16', 'p17', 'p18', 'p19']\n",
    "colormap = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0], [0, 0, 128],\n",
    "            [128, 0, 128], [0, 64, 128], [128, 128, 128], [64, 0, 0], [192, 0, 0],\n",
    "            [64, 128, 0], [192, 128, 0], [64, 0, 128], [192, 0, 128],\n",
    "            [64, 128, 128], [192, 128, 128], [0, 64, 0], [128, 64, 0],\n",
    "            [0, 192, 0], [128, 192, 0]]\n",
    "min_coordinate = [[504,697],[1063,606],[948,888],[286,902],[1079,1191],[1053,1507],[1048,1661],\n",
    "                  [993,1729],[1025,1706],[415,1391],[1121,1355],[1138,1388],[1245,1294],\n",
    "                  [1212,1477],[1170,1152],[1114,1756],[635,1090],[1081,1100],[350,1006]]\n",
    "expand_size = 55\n",
    "# landmark_index = 0\n",
    "data_root = '../data'\n",
    "image_root = data_root + '/CephalometricLandmark/CroppedImage'\n",
    "txt_root = data_root + '/CephalometricLandmark/AnnotationsByMD'\n",
    "rgb_mean = nd.array([0.485, 0.456, 0.406])\n",
    "rgb_std = nd.array([0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "for _ in range(10):\n",
    "    for landmark_index in range(19):\n",
    "        print()\n",
    "        def read_images(dataset_num=0):\n",
    "\n",
    "            if dataset_num == 0:\n",
    "                begin_index = 1\n",
    "                end_index = 151\n",
    "            elif dataset_num == 1:\n",
    "                begin_index = 151\n",
    "                end_index = 301\n",
    "            else:\n",
    "                begin_index = 301\n",
    "                end_index = 401\n",
    "\n",
    "            data, label = [None] * (end_index - begin_index), [None] * (end_index - begin_index)\n",
    "            index = 0\n",
    "            for i in range(begin_index, end_index):\n",
    "                image_filename = image_root + \"/%02d/%03d.bmp\" % (landmark_index + 1, i)\n",
    "                txt_filename1 = txt_root + '/400_senior' + \"/%03d.txt\" % i\n",
    "\n",
    "                with open(txt_filename1, 'r') as f:\n",
    "                    txts = f.read().split()\n",
    "                x = int(txts[landmark_index].split(',')[0]) - min_coordinate[landmark_index][0]\n",
    "                y = int(txts[landmark_index].split(',')[1]) - min_coordinate[landmark_index][1]\n",
    "\n",
    "                minx = x - expand_size\n",
    "                maxx = x + expand_size\n",
    "                if minx < 0:\n",
    "                    minx = 0\n",
    "                if maxx >= 640:\n",
    "                    maxx = 639\n",
    "\n",
    "                miny = y - expand_size\n",
    "                maxy = y + expand_size\n",
    "                if miny < 0:\n",
    "                    miny = 0\n",
    "                if maxy >= 640:\n",
    "                    maxy = 639\n",
    "\n",
    "                data[index] = image.imread(image_filename)\n",
    "                label[index] = nd.zeros((data[index].shape[0], data[index].shape[1]))\n",
    "                label[index][miny:maxy, minx:maxx] = 1\n",
    "                index += 1\n",
    "            return data, label\n",
    "\n",
    "\n",
    "        def normalize_image(data):\n",
    "            return (data.astype('float32') / 255 - rgb_mean) / rgb_std\n",
    "\n",
    "\n",
    "        class VOCSegDataset(gluon.data.Dataset):\n",
    "\n",
    "            def __init__(self, dataset_num, crop_size):\n",
    "                self.crop_size = crop_size\n",
    "                self.data, self.label = read_images(dataset_num=dataset_num)\n",
    "                self.data[:] = [normalize_image(im) for im in self.data]\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                return self.data[idx].transpose((2, 0, 1)), self.label[idx]\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.data)\n",
    "\n",
    "\n",
    "        input_shape = (640, 640)\n",
    "        voc_train = VOCSegDataset(0, input_shape)\n",
    "        voc_test1 = VOCSegDataset(1, input_shape)\n",
    "\n",
    "        batch_size = 16\n",
    "        train_data = gluon.data.DataLoader(\n",
    "            voc_train, batch_size, shuffle=True, last_batch='discard')\n",
    "        test_data = gluon.data.DataLoader(\n",
    "            voc_test1, batch_size, last_batch='discard')\n",
    "\n",
    "        conv = nn.Conv2D(10, kernel_size=4, padding=1, strides=2)\n",
    "        conv_trans = nn.Conv2DTranspose(3, kernel_size=4, padding=1, strides=2)\n",
    "\n",
    "        conv.initialize()\n",
    "        conv_trans.initialize()\n",
    "\n",
    "        pretrained_net = models.resnet18_v2(pretrained=True)\n",
    "\n",
    "        net = nn.HybridSequential()\n",
    "        for layer in pretrained_net.features[:-2]:\n",
    "            net.add(layer)\n",
    "\n",
    "        num_classes = len(classes)\n",
    "\n",
    "        with net.name_scope():\n",
    "            net.add(\n",
    "                nn.Conv2D(2, kernel_size=1),\n",
    "                nn.Conv2DTranspose(2, kernel_size=64, padding=16, strides=32)\n",
    "            )\n",
    "\n",
    "\n",
    "        def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
    "            factor = (kernel_size + 1) // 2\n",
    "            if kernel_size % 2 == 1:\n",
    "                center = factor - 1\n",
    "            else:\n",
    "                center = factor - 0.5\n",
    "            og = np.ogrid[:kernel_size, :kernel_size]\n",
    "            filt = (1 - abs(og[0] - center) / factor) * \\\n",
    "                   (1 - abs(og[1] - center) / factor)\n",
    "            weight = np.zeros(\n",
    "                (in_channels, out_channels, kernel_size, kernel_size),\n",
    "                dtype='float32')\n",
    "            weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "            return nd.array(weight)\n",
    "\n",
    "\n",
    "        conv_trans = net[-1]\n",
    "        conv_trans.initialize(init=init.Zero())\n",
    "        net[-2].initialize(init=init.Xavier())\n",
    "\n",
    "        x = nd.zeros((batch_size, 3, *input_shape))\n",
    "        net(x)\n",
    "\n",
    "        shape = conv_trans.weight.data().shape\n",
    "        conv_trans.weight.set_data(bilinear_kernel(*shape[0:3]))\n",
    "\n",
    "        for round_num in range(5):\n",
    "            loss = gluon.loss.SoftmaxCrossEntropyLoss(axis=1)\n",
    "\n",
    "            ctx = utils.try_all_gpus()\n",
    "            net.collect_params().reset_ctx(ctx)\n",
    "\n",
    "            trainer = gluon.Trainer(net.collect_params(),\n",
    "                                    'sgd', {'learning_rate': .1, 'wd': 1e-3})\n",
    "\n",
    "            utils.train(train_data, test_data, net, loss,\n",
    "                        trainer, ctx, num_epochs=10)\n",
    "\n",
    "\n",
    "            def predict(im):\n",
    "                data = normalize_image(im)\n",
    "                data = data.transpose((2, 0, 1)).expand_dims(axis=0)\n",
    "                yhat = net(data.as_in_context(ctx[0]))\n",
    "                pred = nd.argmax(yhat, axis=1)\n",
    "                return pred.reshape((pred.shape[1], pred.shape[2]))\n",
    "\n",
    "\n",
    "            def label2image(pred):\n",
    "                x = pred.astype('int32').asnumpy()\n",
    "                cm = nd.array(colormap).astype('uint8')\n",
    "                return nd.array(cm[x, :])\n",
    "\n",
    "\n",
    "            test_images, test_labels = read_images(2)\n",
    "\n",
    "\n",
    "            def evaluate_acc(result, label):\n",
    "                if (len(np.where(result.asnumpy() > 0)[1]) == 0):\n",
    "                    return False, False\n",
    "                result_maxx = np.max(np.where(result.asnumpy() > 0)[1])\n",
    "                result_minx = np.min(np.where(result.asnumpy() > 0)[1])\n",
    "                result_maxy = np.max(np.where(result.asnumpy() > 0)[0])\n",
    "                result_miny = np.min(np.where(result.asnumpy() > 0)[0])\n",
    "\n",
    "                result_centerx = int((result_maxx + result_minx) / 2)\n",
    "                result_centery = int((result_maxy + result_miny) / 2)\n",
    "\n",
    "                result_avgx = np.average(np.where(result.asnumpy() > 0)[1])\n",
    "                result_avgy = np.average(np.where(result.asnumpy() > 0)[0])\n",
    "\n",
    "                label_maxx = np.max(np.where(label.asnumpy() > 0)[1])\n",
    "                label_minx = np.min(np.where(label.asnumpy() > 0)[1])\n",
    "                label_maxy = np.max(np.where(label.asnumpy() > 0)[0])\n",
    "                label_miny = np.min(np.where(label.asnumpy() > 0)[0])\n",
    "\n",
    "                label_centerx = int((label_maxx + label_minx) / 2)\n",
    "                label_centery = int((label_maxy + label_miny) / 2)\n",
    "\n",
    "                # lable_avgx = np.average(np.where(label.asnumpy() > 0)[1])\n",
    "                # lable_avgy = np.average(np.where(label.asnumpy() > 0)[0])\n",
    "\n",
    "                d1 = pow((result_centerx - label_centerx), 2) + pow((result_centery - label_centery), 2)\n",
    "                d2 = pow((result_avgx - label_centerx), 2) + pow((result_avgy - label_centery), 2)\n",
    "\n",
    "                return (d1 < 400, d2 < 400)\n",
    "\n",
    "\n",
    "            n = 100\n",
    "            imgs = []\n",
    "            acc1 = 0\n",
    "            acc2 = 0\n",
    "            for i in range(n):\n",
    "                x = test_images[i]\n",
    "                result = predict(x)\n",
    "                f1, f2 = evaluate_acc(result, test_labels[i])\n",
    "                if f1:\n",
    "                    acc1 += 1\n",
    "                if f2:\n",
    "                    acc2 += 1\n",
    "\n",
    "            print(\"landmark_index : \", landmark_index, \" round : \", 10 + 10 * round_num, \" acc1 : \", acc1 / n, \" acc2 : \",acc2 / n)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
