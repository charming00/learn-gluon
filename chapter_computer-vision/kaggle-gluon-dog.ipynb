{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实战Kaggle比赛——使用Gluon识别120种狗 (ImageNet Dogs)\n",
    "\n",
    "\n",
    "我们在本章中选择了Kaggle中的[120种狗类识别问题](https://www.kaggle.com/c/dog-breed-identification)。这是著名的ImageNet的子集数据集。与之前的[CIFAR-10原始图像分类问题](kaggle-gluon-cifar10.md)不同，本问题中的图片文件大小更接近真实照片大小，且大小不一。本问题的输出也变的更加通用：我们将输出每张图片对应120种狗的分别概率。\n",
    "\n",
    "\n",
    "## Kaggle中的CIFAR-10原始图像分类问题\n",
    "\n",
    "[Kaggle](https://www.kaggle.com)是一个著名的供机器学习爱好者交流的平台。为了便于提交结果，请大家注册[Kaggle](https://www.kaggle.com)账号。然后请大家先点击[120种狗类识别问题](https://www.kaggle.com/c/dog-breed-identification)了解有关本次比赛的信息。\n",
    "\n",
    "![](../img/kaggle-dog.png)\n",
    "\n",
    "\n",
    "\n",
    "## 整理原始数据集\n",
    "\n",
    "比赛数据分为训练数据集和测试数据集。训练集包含10,222张图片。测试集包含10,357张图片。\n",
    "\n",
    "两个数据集都是jpg彩色图片，大小接近真实照片大小，且大小不一。训练集一共有120类狗的图片。\n",
    "\n",
    "\n",
    "\n",
    "### 下载数据集\n",
    "\n",
    "\n",
    "登录Kaggle后，数据可以从[120种狗类识别问题](https://www.kaggle.com/c/dog-breed-identification/data)中下载。\n",
    "\n",
    "* [训练数据集train.zip下载地址](https://www.kaggle.com/c/dog-breed-identification/download/train.zip)\n",
    "\n",
    "* [测试数据集test.zip下载地址](https://www.kaggle.com/c/dog-breed-identification/download/test.zip)\n",
    "\n",
    "* [训练数据标签label.csv.zip下载地址](https://www.kaggle.com/c/dog-breed-identification/download/labels.csv.zip)\n",
    "\n",
    "\n",
    "### 解压数据集\n",
    "\n",
    "训练数据集train.zip和测试数据集test.zip都是压缩格式，下载后它们的路径可以如下：\n",
    "\n",
    "* ../data/kaggle_dog/train.zip\n",
    "* ../data/kaggle_dog/test.zip\n",
    "* ../data/kaggle_dog/labels.csv.zip\n",
    "\n",
    "为了使网页编译快一点，我们在git repo里仅仅存放小数据样本（'train_valid_test_tiny.zip'）。执行以下代码会从git repo里解压生成小数据样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "# 如果训练下载的Kaggle的完整数据集，把demo改为False。\n",
    "demo = True\n",
    "data_dir = '../data/kaggle_dog'\n",
    "\n",
    "if demo:\n",
    "    zipfiles= ['train_valid_test_tiny.zip']\n",
    "else:\n",
    "    zipfiles= ['train.zip', 'test.zip', 'labels.csv.zip']\n",
    "\n",
    "import zipfile\n",
    "for fin in zipfiles:\n",
    "    with zipfile.ZipFile(data_dir + '/' + fin, 'r') as zin:\n",
    "        zin.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 整理数据集\n",
    "\n",
    "对于Kaggle的完整数据集，我们需要定义下面的`reorg_dog_data`函数来整理一下。整理后，同一类狗的图片将出现在在同一个文件夹下，便于`Gluon`稍后读取。\n",
    "\n",
    "函数中的参数如data_dir、train_dir和test_dir对应上述数据存放路径及原始训练和测试的图片集文件夹名称。参数label_file为训练数据标签的文件名称。参数input_dir是整理后数据集文件夹名称。参数valid_ratio是验证集中每类狗的数量占原始训练集中数量最少一类的狗的数量（66）的比重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import shutil\n",
    "from collections import Counter\n",
    "\n",
    "def reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir, \n",
    "                   valid_ratio):\n",
    "    # 读取训练数据标签。\n",
    "    with open(os.path.join(data_dir, label_file), 'r') as f:\n",
    "        # 跳过文件头行（栏名称）。\n",
    "        lines = f.readlines()[1:]\n",
    "        tokens = [l.rstrip().split(',') for l in lines]\n",
    "        idx_label = dict(((idx, label) for idx, label in tokens))\n",
    "    labels = set(idx_label.values())\n",
    "\n",
    "    num_train = len(os.listdir(os.path.join(data_dir, train_dir)))\n",
    "    # 训练集中数量最少一类的狗的数量。\n",
    "    min_num_train_per_label = (\n",
    "        Counter(idx_label.values()).most_common()[:-2:-1][0][1])\n",
    "    # 验证集中每类狗的数量。\n",
    "    num_valid_per_label = math.floor(min_num_train_per_label * valid_ratio)\n",
    "    label_count = dict()\n",
    "\n",
    "    def mkdir_if_not_exist(path):\n",
    "        if not os.path.exists(os.path.join(*path)):\n",
    "            os.makedirs(os.path.join(*path))\n",
    "\n",
    "    # 整理训练和验证集。\n",
    "    for train_file in os.listdir(os.path.join(data_dir, train_dir)):\n",
    "        idx = train_file.split('.')[0]\n",
    "        label = idx_label[idx]\n",
    "        mkdir_if_not_exist([data_dir, input_dir, 'train_valid', label])\n",
    "        shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                    os.path.join(data_dir, input_dir, 'train_valid', label))\n",
    "        if label not in label_count or label_count[label] < num_valid_per_label:\n",
    "            mkdir_if_not_exist([data_dir, input_dir, 'valid', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                        os.path.join(data_dir, input_dir, 'valid', label))\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "        else:\n",
    "            mkdir_if_not_exist([data_dir, input_dir, 'train', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                        os.path.join(data_dir, input_dir, 'train', label))\n",
    "\n",
    "    # 整理测试集。\n",
    "    mkdir_if_not_exist([data_dir, input_dir, 'test', 'unknown'])\n",
    "    for test_file in os.listdir(os.path.join(data_dir, test_dir)):\n",
    "        shutil.copy(os.path.join(data_dir, test_dir, test_file),\n",
    "                    os.path.join(data_dir, input_dir, 'test', 'unknown'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次强调，为了使网页编译快一点，我们在这里仅仅使用小数据样本。相应地，我们仅将批量大小设为2。实际训练和测试时应使用Kaggle的完整数据集并调用`reorg_dog_data`函数整理便于`Gluon`读取的格式。由于数据集较大，批量大小batch_size大小可设为一个较大的整数，例如128。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "if demo:\n",
    "    # 注意：此处使用小数据集为便于网页编译。\n",
    "    input_dir = 'train_valid_test_tiny'\n",
    "    # 注意：此处相应使用小批量。对Kaggle的完整数据集可设较大的整数，例如128。\n",
    "    batch_size = 2\n",
    "else:\n",
    "    label_file = 'labels.csv'\n",
    "    train_dir = 'train'\n",
    "    test_dir = 'test'\n",
    "    input_dir = 'train_valid_test'\n",
    "    batch_size = 128\n",
    "    valid_ratio = 0.1 \n",
    "    reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir, \n",
    "                   valid_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Gluon读取整理后的数据集\n",
    "\n",
    "为避免过拟合，我们在这里使用`transforms`来增广数据集。例如我们加入`transforms.RandomFlipLeftRight()`即可随机对每张图片做镜面反转。以下我们列举了所有可能用到的操作，这些操作可以根据需求来决定是否调用，它们的参数也都是可调的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.data import vision\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "import numpy as np\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    # transforms.CenterCrop(32)\n",
    "    # transforms.RandomFlipTopBottom(),\n",
    "    # transforms.RandomColorJitter(brightness=0.0, contrast=0.0, saturation=0.0, hue=0.0),\n",
    "    # transforms.RandomLighting(0.0),\n",
    "    # transforms.Cast('float32'),\n",
    "\n",
    "    # 将图片按比例放缩至短边为256像素\n",
    "    transforms.Resize(256),\n",
    "    # 随机按照scale和ratio裁剪，并放缩为224x224的正方形\n",
    "    transforms.RandomResizedCrop(224, scale=(0.08, 1.0), ratio=(3.0/4.0, 4.0/3.0)),\n",
    "    # 随机左右翻转图片\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    # 将图片像素值缩小到(0,1)内，并将数据格式从\"高*宽*通道\"改为\"通道*高*宽\"\n",
    "    transforms.ToTensor(),\n",
    "    # 对图片的每个通道做标准化\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 去掉随机裁剪/翻转，保留确定性的图像预处理结果\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    # 将图片中央的224x224正方形区域裁剪出来\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们可以使用`Gluon`中的`ImageFolderDataset`类来读取整理后的数据集。注意，我们要在`loader`中调用刚刚定义好的图片增广函数。通过`vision.ImageFolderDataset`读入的数据是一个`(image, label)`组合，`transform_first()`的作用便是对这个组合中的第一个成员（即读入的图像）做图片增广操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "input_str = data_dir + '/' + input_dir + '/'\n",
    "\n",
    "# 读取原始图像文件。flag=1说明输入图像有三个通道（彩色）。\n",
    "train_ds = vision.ImageFolderDataset(input_str + 'train', flag=1)\n",
    "valid_ds = vision.ImageFolderDataset(input_str + 'valid', flag=1)\n",
    "train_valid_ds = vision.ImageFolderDataset(input_str + 'train_valid', flag=1)\n",
    "test_ds = vision.ImageFolderDataset(input_str + 'test', flag=1)\n",
    "\n",
    "loader = gluon.data.DataLoader\n",
    "train_data = loader(train_ds.transform_first(transform_train),\n",
    "                    batch_size, shuffle=True, last_batch='keep')\n",
    "valid_data = loader(valid_ds.transform_first(transform_test),\n",
    "                    batch_size, shuffle=True, last_batch='keep')\n",
    "train_valid_data = loader(train_valid_ds.transform_first(transform_train),\n",
    "                          batch_size, shuffle=True, last_batch='keep')\n",
    "test_data = loader(test_ds.transform_first(transform_test),\n",
    "                   batch_size, shuffle=False, last_batch='keep')\n",
    "\n",
    "# 交叉熵损失函数。\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设计模型\n",
    "\n",
    "这个比赛的数据属于ImageNet数据集的子集，因此我们可以借助[迁移学习](fine-tuning.md)的思想，选用在ImageNet全集上预训练过的模型，并通过微调在新数据集上进行训练。`Gluon`提供了不少预训练模型，综合考虑模型大小与准确率，我们选择使用[ResNet-34](resnet-gluon.md)。\n",
    "\n",
    "这里，我们使用与前述教程略微不同的迁移学习方法。在新的训练数据与预训练数据相似的情况下，我们认为原有特征是可重用的。基于这个原因，在一个预训练好的新模型上，我们可以不去改变原已训练好的权重，而是在原网络结构上新加一个小的输出网络。\n",
    "\n",
    "在训练过程中，我们让训练图片通过正向传播经过原有特征层与新定义的全连接网络，然后只在这个小网络上通过反向传播更新权重。这样的做法既能够节省在整个模型进行后向传播的时间，也能节省在特征层上储存梯度所需要的内存空间。\n",
    "\n",
    "注意，我们在之前定义的数据预处理函数里用了ImageNet数据集上的均值和标准差做标准化，这样才能保证预训练模型能够捕捉正确的数据特征。\n",
    "\n",
    "![](../img/fix_feature_fine_tune.png)\n",
    "\n",
    "首先我们定义一个网络，并拿到预训练好的`ResNet-34`模型权重。接下来我们新定义一个两层的全连接网络作为输出层，并初始化其权重，为接下来的训练做准备。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "\n",
    "def get_net(ctx):\n",
    "    # 设置 pretrained=True 就能拿到预训练模型的权重，第一次使用需要联网下载\n",
    "    finetune_net = models.resnet34_v2(pretrained=True)\n",
    "\n",
    "    # 定义新的输出网络\n",
    "    finetune_net.output_new = nn.HybridSequential(prefix='')\n",
    "    # 定义256个神经元的全连接层\n",
    "    finetune_net.output_new.add(nn.Dense(256, activation='relu'))\n",
    "    # 定义120个神经元的全连接层，输出分类预测\n",
    "    finetune_net.output_new.add(nn.Dense(120))\n",
    "    # 初始化这个输出网络\n",
    "    finetune_net.output_new.initialize(init.Xavier(), ctx=ctx)\n",
    "\n",
    "    # 把网络参数分配到即将用于计算的CPU/GPU上\n",
    "    finetune_net.collect_params().reset_ctx(ctx)\n",
    "    return finetune_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型并调参\n",
    "\n",
    "在[过拟合](../chapter_supervised-learning/underfit-overfit.md)中我们讲过，过度依赖训练数据集的误差来推断测试数据集的误差容易导致过拟合。由于图像分类训练时间可能较长，为了方便，我们这里不再使用K折交叉验证，而是依赖验证集的结果来调参。\n",
    "\n",
    "我们定义损失函数以便于计算验证集上的损失函数值。我们也定义了模型训练函数，其中的优化算法和参数都是可以调的。\n",
    "\n",
    "注意，我们为了只更新新的输出层参数，做了两处修改：\n",
    "\n",
    "1. 在`gluon.Trainer`里只对`net.output_new.collect_params()`定义了优化方法和参数。\n",
    "2. 在训练时只在新输出层上记录自动求导的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "\n",
    "def get_loss(data, net, ctx):\n",
    "    loss = 0.0\n",
    "    for feas, label in data:\n",
    "        label = label.as_in_context(ctx)\n",
    "        # 计算特征层的结果\n",
    "        output_features = net.features(feas.as_in_context(ctx))\n",
    "        # 将特征层的结果作为输入，计算全连接网络的结果\n",
    "        output = net.output_new(output_features)\n",
    "        cross_entropy = softmax_cross_entropy(output, label)\n",
    "        loss += nd.mean(cross_entropy).asscalar()\n",
    "    return loss / len(data)\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, lr, wd, ctx, lr_period,\n",
    "          lr_decay):\n",
    "    # 只在新的全连接网络的参数上进行训练\n",
    "    trainer = gluon.Trainer(net.output_new.collect_params(),\n",
    "                            'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for data, label in train_data:\n",
    "            label = label.astype('float32').as_in_context(ctx)\n",
    "            # 正向传播计算特征层的结果\n",
    "            output_features = net.features(data.as_in_context(ctx))\n",
    "            with autograd.record():\n",
    "                # 将特征层的结果作为输入，计算全连接网络的结果\n",
    "                output = net.output_new(output_features)\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            # 反向传播与权重更新只发生在全连接网络上\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        if valid_data is not None:\n",
    "            valid_loss = get_loss(valid_data, net, ctx)\n",
    "            epoch_str = (\"Epoch %d. Train loss: %f, Valid loss %f, \"\n",
    "                         % (epoch, train_loss / len(train_data), valid_loss))\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Train loss: %f, \"\n",
    "                         % (epoch, train_loss / len(train_data)))\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str + ', lr ' + str(trainer.learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下定义训练参数并训练模型。这些参数均可调。为了使网页编译快一点，我们这里将epoch数量有意设为1。事实上，epoch一般可以调大些。我们将依据验证集的结果不断优化模型设计和调整参数。\n",
    "\n",
    "另外，微调一个预训练模型往往不需要特别久的额外训练。依据下面的参数设置，优化算法的学习率设为0.01，并将在每10个epoch自乘0.1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train loss: 5.371835, Valid loss 4.732098, Time 00:00:01, lr 0.01\n"
     ]
    }
   ],
   "source": [
    "ctx = utils.try_gpu()\n",
    "num_epochs = 1\n",
    "learning_rate = 0.01\n",
    "weight_decay = 1e-4\n",
    "lr_period = 10\n",
    "lr_decay = 0.1\n",
    "\n",
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      weight_decay, ctx, lr_period, lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对测试集分类\n",
    "\n",
    "当得到一组满意的模型设计和参数后，我们使用全部训练数据集（含验证集）重新训练模型，并对测试集分类。注意，我们要用刚训练好的新输出层做预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train loss: 5.068107, Time 00:00:02, lr 0.01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_valid_data, None, num_epochs, learning_rate, weight_decay, \n",
    "      ctx, lr_period, lr_decay)\n",
    "\n",
    "outputs = []\n",
    "for data, label in test_data:\n",
    "    # 计算特征层的结果\n",
    "    output_features = net.features(data.as_in_context(ctx))\n",
    "    # 将特征层的结果作为输入，计算全连接网络的结果\n",
    "    output = nd.softmax(net.output_new(output_features))\n",
    "    outputs.extend(output.asnumpy())\n",
    "ids = sorted(os.listdir(os.path.join(data_dir, input_dir, 'test/unknown')))\n",
    "with open('submission.csv', 'w') as f:\n",
    "    f.write('id,' + ','.join(train_valid_ds.synsets) + '\\n')\n",
    "    for i, output in zip(ids, outputs):\n",
    "        f.write(i.split('.')[0] + ',' + ','.join(\n",
    "            [str(num) for num in output]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述代码执行完会生成一个`submission.csv`的文件用于在Kaggle上提交。这是Kaggle要求的提交格式。这时我们可以在Kaggle上把对测试集分类的结果提交并查看分类准确率。你需要登录Kaggle网站，打开[120种狗类识别问题](https://www.kaggle.com/c/dog-breed-identification)，并点击下方右侧`Submit Predictions`按钮。\n",
    "\n",
    "![](../img/kaggle-dog-submit1.png)\n",
    "\n",
    "\n",
    "请点击下方`Upload Submission File`选择需要提交的预测结果。然后点击下方的`Make Submission`按钮就可以查看结果啦！\n",
    "\n",
    "![](../img/kaggle-dog-submit2.png)\n",
    "\n",
    "温馨提醒，目前**Kaggle仅限每个账号一天以内5次提交结果的机会**。所以提交结果前务必三思。\n",
    "\n",
    "\n",
    "## 作业（[汇报作业和查看其他小伙伴作业](https://discuss.gluon.ai/t/topic/2399)）：\n",
    "\n",
    "* 使用Kaggle完整数据集，把batch_size和num_epochs分别调大些，可以在Kaggle上拿到什么样的准确率和名次？\n",
    "* 你还有什么其他办法可以继续改进模型和参数？小伙伴们都期待你的分享。\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/2399)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}