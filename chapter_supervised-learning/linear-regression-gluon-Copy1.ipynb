{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归——使用Gluon\n",
    "\n",
    "[前一章](linear-regression-scratch.md)我们仅仅使用了`ndarray`和`autograd`来实现线性回归，这一章我们仍然实现同样的模型，但是使用高层抽象包`gluon`。\n",
    "\n",
    "## 创建数据集\n",
    "\n",
    "我们生成同样的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[[ 1333.  2200.]\n",
      "  [ 1263.  2272.]\n",
      "  [ 1305.  2252.]]\n",
      "\n",
      " [[ 1381.  2005.]\n",
      "  [ 1320.  2091.]\n",
      "  [ 1363.  2063.]]\n",
      "\n",
      " [[ 1460.  1882.]\n",
      "  [ 1406.  1935.]\n",
      "  [ 1448.  1920.]]\n",
      "\n",
      " [[ 1468.  2016.]\n",
      "  [ 1418.  2091.]\n",
      "  [ 1449.  2061.]]\n",
      "\n",
      " [[ 1324.  1964.]\n",
      "  [ 1278.  2010.]\n",
      "  [ 1309.  1993.]]\n",
      "\n",
      " [[ 1388.  2109.]\n",
      "  [ 1345.  2155.]\n",
      "  [ 1368.  2140.]]\n",
      "\n",
      " [[ 1345.  2143.]\n",
      "  [ 1287.  2188.]\n",
      "  [ 1325.  2178.]]\n",
      "\n",
      " [[ 1367.  1825.]\n",
      "  [ 1318.  1886.]\n",
      "  [ 1354.  1867.]]\n",
      "\n",
      " [[ 1307.  2014.]\n",
      "  [ 1272.  2064.]\n",
      "  [ 1294.  2045.]]\n",
      "\n",
      " [[ 1366.  1876.]\n",
      "  [ 1332.  1934.]\n",
      "  [ 1359.  1912.]]\n",
      "\n",
      " [[ 1269.  1974.]\n",
      "  [ 1198.  2035.]\n",
      "  [ 1244.  2018.]]\n",
      "\n",
      " [[ 1620.  2077.]\n",
      "  [ 1581.  2136.]\n",
      "  [ 1604.  2109.]]\n",
      "\n",
      " [[ 1278.  1942.]\n",
      "  [ 1234.  2002.]\n",
      "  [ 1264.  1980.]]\n",
      "\n",
      " [[ 1245.  2018.]\n",
      "  [ 1198.  2066.]\n",
      "  [ 1229.  2051.]]\n",
      "\n",
      " [[ 1292.  2038.]\n",
      "  [ 1245.  2104.]\n",
      "  [ 1276.  2079.]]\n",
      "\n",
      " [[ 1435.  1956.]\n",
      "  [ 1393.  2003.]\n",
      "  [ 1421.  1989.]]\n",
      "\n",
      " [[ 1411.  2067.]\n",
      "  [ 1358.  2138.]\n",
      "  [ 1389.  2110.]]\n",
      "\n",
      " [[ 1249.  1994.]\n",
      "  [ 1194.  2028.]\n",
      "  [ 1226.  2019.]]\n",
      "\n",
      " [[ 1317.  1873.]\n",
      "  [ 1289.  1933.]\n",
      "  [ 1307.  1906.]]\n",
      "\n",
      " [[ 1313.  1976.]\n",
      "  [ 1253.  2037.]\n",
      "  [ 1295.  2013.]]\n",
      "\n",
      " [[ 1187.  2124.]\n",
      "  [ 1116.  2206.]\n",
      "  [ 1178.  2188.]]\n",
      "\n",
      " [[ 1434.  1915.]\n",
      "  [ 1389.  1989.]\n",
      "  [ 1424.  1961.]]\n",
      "\n",
      " [[ 1553.  2188.]\n",
      "  [ 1478.  2255.]\n",
      "  [ 1528.  2234.]]\n",
      "\n",
      " [[ 1332.  1984.]\n",
      "  [ 1271.  2058.]\n",
      "  [ 1310.  2034.]]\n",
      "\n",
      " [[ 1116.  2069.]\n",
      "  [ 1046.  2124.]\n",
      "  [ 1086.  2105.]]\n",
      "\n",
      " [[ 1503.  2084.]\n",
      "  [ 1435.  2142.]\n",
      "  [ 1480.  2126.]]\n",
      "\n",
      " [[ 1411.  1955.]\n",
      "  [ 1370.  2005.]\n",
      "  [ 1397.  1985.]]\n",
      "\n",
      " [[ 1448.  1950.]\n",
      "  [ 1394.  2004.]\n",
      "  [ 1430.  1986.]]\n",
      "\n",
      " [[ 1384.  2036.]\n",
      "  [ 1313.  2097.]\n",
      "  [ 1359.  2080.]]\n",
      "\n",
      " [[ 1270.  2146.]\n",
      "  [ 1215.  2189.]\n",
      "  [ 1251.  2175.]]\n",
      "\n",
      " [[ 1476.  1893.]\n",
      "  [ 1428.  1963.]\n",
      "  [ 1464.  1942.]]\n",
      "\n",
      " [[ 1313.  1909.]\n",
      "  [ 1263.  1966.]\n",
      "  [ 1302.  1951.]]\n",
      "\n",
      " [[ 1408.  2097.]\n",
      "  [ 1343.  2160.]\n",
      "  [ 1387.  2143.]]\n",
      "\n",
      " [[ 1494.  1934.]\n",
      "  [ 1442.  2006.]\n",
      "  [ 1476.  1981.]]\n",
      "\n",
      " [[ 1323.  1905.]\n",
      "  [ 1261.  1969.]\n",
      "  [ 1302.  1953.]]\n",
      "\n",
      " [[ 1259.  2028.]\n",
      "  [ 1204.  2078.]\n",
      "  [ 1239.  2063.]]\n",
      "\n",
      " [[ 1390.  2144.]\n",
      "  [ 1343.  2188.]\n",
      "  [ 1374.  2173.]]\n",
      "\n",
      " [[ 1220.  2008.]\n",
      "  [ 1152.  2068.]\n",
      "  [ 1193.  2047.]]\n",
      "\n",
      " [[ 1410.  1949.]\n",
      "  [ 1329.  2012.]\n",
      "  [ 1391.  1997.]]\n",
      "\n",
      " [[ 1238.  1953.]\n",
      "  [ 1179.  2008.]\n",
      "  [ 1218.  1988.]]\n",
      "\n",
      " [[ 1510.  1920.]\n",
      "  [ 1454.  2022.]\n",
      "  [ 1501.  1977.]]\n",
      "\n",
      " [[ 1327.  1850.]\n",
      "  [ 1279.  1923.]\n",
      "  [ 1312.  1893.]]\n",
      "\n",
      " [[ 1354.  1903.]\n",
      "  [ 1304.  1962.]\n",
      "  [ 1337.  1939.]]\n",
      "\n",
      " [[ 1258.  1959.]\n",
      "  [ 1212.  1998.]\n",
      "  [ 1243.  1988.]]\n",
      "\n",
      " [[ 1290.  1929.]\n",
      "  [ 1241.  1963.]\n",
      "  [ 1270.  1956.]]\n",
      "\n",
      " [[ 1307.  1979.]\n",
      "  [ 1248.  2039.]\n",
      "  [ 1288.  2019.]]\n",
      "\n",
      " [[ 1296.  2010.]\n",
      "  [ 1263.  2075.]\n",
      "  [ 1289.  2049.]]\n",
      "\n",
      " [[ 1324.  1966.]\n",
      "  [ 1283.  2030.]\n",
      "  [ 1315.  2012.]]\n",
      "\n",
      " [[ 1343.  1840.]\n",
      "  [ 1295.  1879.]\n",
      "  [ 1327.  1868.]]\n",
      "\n",
      " [[ 1480.  2077.]\n",
      "  [ 1407.  2153.]\n",
      "  [ 1456.  2131.]]\n",
      "\n",
      " [[ 1361.  1902.]\n",
      "  [ 1307.  1950.]\n",
      "  [ 1344.  1939.]]\n",
      "\n",
      " [[ 1528.  2063.]\n",
      "  [ 1464.  2158.]\n",
      "  [ 1511.  2129.]]\n",
      "\n",
      " [[ 1373.  1984.]\n",
      "  [ 1325.  2039.]\n",
      "  [ 1360.  2021.]]\n",
      "\n",
      " [[ 1366.  1901.]\n",
      "  [ 1303.  1955.]\n",
      "  [ 1344.  1936.]]\n",
      "\n",
      " [[ 1400.  1859.]\n",
      "  [ 1351.  1903.]\n",
      "  [ 1386.  1886.]]\n",
      "\n",
      " [[ 1166.  2088.]\n",
      "  [ 1105.  2124.]\n",
      "  [ 1144.  2117.]]\n",
      "\n",
      " [[ 1294.  1896.]\n",
      "  [ 1242.  1935.]\n",
      "  [ 1276.  1922.]]\n",
      "\n",
      " [[ 1222.  1856.]\n",
      "  [ 1163.  1906.]\n",
      "  [ 1202.  1892.]]\n",
      "\n",
      " [[ 1321.  1838.]\n",
      "  [ 1249.  1901.]\n",
      "  [ 1298.  1880.]]\n",
      "\n",
      " [[ 1346.  2122.]\n",
      "  [ 1294.  2166.]\n",
      "  [ 1328.  2150.]]\n",
      "\n",
      " [[ 1460.  1843.]\n",
      "  [ 1402.  1907.]\n",
      "  [ 1443.  1880.]]\n",
      "\n",
      " [[ 1326.  2018.]\n",
      "  [ 1259.  2063.]\n",
      "  [ 1303.  2054.]]\n",
      "\n",
      " [[ 1396.  1999.]\n",
      "  [ 1327.  2048.]\n",
      "  [ 1371.  2029.]]\n",
      "\n",
      " [[ 1315.  1929.]\n",
      "  [ 1247.  1972.]\n",
      "  [ 1292.  1964.]]\n",
      "\n",
      " [[ 1394.  1951.]\n",
      "  [ 1331.  2009.]\n",
      "  [ 1375.  1991.]]\n",
      "\n",
      " [[ 1322.  1916.]\n",
      "  [ 1270.  1976.]\n",
      "  [ 1307.  1963.]]\n",
      "\n",
      " [[ 1443.  1937.]\n",
      "  [ 1390.  1996.]\n",
      "  [ 1424.  1972.]]\n",
      "\n",
      " [[ 1544.  2098.]\n",
      "  [ 1453.  2180.]\n",
      "  [ 1517.  2160.]]\n",
      "\n",
      " [[ 1215.  1966.]\n",
      "  [ 1158.  2027.]\n",
      "  [ 1193.  2012.]]\n",
      "\n",
      " [[ 1228.  1979.]\n",
      "  [ 1177.  2025.]\n",
      "  [ 1212.  2010.]]\n",
      "\n",
      " [[ 1291.  1790.]\n",
      "  [ 1262.  1831.]\n",
      "  [ 1287.  1819.]]\n",
      "\n",
      " [[ 1343.  1938.]\n",
      "  [ 1282.  2002.]\n",
      "  [ 1322.  1980.]]\n",
      "\n",
      " [[ 1351.  1986.]\n",
      "  [ 1281.  2057.]\n",
      "  [ 1327.  2033.]]\n",
      "\n",
      " [[ 1477.  1927.]\n",
      "  [ 1403.  1995.]\n",
      "  [ 1451.  1969.]]\n",
      "\n",
      " [[ 1280.  1963.]\n",
      "  [ 1234.  2023.]\n",
      "  [ 1268.  1999.]]\n",
      "\n",
      " [[ 1403.  1869.]\n",
      "  [ 1333.  1931.]\n",
      "  [ 1375.  1908.]]\n",
      "\n",
      " [[ 1383.  1930.]\n",
      "  [ 1317.  2027.]\n",
      "  [ 1365.  1989.]]\n",
      "\n",
      " [[ 1454.  1979.]\n",
      "  [ 1400.  2025.]\n",
      "  [ 1433.  2006.]]\n",
      "\n",
      " [[ 1486.  2118.]\n",
      "  [ 1449.  2199.]\n",
      "  [ 1478.  2164.]]\n",
      "\n",
      " [[ 1191.  1924.]\n",
      "  [ 1161.  1947.]\n",
      "  [ 1184.  1945.]]\n",
      "\n",
      " [[ 1288.  2080.]\n",
      "  [ 1223.  2142.]\n",
      "  [ 1267.  2124.]]\n",
      "\n",
      " [[ 1357.  2002.]\n",
      "  [ 1298.  2060.]\n",
      "  [ 1335.  2035.]]\n",
      "\n",
      " [[ 1291.  1800.]\n",
      "  [ 1240.  1851.]\n",
      "  [ 1277.  1833.]]\n",
      "\n",
      " [[ 1194.  2027.]\n",
      "  [ 1141.  2041.]\n",
      "  [ 1169.  2041.]]\n",
      "\n",
      " [[ 1184.  2133.]\n",
      "  [ 1120.  2174.]\n",
      "  [ 1159.  2162.]]\n",
      "\n",
      " [[ 1505.  2010.]\n",
      "  [ 1436.  2067.]\n",
      "  [ 1485.  2050.]]\n",
      "\n",
      " [[ 1341.  2035.]\n",
      "  [ 1283.  2118.]\n",
      "  [ 1330.  2090.]]\n",
      "\n",
      " [[ 1262.  1896.]\n",
      "  [ 1203.  1925.]\n",
      "  [ 1237.  1920.]]\n",
      "\n",
      " [[ 1503.  1988.]\n",
      "  [ 1453.  2071.]\n",
      "  [ 1490.  2036.]]\n",
      "\n",
      " [[ 1192.  1914.]\n",
      "  [ 1123.  1986.]\n",
      "  [ 1167.  1960.]]\n",
      "\n",
      " [[ 1349.  2107.]\n",
      "  [ 1283.  2159.]\n",
      "  [ 1327.  2146.]]\n",
      "\n",
      " [[ 1431.  1959.]\n",
      "  [ 1381.  2036.]\n",
      "  [ 1417.  2003.]]\n",
      "\n",
      " [[ 1327.  1912.]\n",
      "  [ 1247.  1979.]\n",
      "  [ 1302.  1957.]]\n",
      "\n",
      " [[ 1438.  2018.]\n",
      "  [ 1366.  2079.]\n",
      "  [ 1414.  2060.]]\n",
      "\n",
      " [[ 1405.  2090.]\n",
      "  [ 1337.  2161.]\n",
      "  [ 1384.  2135.]]\n",
      "\n",
      " [[ 1256.  2004.]\n",
      "  [ 1192.  2071.]\n",
      "  [ 1237.  2051.]]\n",
      "\n",
      " [[ 1454.  2008.]\n",
      "  [ 1397.  2067.]\n",
      "  [ 1437.  2047.]]\n",
      "\n",
      " [[ 1404.  1884.]\n",
      "  [ 1338.  1944.]\n",
      "  [ 1382.  1929.]]\n",
      "\n",
      " [[ 1400.  1895.]\n",
      "  [ 1362.  1951.]\n",
      "  [ 1390.  1933.]]\n",
      "\n",
      " [[ 1276.  2063.]\n",
      "  [ 1238.  2121.]\n",
      "  [ 1269.  2101.]]\n",
      "\n",
      " [[ 1359.  1946.]\n",
      "  [ 1297.  2021.]\n",
      "  [ 1346.  1995.]]\n",
      "\n",
      " [[ 1325.  2052.]\n",
      "  [ 1265.  2099.]\n",
      "  [ 1302.  2085.]]\n",
      "\n",
      " [[ 1303.  1967.]\n",
      "  [ 1241.  2011.]\n",
      "  [ 1284.  2003.]]\n",
      "\n",
      " [[ 1397.  2064.]\n",
      "  [ 1336.  2122.]\n",
      "  [ 1375.  2107.]]\n",
      "\n",
      " [[ 1231.  1874.]\n",
      "  [ 1164.  1921.]\n",
      "  [ 1208.  1902.]]\n",
      "\n",
      " [[ 1526.  2016.]\n",
      "  [ 1435.  2111.]\n",
      "  [ 1500.  2076.]]\n",
      "\n",
      " [[ 1373.  1959.]\n",
      "  [ 1334.  2045.]\n",
      "  [ 1369.  2010.]]\n",
      "\n",
      " [[ 1407.  1958.]\n",
      "  [ 1342.  2027.]\n",
      "  [ 1389.  2007.]]\n",
      "\n",
      " [[ 1215.  2013.]\n",
      "  [ 1157.  2052.]\n",
      "  [ 1192.  2046.]]\n",
      "\n",
      " [[ 1509.  2048.]\n",
      "  [ 1461.  2109.]\n",
      "  [ 1497.  2095.]]\n",
      "\n",
      " [[ 1334.  2012.]\n",
      "  [ 1271.  2071.]\n",
      "  [ 1315.  2057.]]\n",
      "\n",
      " [[ 1405.  1988.]\n",
      "  [ 1343.  2045.]\n",
      "  [ 1387.  2023.]]\n",
      "\n",
      " [[ 1350.  1916.]\n",
      "  [ 1292.  1965.]\n",
      "  [ 1333.  1955.]]\n",
      "\n",
      " [[ 1368.  2128.]\n",
      "  [ 1314.  2182.]\n",
      "  [ 1353.  2168.]]\n",
      "\n",
      " [[ 1378.  1984.]\n",
      "  [ 1310.  2064.]\n",
      "  [ 1360.  2041.]]\n",
      "\n",
      " [[ 1544.  2005.]\n",
      "  [ 1462.  2069.]\n",
      "  [ 1519.  2048.]]\n",
      "\n",
      " [[ 1348.  1965.]\n",
      "  [ 1307.  2019.]\n",
      "  [ 1333.  1998.]]\n",
      "\n",
      " [[ 1181.  2076.]\n",
      "  [ 1129.  2114.]\n",
      "  [ 1162.  2100.]]\n",
      "\n",
      " [[ 1307.  2022.]\n",
      "  [ 1274.  2070.]\n",
      "  [ 1295.  2051.]]\n",
      "\n",
      " [[ 1427.  1997.]\n",
      "  [ 1375.  2073.]\n",
      "  [ 1408.  2043.]]\n",
      "\n",
      " [[ 1311.  2126.]\n",
      "  [ 1261.  2197.]\n",
      "  [ 1294.  2168.]]\n",
      "\n",
      " [[ 1125.  1984.]\n",
      "  [ 1073.  2021.]\n",
      "  [ 1106.  2010.]]\n",
      "\n",
      " [[ 1297.  2008.]\n",
      "  [ 1246.  2080.]\n",
      "  [ 1283.  2055.]]\n",
      "\n",
      " [[ 1383.  2046.]\n",
      "  [ 1316.  2113.]\n",
      "  [ 1357.  2087.]]\n",
      "\n",
      " [[ 1314.  1946.]\n",
      "  [ 1247.  1998.]\n",
      "  [ 1292.  1987.]]\n",
      "\n",
      " [[ 1467.  2038.]\n",
      "  [ 1416.  2098.]\n",
      "  [ 1452.  2080.]]\n",
      "\n",
      " [[ 1420.  1956.]\n",
      "  [ 1374.  2019.]\n",
      "  [ 1407.  1998.]]\n",
      "\n",
      " [[ 1309.  1794.]\n",
      "  [ 1249.  1875.]\n",
      "  [ 1291.  1845.]]\n",
      "\n",
      " [[ 1357.  1978.]\n",
      "  [ 1318.  2050.]\n",
      "  [ 1344.  2019.]]\n",
      "\n",
      " [[ 1520.  1987.]\n",
      "  [ 1468.  2077.]\n",
      "  [ 1505.  2042.]]\n",
      "\n",
      " [[ 1252.  1851.]\n",
      "  [ 1192.  1906.]\n",
      "  [ 1229.  1891.]]\n",
      "\n",
      " [[ 1348.  1943.]\n",
      "  [ 1302.  2024.]\n",
      "  [ 1335.  1992.]]\n",
      "\n",
      " [[ 1288.  1763.]\n",
      "  [ 1244.  1826.]\n",
      "  [ 1277.  1801.]]\n",
      "\n",
      " [[ 1236.  1790.]\n",
      "  [ 1189.  1842.]\n",
      "  [ 1220.  1823.]]\n",
      "\n",
      " [[ 1477.  2041.]\n",
      "  [ 1434.  2095.]\n",
      "  [ 1463.  2076.]]\n",
      "\n",
      " [[ 1411.  1987.]\n",
      "  [ 1342.  2054.]\n",
      "  [ 1387.  2041.]]\n",
      "\n",
      " [[ 1522.  2072.]\n",
      "  [ 1469.  2147.]\n",
      "  [ 1504.  2121.]]\n",
      "\n",
      " [[ 1289.  2006.]\n",
      "  [ 1257.  2054.]\n",
      "  [ 1281.  2035.]]\n",
      "\n",
      " [[ 1300.  2147.]\n",
      "  [ 1247.  2221.]\n",
      "  [ 1284.  2197.]]\n",
      "\n",
      " [[ 1287.  1905.]\n",
      "  [ 1235.  1960.]\n",
      "  [ 1271.  1942.]]\n",
      "\n",
      " [[ 1328.  2036.]\n",
      "  [ 1275.  2089.]\n",
      "  [ 1306.  2063.]]\n",
      "\n",
      " [[ 1289.  1824.]\n",
      "  [ 1234.  1899.]\n",
      "  [ 1275.  1874.]]\n",
      "\n",
      " [[ 1434.  1938.]\n",
      "  [ 1396.  2009.]\n",
      "  [ 1426.  1987.]]\n",
      "\n",
      " [[ 1413.  2055.]\n",
      "  [ 1360.  2131.]\n",
      "  [ 1399.  2102.]]\n",
      "\n",
      " [[ 1470.  1975.]\n",
      "  [ 1427.  2047.]\n",
      "  [ 1455.  2022.]]\n",
      "\n",
      " [[ 1179.  2162.]\n",
      "  [ 1106.  2199.]\n",
      "  [ 1146.  2194.]]\n",
      "\n",
      " [[ 1245.  1956.]\n",
      "  [ 1197.  2029.]\n",
      "  [ 1229.  2001.]]\n",
      "\n",
      " [[ 1207.  1990.]\n",
      "  [ 1144.  2059.]\n",
      "  [ 1184.  2033.]]\n",
      "\n",
      " [[ 1430.  1899.]\n",
      "  [ 1380.  1981.]\n",
      "  [ 1415.  1945.]]\n",
      "\n",
      " [[ 1196.  1832.]\n",
      "  [ 1138.  1882.]\n",
      "  [ 1176.  1865.]]]\n",
      "<NDArray 150x3x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "\n",
    "data_root = '../data'\n",
    "txt_root = data_root + '/CephalometricLandmark/AnnotationsByMD'\n",
    "landmark_index = 15\n",
    "num_examples = 150\n",
    "num_inputs = 2\n",
    "X = nd.zeros(shape=(num_examples, 3, 2))\n",
    "y = nd.zeros(shape=(num_examples, 2))\n",
    "for i in range(num_examples):\n",
    "    txt_filename1 = txt_root + '/400_senior' + \"/%03d.txt\" % (i+1)\n",
    "    with open(txt_filename1, 'r') as f:\n",
    "        txts = f.read().split()\n",
    "    x7 = int(txts[6].split(',')[0])\n",
    "    y7 = int(txts[6].split(',')[1])\n",
    "    X[i][0][0] = x7\n",
    "    X[i][0][1] = y7\n",
    "    \n",
    "    x8 = int(txts[7].split(',')[0])\n",
    "    y8 = int(txts[7].split(',')[1])\n",
    "    X[i][1][0] = x8\n",
    "    X[i][1][1] = y8\n",
    "    \n",
    "    x9 = int(txts[8].split(',')[0])\n",
    "    y9 = int(txts[8].split(',')[1])\n",
    "    X[i][2][0] = x9\n",
    "    X[i][2][1] = y9\n",
    "    \n",
    "    x16 = int(txts[15].split(',')[0])\n",
    "    y16 = int(txts[15].split(',')[1])\n",
    "    y[i][0] = x16\n",
    "    y[i][1] = y16\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取\n",
    "\n",
    "但这里使用`data`模块来读取数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "dataset = gluon.data.ArrayDataset(X, y)\n",
    "data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取跟前面一致："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[[ 1245.  2018.]\n",
      "  [ 1198.  2066.]\n",
      "  [ 1229.  2051.]]\n",
      "\n",
      " [[ 1350.  1916.]\n",
      "  [ 1292.  1965.]\n",
      "  [ 1333.  1955.]]\n",
      "\n",
      " [[ 1300.  2147.]\n",
      "  [ 1247.  2221.]\n",
      "  [ 1284.  2197.]]\n",
      "\n",
      " [[ 1400.  1859.]\n",
      "  [ 1351.  1903.]\n",
      "  [ 1386.  1886.]]\n",
      "\n",
      " [[ 1326.  2018.]\n",
      "  [ 1259.  2063.]\n",
      "  [ 1303.  2054.]]\n",
      "\n",
      " [[ 1431.  1959.]\n",
      "  [ 1381.  2036.]\n",
      "  [ 1417.  2003.]]\n",
      "\n",
      " [[ 1438.  2018.]\n",
      "  [ 1366.  2079.]\n",
      "  [ 1414.  2060.]]\n",
      "\n",
      " [[ 1116.  2069.]\n",
      "  [ 1046.  2124.]\n",
      "  [ 1086.  2105.]]\n",
      "\n",
      " [[ 1288.  2080.]\n",
      "  [ 1223.  2142.]\n",
      "  [ 1267.  2124.]]\n",
      "\n",
      " [[ 1454.  2008.]\n",
      "  [ 1397.  2067.]\n",
      "  [ 1437.  2047.]]]\n",
      "<NDArray 10x3x2 @cpu(0)> \n",
      "[[ 1312.  2105.]\n",
      " [ 1456.  1994.]\n",
      " [ 1389.  2249.]\n",
      " [ 1474.  1942.]\n",
      " [ 1369.  2082.]\n",
      " [ 1487.  2018.]\n",
      " [ 1497.  2079.]\n",
      " [ 1206.  2190.]\n",
      " [ 1305.  2155.]\n",
      " [ 1526.  2083.]]\n",
      "<NDArray 10x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "for data, label in data_iter:\n",
    "    print(data, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "之前一章中，当我们从0开始训练模型时，需要先声明模型参数，然后再使用它们来构建模型。但`gluon`提供大量预定义的层，我们只需要关注使用哪些层来构建模型。例如线性模型就是使用对应的`Dense`层；之所以称为dense层，是因为输入的所有节点都与后续的节点相连。在这个例子中仅有一个输出，但在大多数后续章节中，我们会用到具有多个输出的网络。\n",
    "\n",
    "我们之后还会介绍如何构造任意结构的神经网络，但对于初学者来说，构建模型最简单的办法是利用`Sequential`来所有层串起来。输入数据之后，`Sequential`会依次执行每一层，并将前一层的输出，作为输入提供给后面的层。首先我们定义一个空的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = gluon.nn.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们加入一个`Dense`层，它唯一必须定义的参数就是输出节点的个数，在线性模型里面是1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.add(gluon.nn.Dense(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（注意这里我们并没有定义说这个层的输入节点是多少，这个在之后真正给数据的时候系统会自动赋值。我们之后会详细介绍这个特性是如何工作的。）\n",
    "\n",
    "## 初始化模型参数\n",
    "\n",
    "在使用前`net`我们必须要初始化模型权重，这里我们使用默认随机初始化方法（之后我们会介绍更多的初始化方法）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "`gluon`提供了平方误差函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "square_loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化\n",
    "\n",
    "同样我们无需手动实现随机梯度下降，我们可以创建一个`Trainer`的实例，并且将模型参数传递给它就行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(\n",
    "    net.collect_params(), 'sgd', {'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "使用`gluon`使模型训练过程更为简洁。我们不需要挨个定义相关参数、损失函数，也不需使用随机梯度下降。`gluon`的抽象和便利的优势将随着我们着手处理更多复杂模型的愈发显现。不过在完成初始设置后，训练过程本身和前面没有太多区别，唯一的不同在于我们不再是调用`SGD`，而是`trainer.step`来更新模型（此处一并省略之前绘制损失变化的折线图和散点图的过程，有兴趣的同学可以自行尝试）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average loss: nan\n",
      "Epoch 1, average loss: nan\n",
      "Epoch 2, average loss: nan\n",
      "Epoch 3, average loss: nan\n",
      "Epoch 4, average loss: nan\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 10\n",
    "for e in range(epochs):\n",
    "    total_loss = 0\n",
    "    for data, label in data_iter:\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = square_loss(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        total_loss += nd.sum(loss).asscalar()\n",
    "    print(\"Epoch %d, average loss: %f\" % (e, total_loss/num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较学到的和真实模型。我们先从`net`拿到需要的层，然后访问其权重和位移。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ nan  nan  nan  nan  nan  nan]\n",
       " [ nan  nan  nan  nan  nan  nan]]\n",
       "<NDArray 2x6 @cpu(0)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = net[0]\n",
    "dense.weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    "true_b, dense.bias.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结论\n",
    "\n",
    "可以看到`gluon`可以帮助我们更快更干净地实现模型。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "- 在训练的时候，为什么我们用了比前面要大10倍的学习率呢？（提示：可以尝试运行 `help(trainer.step)`来寻找答案。）\n",
    "- 如何拿到`weight`的梯度呢？（提示：尝试 `help(dense.weight)`）\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/742)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
