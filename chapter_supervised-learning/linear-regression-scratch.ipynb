{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归——从零开始\n",
    "\n",
    "尽管强大的深度学习框架可以减少大量重复性工作，但若过于依赖它提供的便利，你就会很难深入理解深度学习是如何工作的。因此，我们的第一个教程是如何只利用ndarray和autograd来实现一个线性回归的训练。\n",
    "\n",
    "## 线性回归\n",
    "\n",
    "给定一个数据点集合`X`和对应的目标值`y`，线性模型的目标就是找到一条使用向量`w`和位移`b`描述的线，来尽可能地近似每个样本`X[i]`和`y[i]`。用数学符号来表示就是：\n",
    "\n",
    "$$\\boldsymbol{\\hat{y}} = X \\boldsymbol{w} + b$$\n",
    "\n",
    "并最小化所有数据点上的平方误差\n",
    "\n",
    "$$\\sum_{i=1}^n (\\hat{y}_i-y_i)^2.$$\n",
    "\n",
    "你可能会对我们把古老的线性回归作为深度学习的一个样例表示奇怪。实际上线性模型是最简单、但也是最有用的神经网络。一个神经网络就是一个由节点（神经元）和有向边组成的集合。我们一般把一些节点组成层，每一层先从下面一层的节点获取输入，然后输出给上面的层使用。要计算一个节点值，我们需要将输入节点值做加权和（权数值即 `w`），然后再加上一个**激活函数（activation function）**。对于线性回归而言，它是一个两层神经网络，其中第一层是（下图橙色点）输入，每个节点对应输入数据点的一个维度，第二层是单输出节点（下图绿色点），它使用身份函数（$f(x)=x$）作为激活函数。\n",
    "\n",
    "![](../img/onelayer.png)\n",
    "\n",
    "## 创建数据集\n",
    "\n",
    "这里我们使用一个数据集来尽量简单地解释清楚，真实的模型是什么样的。具体来说，我们使用如下方法来生成数据；随机数值 `X[i]`，其相应的标注为 `y[i]`：\n",
    "\n",
    "`y[i] = 2 * X[i][0] - 3.4 * X[i][1] + 4.2 + noise`\n",
    "\n",
    "使用数学符号表示：\n",
    "\n",
    "$$y = X \\cdot w + b + \\eta, \\quad \\text{for } \\eta \\sim \\mathcal{N}(0,\\sigma^2)$$\n",
    "\n",
    "这里噪音服从均值0和标准差为0.01的正态分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 2.21220636  0.7740038 ]\n",
      " [ 1.04344046  1.18392551]\n",
      " [ 1.89171135 -1.23474145]\n",
      " ..., \n",
      " [ 0.42236066  0.36030969]\n",
      " [ 0.1177411  -1.06993568]\n",
      " [-0.03442945 -0.11181278]]\n",
      "<NDArray 1000x2 @cpu(0)>\n",
      "\n",
      "[  6.00058699e+00   2.26762176e+00   1.21922855e+01   2.19331312e+00\n",
      "   1.16779327e+01   9.45502162e-01   4.81350183e+00   3.31076193e+00\n",
      "   3.67973149e-01   4.37350655e+00   5.92764187e+00   4.89123249e+00\n",
      "   1.14703417e-01   4.89729834e+00   6.63699627e+00   3.04140449e+00\n",
      "   3.68024325e+00  -1.09772599e+00   4.24649858e+00   1.76260531e+00\n",
      "   3.12361550e+00   7.26333475e+00   2.02706981e+00   9.56701851e+00\n",
      "   7.03674841e+00   9.26425552e+00   4.29679267e-03   3.05101156e+00\n",
      "   9.62822914e+00   5.14393282e+00   2.61765623e+00   4.65436125e+00\n",
      "   4.05559874e+00  -1.06317675e+00   1.42257905e+00  -1.48423821e-01\n",
      "   4.72640228e+00   2.17679954e+00   8.60468769e+00   3.21637201e+00\n",
      "   2.36237001e+00   4.25050449e+00   8.13547325e+00   5.16207981e+00\n",
      "  -1.01195765e+00   1.29253113e+00   3.95248842e+00   7.92515612e+00\n",
      "   7.50782633e+00   2.38162780e+00  -6.62774944e+00  -2.11574531e+00\n",
      "   4.57787848e+00   2.38421607e+00   3.08520943e-01   5.98483562e+00\n",
      "   3.86126876e+00   4.92344046e+00   2.90122598e-01   5.80693483e+00\n",
      "   3.51592278e+00   1.73715746e+00   9.71273422e+00  -5.76688647e-01\n",
      "   8.89702225e+00   1.53921053e-01   2.86934710e+00   1.82194090e+00\n",
      "   8.14363956e+00   7.42253828e+00   2.27488875e+00   1.03225090e-01\n",
      "   9.29454386e-01   2.30917668e+00   9.32141590e+00   3.35138774e+00\n",
      "   9.67193317e+00  -7.92611778e-01  -8.12502876e-02   1.05593004e+01\n",
      "   1.02993953e+00   9.35839558e+00   7.06428909e+00   1.35025215e+01\n",
      "   9.78747463e+00   3.11530471e+00   8.99153042e+00   3.34914184e+00\n",
      "   9.13966745e-02   6.04922581e+00  -4.10319567e+00   2.36622167e+00\n",
      "   3.70883250e+00  -3.64806652e+00   5.56016159e+00   8.07423782e+00\n",
      "   1.02571678e+01  -3.29724145e+00   3.98174500e+00   2.73201871e+00\n",
      "   4.35635042e+00  -1.57184696e+00  -3.65555465e-01   1.35374701e+00\n",
      "   5.30461168e+00   9.66518462e-01   1.12725010e+01   7.55321264e+00\n",
      "  -8.65439028e-02   8.20743465e+00  -1.86221623e+00   5.11474609e+00\n",
      "  -4.66216516e+00   1.56233668e+00   8.67557907e+00   4.14227664e-01\n",
      "   5.77486467e+00   3.20797300e+00   5.44580126e+00   9.89664650e+00\n",
      "   6.74980021e+00   1.00209343e+00   8.51570034e+00   1.42433393e+00\n",
      "   4.32575035e+00   1.44474387e+00  -9.83616948e-01   7.40349483e+00\n",
      "  -1.72905779e+00   6.05985546e+00   5.53117800e+00   1.16507654e+01\n",
      "   5.63614178e+00   4.64012289e+00   1.29405487e+00   3.14228272e+00\n",
      "   1.21743574e+01   8.20498848e+00   6.34069538e+00   5.55995798e+00\n",
      "   1.18782930e+01   2.13838172e+00   1.24435387e+01   3.15871882e+00\n",
      "   2.62262368e+00  -2.21673155e+00   6.41084862e+00   1.14791136e+01\n",
      "   8.48192692e+00   9.62120342e+00   4.85536432e+00  -7.10032642e-01\n",
      "   4.38504028e+00   5.69396114e+00   7.11519384e+00   2.91931224e+00\n",
      "   1.36435509e+00   3.19644499e+00   6.11455297e+00   3.55946016e+00\n",
      "  -1.65116966e+00   2.07375264e+00  -5.27001977e-01   5.75022221e+00\n",
      "   4.13951397e+00  -1.49378276e+00   7.40535378e-01   5.86501169e+00\n",
      "   1.98225629e+00   4.01002645e+00   1.06874638e+01   4.76217604e+00\n",
      "   3.95448780e+00   2.88908339e+00   1.32351789e+01  -4.38445044e+00\n",
      "   6.42942095e+00   3.90420747e+00   8.41411972e+00   3.55600786e+00\n",
      "   3.69913387e+00   1.91536343e+00   2.88293600e+00   2.12675780e-01\n",
      "   4.91452932e+00   8.69855359e-02  -2.05716062e+00   8.70717907e+00\n",
      "  -7.08984137e-01   2.40901399e+00  -5.52597237e+00   6.79306793e+00\n",
      "   1.15456009e+00   8.31014538e+00  -2.62378383e+00  -1.90041220e+00\n",
      "   5.23237371e+00   6.98703671e+00   7.12111294e-01  -4.70846033e+00\n",
      "  -4.88409132e-01   3.50135136e+00   1.80808055e+00   4.27718925e+00\n",
      "   9.21588230e+00   9.37748432e+00  -1.76137888e+00   9.79548037e-01\n",
      "   1.40049624e+00   1.16494155e+00   3.16576910e+00  -1.33852196e+00\n",
      "   9.40725327e+00   1.04858851e+00   1.29105926e+00   4.27871370e+00\n",
      "   7.26528597e+00   4.68532658e+00   5.62765598e+00   2.93269873e+00\n",
      "   5.32553244e+00   6.12610054e+00   3.78052950e+00  -7.64051056e+00\n",
      "   7.78089762e+00   6.84926808e-01   5.24146557e+00   3.86367708e-01\n",
      "   8.98524189e+00  -1.94243371e+00   1.19108248e+01   6.30169392e+00\n",
      "   2.61462975e+00   7.40255070e+00   4.59340477e+00   7.03164005e+00\n",
      "   3.41441464e+00   1.28088970e+01  -6.67174935e-01   1.61928570e+00\n",
      "   6.38893175e+00   1.98895013e+00   3.17065406e+00   6.72600317e+00\n",
      "   6.99788809e-01   4.48208475e+00   3.24432778e+00   1.87119663e+00\n",
      "   5.48333931e+00  -2.03457570e+00   4.42859745e+00   7.09018898e+00\n",
      "  -9.75501776e-01  -1.73330581e+00  -2.70084476e+00   9.67674160e+00\n",
      "   9.71403790e+00   6.68439150e+00   9.87303615e-01   5.79172945e+00\n",
      "   5.18367100e+00   2.59357953e+00   2.57632899e+00  -1.88849375e-01\n",
      "  -2.36600971e+00   6.07925510e+00   5.79823351e+00   7.07915688e+00\n",
      "   8.06984234e+00   2.42178297e+00   2.77828097e+00   8.21928024e+00\n",
      "   8.59291267e+00   4.61766386e+00   8.17334175e+00   4.34976053e+00\n",
      "  -5.52946329e-01  -1.67340815e+00   1.70878735e+01  -3.96514177e+00\n",
      "  -2.86869621e+00   6.77798796e+00   1.02916451e+01   9.87615299e+00\n",
      "   5.06920481e+00   8.62150383e+00   3.68891907e+00   5.76814985e+00\n",
      "  -2.55985521e-02   7.93133676e-01   7.35282707e+00   5.28146982e+00\n",
      "  -2.08299771e-01   2.27782226e+00   2.72940779e+00  -7.39396736e-03\n",
      "   4.21506834e+00   7.16123343e+00   5.77291393e+00   1.32580786e+01\n",
      "   9.12069607e+00   2.83970833e+00   7.08258963e+00   1.09765589e+00\n",
      "   6.11318254e+00  -1.29311502e+00   9.78116512e+00   6.10187912e+00\n",
      "   3.86205363e+00  -4.70892811e+00  -9.80960429e-02   5.03779316e+00\n",
      "   1.39133859e+00   1.00550709e+01   8.90625763e+00   2.70029879e+00\n",
      "   9.40862373e-02   7.39991236e+00   4.42113304e+00   3.10426426e+00\n",
      "   4.37246084e+00   1.97206652e+00   6.70738220e+00  -1.71017110e+00\n",
      "   9.99263382e+00   9.87891102e+00   3.79343486e+00  -1.52539301e+00\n",
      "   9.88493061e+00   6.31597376e+00   1.78410268e+00   7.09890079e+00\n",
      "   8.55360508e+00   4.69854689e+00  -3.79908949e-01   6.90137768e+00\n",
      "  -1.48972407e-01   3.18764389e-01  -3.76936579e+00   5.97870874e+00\n",
      "   4.57754359e-02  -4.49842274e-01   5.97232008e+00   8.98420334e+00\n",
      "   3.92714834e+00  -8.12351465e-01   9.19868088e+00   7.98897839e+00\n",
      "  -3.32293463e+00   5.89508963e+00   3.69898033e+00   2.99195737e-01\n",
      "   5.72280836e+00   7.18671417e+00  -1.09479618e+00   5.42730379e+00\n",
      "   8.18839073e+00   7.61334229e+00   5.09120703e+00   5.25908184e+00\n",
      "   7.29651594e+00   4.60882616e+00   2.85096073e+00  -4.16522592e-01\n",
      "  -1.34369612e+00   1.84286082e+00   3.52616143e+00   1.53494728e+00\n",
      "   2.48744869e+00   1.17669115e+01  -1.06930697e+00   1.92389178e+00\n",
      "  -1.30710232e+00   1.74281847e+00   8.92420292e+00   1.28497858e+01\n",
      "   7.97057927e-01   4.66987896e+00   1.84583116e+00  -2.54839611e+00\n",
      "   4.76477957e+00   5.47918653e+00   8.14660263e+00  -7.92679846e-01\n",
      "   2.65141845e+00  -1.75450057e-01   9.05238533e+00   7.67654753e+00\n",
      "  -3.12142074e-01   5.21575594e+00   9.73895454e+00   4.07446909e+00\n",
      "  -2.27614570e+00   6.99145937e+00  -2.92020679e+00   1.11757922e+00\n",
      "   8.66250610e+00   4.90554667e+00   1.29007826e+01   9.89151001e+00\n",
      "   3.32292080e+00  -1.56150401e+00   1.61185093e+01  -1.22812700e+00\n",
      "   3.78411531e+00   8.80790043e+00   5.80803633e+00   9.12265587e+00\n",
      "   5.15670633e+00   6.49900579e+00  -1.66377455e-01   4.96885157e+00\n",
      "   4.77802181e+00   2.43111348e+00   4.72880602e+00   6.01344442e+00\n",
      "   8.33612823e+00   5.25129795e+00   3.73932242e+00   7.50471473e-01\n",
      "   2.60920477e+00   1.55012789e+01   7.29501104e+00   5.19673347e-01\n",
      "  -7.84326732e-01   4.92062283e+00  -4.86394882e+00   1.13254941e+00\n",
      "   5.42621803e+00  -2.18518162e+00   3.19179320e+00   2.00944257e+00\n",
      "   8.29669380e+00   8.89814949e+00   1.47966928e+01   4.82637691e+00\n",
      "   8.54947662e+00   4.20721912e+00   1.17988863e+01   7.05946207e+00\n",
      "   5.78997183e+00   3.53572321e+00   6.87929535e+00   3.80280733e+00\n",
      "   6.01859283e+00   8.65779781e+00   8.96641541e+00   3.49980664e+00\n",
      "  -6.27427995e-01   4.51252556e+00   5.65439558e+00  -2.83031404e-01\n",
      "   3.91674072e-01  -2.05255270e+00   3.03777957e+00  -9.21066105e-01\n",
      "  -1.89065230e+00   1.12429304e+01   5.70039415e+00   4.87603617e+00\n",
      "   6.59181404e+00  -3.59120798e+00   1.11462650e+01   5.47595358e+00\n",
      "   8.01419449e+00  -4.01379585e+00   6.27078485e+00  -3.46467543e+00\n",
      "   1.19839678e+01   1.05411549e+01   8.46215153e+00   3.45131230e+00\n",
      "   1.17900813e+00   1.13934100e+00   6.39366102e+00   2.31259346e+00\n",
      "   1.90567887e+00   2.84408903e+00   1.73253512e+00   7.05463696e+00\n",
      "   2.34412599e+00   1.13427277e+01   1.11918306e+01  -2.38636637e+00\n",
      "  -5.76642942e+00   2.95453906e+00   3.13316488e+00   1.33436632e+01\n",
      "  -1.61392677e+00   4.12615252e+00   2.52690601e+00   6.59469557e+00\n",
      "   4.41452789e+00  -5.26043661e-02   1.37078381e+00   3.54393268e+00\n",
      "   3.60916424e+00   4.56775278e-01   1.83015716e+00  -2.38790965e+00\n",
      "  -3.88058662e+00   1.12608595e+01   3.21273947e+00  -4.56589222e+00\n",
      "   5.12437916e+00  -4.23733759e+00  -5.31485140e-01   3.89248991e+00\n",
      "   1.13409281e+00   3.50865316e+00  -8.29684854e-01   5.21893072e+00\n",
      "  -3.24041057e+00   9.34037876e+00  -1.20608056e+00  -2.67114639e+00\n",
      "   5.00975561e+00   7.12491941e+00   2.16426301e+00   5.05239058e+00\n",
      "   3.17869473e+00   7.75799394e-01   8.88365173e+00   8.21045494e+00\n",
      "  -1.57305002e-01   3.27913947e-02   4.87408876e+00  -1.76639521e+00\n",
      "   4.42936563e+00   8.73447132e+00   1.24195395e+01   5.38932848e+00\n",
      "   5.46872282e+00   7.21341658e+00   3.57704949e+00   7.77978754e+00\n",
      "   6.14234972e+00   5.88771582e+00  -4.70794773e+00   3.06713271e+00\n",
      "   3.66242433e+00   5.07093716e+00   8.94575024e+00   1.70424551e-01\n",
      "   5.36111784e+00   2.12545276e+00   4.96618080e+00   9.04986858e+00\n",
      "   2.77093434e+00   1.52540264e+01   1.01800251e+00   1.85924375e+00\n",
      "   3.22899032e+00   6.47142410e-01   4.44664669e+00   6.40372467e+00\n",
      "  -8.90121460e-01  -1.14217043e+01  -9.62112188e-01   6.43423367e+00\n",
      "   2.38298106e+00   8.90343070e-01   3.86103249e+00   2.87050509e+00\n",
      "   6.68229628e+00   6.91686440e+00   7.31231928e+00   1.25544329e+01\n",
      "   4.79142618e+00   2.47755694e+00   1.03928909e+01   2.50096440e+00\n",
      "   8.01118183e+00   7.95408964e-01   9.47745144e-01   4.27281237e+00\n",
      "   7.54309368e+00   2.36477113e+00   1.44110608e+00   4.03328657e-01\n",
      "   3.47709560e+00   1.15732059e+01   4.61085653e+00   1.18035686e+00\n",
      "   4.75159025e+00   1.83634818e-01   6.13815975e+00   5.34305954e+00\n",
      "   6.06442308e+00   1.54914503e+01  -1.45452106e+00   9.08171654e+00\n",
      "   5.41019535e+00  -1.42846990e+00   5.82623625e+00   4.64837599e+00\n",
      "  -6.76413000e-01   4.96319473e-01   2.58162022e+00   4.05962199e-01\n",
      "  -7.97343135e-01  -1.59934902e+00  -1.27362907e+00   2.71067202e-01\n",
      "  -5.16236210e+00   3.96879983e+00  -3.10759276e-01   2.55846810e+00\n",
      "   1.07698793e+01   7.54887772e+00   2.19294548e+00   2.32561326e+00\n",
      "  -7.00254381e-01   3.53053069e+00   6.03366733e-01   6.33279324e+00\n",
      "   1.56030500e+00   9.78838539e+00   3.42146248e-01   3.16287518e+00\n",
      "   3.36411524e+00   4.67184514e-01   7.40979671e+00   4.70935440e+00\n",
      "  -5.50788403e-01   1.18096352e+00  -2.28434110e+00   3.91800594e+00\n",
      "   1.17673140e+01   4.46843958e+00   4.54620600e+00   3.49116659e+00\n",
      "   9.23867798e+00   4.40550089e+00   2.63797426e+00   6.40504980e+00\n",
      "   5.64667845e+00   3.95424938e+00   7.22414446e+00   1.37665138e+01\n",
      "   7.63640976e+00  -2.39337754e+00   9.46392727e+00   1.50527039e+01\n",
      "   1.11008215e+01  -5.46222687e-01   3.85465860e+00   7.37199306e+00\n",
      "   1.86128393e-01   8.87740326e+00   5.68471813e+00   5.96836281e+00\n",
      "   1.50849080e+00   7.13710117e+00  -7.81189799e-01   7.10845709e+00\n",
      "   8.04768944e+00   2.78509569e+00  -6.23702645e-01   8.69002247e+00\n",
      "   7.42936659e+00  -1.98298836e+00   2.69949102e+00  -1.38269782e+00\n",
      "   1.96533954e+00   6.40672398e+00   1.41371882e+00   4.37562990e+00\n",
      "  -3.81485581e+00   4.45923281e+00   6.33250284e+00  -2.81465912e+00\n",
      "  -7.75278091e-01   9.42146873e+00   4.81382465e+00   7.12984419e+00\n",
      "   2.26032662e+00   9.19952214e-01   3.97893429e+00   7.64369106e+00\n",
      "   4.40580988e+00   1.89279211e+00  -4.00925732e+00   1.08413637e+00\n",
      "   1.03429489e+01   4.94102526e+00   7.62891722e+00   1.54842710e+00\n",
      "   3.84540486e+00   6.19823873e-01  -6.87966824e-01   6.77848387e+00\n",
      "  -3.48340917e+00  -4.11237240e-01   6.62664413e+00   2.26893950e+00\n",
      "   3.05684471e+00   8.52682495e+00   6.20203733e+00   3.18180585e+00\n",
      "   4.13710546e+00   5.78279591e+00   1.21527205e+01   3.40946674e+00\n",
      "   4.28683424e+00   2.63194728e+00  -1.22866189e+00  -5.01083565e+00\n",
      "   5.08834791e+00   7.10330820e+00   3.18057835e-01   2.72989154e+00\n",
      "  -2.02247977e+00   2.94114065e+00   8.72872734e+00   7.00197601e+00\n",
      "  -1.12015629e+00   2.53477621e+00  -1.13211587e-01   2.90651035e+00\n",
      "   1.72551680e+00   4.20741510e+00   7.32196760e+00   7.46077585e+00\n",
      "  -3.24618196e+00   3.73270988e+00   6.85699129e+00  -2.22571969e+00\n",
      "   6.54470444e+00   4.70553923e+00   3.17774701e+00  -1.64391470e+00\n",
      "   6.01217413e+00   9.06552029e+00   3.92091894e+00   3.85747463e-01\n",
      "   3.33713627e+00   1.01704443e+00   6.36459208e+00   9.12652016e+00\n",
      "   2.88303924e+00   9.40188503e+00   3.62823081e+00   7.61170244e+00\n",
      "   8.14017677e+00   5.81346989e+00   4.59084320e+00   1.68221915e+00\n",
      "   2.76216483e+00   8.03220177e+00   4.14855099e+00   3.51394248e+00\n",
      "   7.91679859e+00   5.50112820e+00   3.37374300e-01  -2.17151999e-01\n",
      "   3.16113949e+00   5.87866735e+00   7.57665873e+00   1.42904925e+00\n",
      "  -3.02141833e+00   3.16073442e+00   4.94941854e+00   5.45097303e+00\n",
      "   4.62332296e+00   2.15696359e+00   2.92299676e+00   8.15139961e+00\n",
      "   5.79070854e+00   1.29249945e+01   7.91227579e+00   6.85617256e+00\n",
      "   1.18015518e+01   5.35112286e+00   5.03606617e-01  -7.71149695e-01\n",
      "   4.74106836e+00   4.49377251e+00   5.55013084e+00  -2.27074131e-01\n",
      "  -2.28026956e-02   4.79263592e+00  -4.03558493e+00   9.42656803e+00\n",
      "   4.61896706e+00   2.56061864e+00   5.11962461e+00   3.31743431e+00\n",
      "   2.81659317e+00   7.02215433e+00   1.69073433e-01   7.39950609e+00\n",
      "   4.81620407e+00   1.30521383e+01  -2.27135062e+00   5.60687542e+00\n",
      "   7.40121746e+00  -1.29783833e+00  -7.78524637e-01   2.05339015e-01\n",
      "  -2.78285122e+00   9.45749187e+00   2.86690354e-01   1.08132148e+00\n",
      "   5.73208857e+00   6.60449409e+00   2.79000473e+00  -3.06508279e+00\n",
      "   6.29938459e+00  -1.83509505e+00   1.05566442e+00  -6.27610922e+00\n",
      "   8.10410023e+00  -1.58256614e+00   4.95688438e+00   1.50959146e+00\n",
      "   8.27841222e-01  -1.96112406e+00   1.61399984e+00   1.28002806e+01\n",
      "   5.47470808e+00  -3.82210426e-02   4.59931612e+00   1.69119096e+00\n",
      "   2.75938034e+00   1.94756258e+00   2.04827368e-01  -4.99723864e+00\n",
      "   2.70537806e+00   2.47587609e+00   8.51377487e+00   3.51317787e+00\n",
      "   5.98254395e+00   8.75278854e+00   9.42298222e+00  -1.39522910e+00\n",
      "   7.10365915e+00   9.13641071e+00   3.30985808e+00   1.20273376e+00\n",
      "   6.42886543e+00   8.04833603e+00   9.21890640e+00   3.51106358e+00\n",
      "  -1.61434817e+00  -8.19816351e-01   9.60223007e+00   1.83850694e+00\n",
      "   8.47514153e+00   9.56686497e+00   3.75156140e+00   4.76806402e+00\n",
      "  -4.87159282e-01  -9.73066270e-01   2.62849760e+00   5.16971922e+00\n",
      "   2.67071676e+00   2.82153869e+00   6.57980204e+00   5.77237797e+00\n",
      "   8.94351065e-01   4.76957941e+00   9.86030197e+00   1.66023350e+00\n",
      "   2.27282310e+00   2.11969614e+00   5.85545254e+00  -1.12412035e+00\n",
      "   1.75792730e+00   1.11243181e+01   6.69076061e+00   1.33084881e+00\n",
      "   2.41006303e+00   8.93000793e+00   4.59440321e-01   2.49031067e+00\n",
      "   8.34835339e+00  -2.51507461e-01   5.62525082e+00   4.98562527e+00\n",
      "   7.32537222e+00   1.93068516e+00   8.54326725e+00   3.40901208e+00\n",
      "   3.53382730e+00   3.86360836e+00   5.32632971e+00   5.64660168e+00\n",
      "  -7.82870817e+00   1.14821637e+00   1.21908722e+01  -4.03629124e-01\n",
      "   7.22247267e+00   8.11946201e+00   2.63211989e+00   1.59613323e+00\n",
      "   1.53097463e+00   1.38983946e+01   1.05833712e+01   4.46778917e+00\n",
      "   3.84101677e+00   3.40346050e+00   7.15676975e+00  -2.64578891e+00\n",
      "   9.79193270e-01   1.89632845e+00   2.60853720e+00   5.88136578e+00\n",
      "   3.32437843e-01   6.11946058e+00  -6.56673551e-01  -2.28252769e+00\n",
      "   1.09371023e+01   2.06014180e+00   2.55328774e+00   7.00869608e+00\n",
      "   4.68266487e+00   3.18317318e+00   5.89429522e+00   3.74910378e+00\n",
      "   3.26037049e+00   1.41695118e+00   2.08151150e+00  -2.12272429e+00\n",
      "   4.63486099e+00   1.02467260e+01   2.56387091e+00   1.34774256e+01\n",
      "   2.44735646e+00   4.29428959e+00  -4.49032672e-02   6.32208109e+00\n",
      "   3.96009493e+00  -3.43451411e-01   2.19488525e+00   3.66045594e+00\n",
      "   7.80727565e-01   1.18955116e+01   2.37387371e+00   6.26431084e+00\n",
      "   1.54112501e+01   2.03872871e+00   8.31858826e+00  -2.41219449e+00\n",
      "   1.18664322e+01   9.92855930e+00   7.76466250e-01   3.77529478e+00\n",
      "   7.99683189e+00   9.24902439e+00   4.95075274e+00   6.00424242e+00\n",
      "   2.39674401e+00   7.69683599e+00   3.14462352e+00   1.35459490e+01\n",
      "  -1.40117180e+00   5.39461710e-02   8.94194221e+00   5.40881586e+00\n",
      "   2.28203773e+00   4.71127129e+00   1.43572879e+00   3.07920158e-01\n",
      "   5.89453650e+00   1.00337210e+01   4.99126530e+00   9.47961426e+00\n",
      "   7.47742796e+00  -3.22903466e+00   1.70430601e+00   7.14670324e+00\n",
      "   6.83804941e+00  -1.11288345e+00   1.00667439e+01   5.59296465e+00\n",
      "   4.81434011e+00   7.37973213e+00   6.85965109e+00   1.05132370e+01\n",
      "   5.67194462e+00   4.78513193e+00   2.73524332e+00  -1.85216641e+00\n",
      "   3.36110044e+00   3.35283446e+00   2.19889474e+00   4.68619013e+00\n",
      "  -2.41917205e+00   9.58864033e-01   6.34757566e+00   1.80053329e+00\n",
      "   3.82795906e+00  -2.00560665e+00   4.07729626e+00   6.07371902e+00\n",
      "  -4.61031616e-01   3.81517696e+00   8.06125736e+00   4.51572323e+00]\n",
      "<NDArray 1000 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "\n",
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "\n",
    "X = nd.random_normal(shape=(num_examples, num_inputs))\n",
    "y = true_w[0] * X[:, 0] + true_w[1] * X[:, 1] + true_b\n",
    "y += .01 * nd.random_normal(shape=y.shape)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到`X`的每一行是一个长度为2的向量，而`y`的每一行是一个长度为1的向量（标量）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 2.21220636  0.7740038 ]\n",
      "<NDArray 2 @cpu(0)> \n",
      "[ 6.00058699]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(X[0], y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果有兴趣，可以使用安装包中已包括的 Python 绘图包 `matplotlib`，生成第二个特征值 (`X[:, 1]`) 和目标值 `Y` 的散点图，更直观地观察两者间的关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feec75c5908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:, 1].asnumpy(),y.asnumpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取\n",
    "\n",
    "当我们开始训练神经网络的时候，我们需要不断读取数据块。这里我们定义一个函数它每次返回`batch_size`个随机的样本和对应的目标。我们通过python的`yield`来构造一个迭代器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "batch_size = 10\n",
    "def data_iter():\n",
    "    # 产生一个随机索引\n",
    "    idx = list(range(num_examples))\n",
    "    random.shuffle(idx)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = nd.array(idx[i:min(i+batch_size,num_examples)])\n",
    "        yield nd.take(X, j), nd.take(y, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面代码读取第一个随机数据块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-0.9906556   0.1680257 ]\n",
      " [ 0.79784435  0.4279041 ]\n",
      " [-0.54206413  1.07188106]\n",
      " [-0.54291499  1.29740858]\n",
      " [ 0.12767315 -0.3437213 ]\n",
      " [-0.67570597 -1.05316365]\n",
      " [-0.69309121 -0.68236506]\n",
      " [-0.5013274  -1.67310596]\n",
      " [ 0.52217364 -0.96701854]\n",
      " [ 0.29560572 -1.21692061]]\n",
      "<NDArray 10x2 @cpu(0)> \n",
      "[ 1.6602335   4.35635042 -0.53148514 -1.29311502  5.63614178  6.42942095\n",
      "  5.12437916  8.87740326  8.52682495  8.94194221]\n",
      "<NDArray 10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "for data, label in data_iter():\n",
    "    print(data, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型参数\n",
    "\n",
    "下面我们随机初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = nd.random_normal(shape=(num_inputs, 1))\n",
    "b = nd.zeros((1,))\n",
    "params = [w, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后训练时我们需要对这些参数求导来更新它们的值，使损失尽量减小；因此我们需要创建它们的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "线性模型就是将输入和模型的权重（`w`）相乘，再加上偏移（`b`）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "#     print(X.shape)\n",
    "#     print(w.shape)\n",
    "#     print(nd.dot(X, w).shape)\n",
    "    return nd.dot(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "我们使用常见的平方误差来衡量预测目标和真实目标之间的差距。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square_loss(yhat, y):\n",
    "    # 注意这里我们把y变形成yhat的形状来避免矩阵形状的自动转换\n",
    "    return (yhat - y.reshape(yhat.shape)) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化\n",
    "\n",
    "虽然线性回归有显式解，但绝大部分模型并没有。所以我们这里通过随机梯度下降来求解。每一步，我们将模型参数沿着梯度的反方向走特定距离，这个距离一般叫**学习率（learning rate）** `lr`。（我们会之后一直使用这个函数，我们将其保存在[utils.py](../utils.py)。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        print(param.grad)\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "现在我们可以开始训练了。训练通常需要迭代数据数次，在这里使用`epochs`表示迭代总次数；一次迭代中，我们每次随机读取固定数个数据点，计算梯度并更新模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 模型函数\n",
    "def real_fn(X):\n",
    "    return true_w[0] * X[:, 0] + true_w[1] * X[:, 1] + true_b\n",
    "# 绘制损失随训练次数降低的折线图，以及预测值和真实值的散点图\n",
    "def plot(losses, X, sample_size=100):\n",
    "    xs = list(range(len(losses)))\n",
    "    f, (fg1, fg2) = plt.subplots(1, 2)\n",
    "    fg1.set_title('Loss during training')\n",
    "    fg1.plot(xs, losses, '-r')\n",
    "    fg2.set_title('Estimated vs real function')\n",
    "    fg2.plot(X[:sample_size, 1].asnumpy(),\n",
    "             net(X[:sample_size, :]).asnumpy(), 'or', label='Estimated')\n",
    "    fg2.plot(X[:sample_size, 1].asnumpy(),\n",
    "             real_fn(X[:sample_size, :]).asnumpy(), '*g', label='Real')\n",
    "    fg2.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-42.62696838]\n",
      " [ 22.99453545]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-83.30425262]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 1. Moving avg of loss: 25.0850715637. Average loss: 0.250851\n",
      "\n",
      "[[-31.33101273]\n",
      " [ 69.46697998]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-89.13998413]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 2. Moving avg of loss: 25.9474624749. Average loss: 0.518863\n",
      "\n",
      "[[-62.18538666]\n",
      " [ 70.70920563]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-78.05757141]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 3. Moving avg of loss: 26.7736844651. Average loss: 0.802877\n",
      "\n",
      "[[-24.2500248 ]\n",
      " [ 15.96902561]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-64.31415558]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 4. Moving avg of loss: 24.2633632186. Average loss: 0.971697\n",
      "\n",
      "[[-23.09512711]\n",
      " [ -8.23345852]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-66.72548676]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 5. Moving avg of loss: 22.5289004203. Average loss: 1.129325\n",
      "\n",
      "[[-60.09641266]\n",
      " [ -7.57219839]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-65.38860321]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 6. Moving avg of loss: 22.1600155335. Average loss: 1.333027\n",
      "\n",
      "[[-12.10704613]\n",
      " [ 36.44000626]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-50.7703476]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 7. Moving avg of loss: 20.8049307505. Average loss: 1.462570\n",
      "\n",
      "[[-40.80815125]\n",
      " [ 55.26136017]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-69.10778046]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 8. Moving avg of loss: 20.8194363361. Average loss: 1.671740\n",
      "\n",
      "[[-83.74175262]\n",
      " [  7.67040682]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-81.13935852]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 9. Moving avg of loss: 21.4377470503. Average loss: 1.933408\n",
      "\n",
      "[[-76.41144562]\n",
      " [ 53.32946014]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-74.96559143]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 10. Moving avg of loss: 21.8602276521. Average loss: 2.188182\n",
      "\n",
      "[[-32.86351776]\n",
      " [ -3.48077154]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-55.86949158]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 11. Moving avg of loss: 21.0768358061. Average loss: 2.324793\n",
      "\n",
      "[[-81.7784729 ]\n",
      " [ 71.05651093]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-90.26041412]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 12. Moving avg of loss: 21.7248890526. Average loss: 2.609190\n",
      "\n",
      "[[-41.42614746]\n",
      " [-39.37747955]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-39.70108032]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 13. Moving avg of loss: 20.7673869599. Average loss: 2.709165\n",
      "\n",
      "[[-15.40121078]\n",
      " [ 32.17044449]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-60.37146759]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 14. Moving avg of loss: 20.178826537. Average loss: 2.839588\n",
      "\n",
      "[[-40.67755127]\n",
      " [ -4.08992195]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-63.646595]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 15. Moving avg of loss: 19.795965678. Average loss: 2.987798\n",
      "\n",
      "[[-32.57003784]\n",
      " [ 23.17578316]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-67.50840759]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 16. Moving avg of loss: 19.4914369152. Average loss: 3.140522\n",
      "\n",
      "[[-29.77101326]\n",
      " [ -7.5572772 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-40.79290009]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 17. Moving avg of loss: 18.8467093682. Average loss: 3.234178\n",
      "\n",
      "[[-27.54059219]\n",
      " [ 46.48493195]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-74.69745636]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 18. Moving avg of loss: 18.6902198297. Average loss: 3.396748\n",
      "\n",
      "[[-26.57987595]\n",
      " [  9.55135632]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-54.59941101]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 19. Moving avg of loss: 18.269762348. Average loss: 3.510561\n",
      "\n",
      "[[-30.07402992]\n",
      " [-39.38851166]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-43.10916519]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 20. Moving avg of loss: 17.7148339265. Average loss: 3.592210\n",
      "\n",
      "[[-6.89874649]\n",
      " [ 9.24227524]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-50.75804901]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 21. Moving avg of loss: 17.2256323161. Average loss: 3.676277\n",
      "\n",
      "[[-49.37652969]\n",
      " [  2.37418723]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-59.06348801]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 22. Moving avg of loss: 17.0473844639. Average loss: 3.813175\n",
      "\n",
      "[[-19.55846214]\n",
      " [ 10.51036549]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-31.48723221]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 23. Moving avg of loss: 16.5493166306. Average loss: 3.880854\n",
      "\n",
      "[[-27.70716286]\n",
      " [  6.19327641]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-41.55632019]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 24. Moving avg of loss: 16.1858751228. Average loss: 3.968454\n",
      "\n",
      "[[-38.10133362]\n",
      " [-14.49318981]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-36.86777496]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 25. Moving avg of loss: 15.8315183809. Average loss: 4.051582\n",
      "\n",
      "[[ -9.33762455]\n",
      " [ 45.45249939]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-47.00671387]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 26. Moving avg of loss: 15.5282530896. Average loss: 4.140160\n",
      "\n",
      "[[-58.53044128]\n",
      " [ -9.6982336 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-55.39957428]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 27. Moving avg of loss: 15.409585151. Average loss: 4.267240\n",
      "\n",
      "[[-34.49823761]\n",
      " [ 22.87238884]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-66.34127045]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 28. Moving avg of loss: 15.2960179807. Average loss: 4.393480\n",
      "\n",
      "[[-63.12573242]\n",
      " [ 99.29164886]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-71.81298065]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 29. Moving avg of loss: 15.4130129316. Average loss: 4.576020\n",
      "\n",
      "[[-89.23168182]\n",
      " [ -0.18733196]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-60.9049263]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 30. Moving avg of loss: 15.4165227935. Average loss: 4.731063\n",
      "\n",
      "[[-33.52258682]\n",
      " [-11.48326969]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-52.23985672]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 31. Moving avg of loss: 15.1666570556. Average loss: 4.818340\n",
      "\n",
      "[[-3.29750228]\n",
      " [-2.1022892 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-16.43711662]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 32. Moving avg of loss: 14.6920860943. Average loss: 4.839491\n",
      "\n",
      "[[-82.09667969]\n",
      " [ 46.96648788]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-51.35342407]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 33. Moving avg of loss: 14.6756258862. Average loss: 4.981765\n",
      "\n",
      "[[-54.52051926]\n",
      " [ 19.21407127]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-63.32702255]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 34. Moving avg of loss: 14.5842257654. Average loss: 5.102066\n",
      "\n",
      "[[-93.25069427]\n",
      " [ 44.28538513]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-68.20954895]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 35. Moving avg of loss: 14.6276428021. Average loss: 5.260784\n",
      "\n",
      "[[-24.4713726 ]\n",
      " [ -2.79181242]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-41.33427429]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 36. Moving avg of loss: 14.3463225088. Average loss: 5.321655\n",
      "\n",
      "[[-50.25865173]\n",
      " [ -5.21505213]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-46.68711472]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 37. Moving avg of loss: 14.1520093832. Average loss: 5.404774\n",
      "\n",
      "[[-29.52217865]\n",
      " [-12.9379282 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-40.83995819]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 38. Moving avg of loss: 13.8911273025. Average loss: 5.463478\n",
      "\n",
      "[[-30.24579048]\n",
      " [ -7.74551392]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-26.88370514]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 39. Moving avg of loss: 13.6023399467. Average loss: 5.508744\n",
      "\n",
      "[[-25.0459938 ]\n",
      " [-15.12809372]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-33.11849594]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 40. Moving avg of loss: 13.3279421995. Average loss: 5.553934\n",
      "\n",
      "[[-22.65939522]\n",
      " [ 16.90398788]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-39.20711517]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 41. Moving avg of loss: 13.1015619015. Average loss: 5.610761\n",
      "\n",
      "[[-0.07788406]\n",
      " [ 2.75541401]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-30.76549339]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 42. Moving avg of loss: 12.8075123527. Average loss: 5.640523\n",
      "\n",
      "[[ -4.08754492]\n",
      " [ 10.46666336]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-31.83358765]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 43. Moving avg of loss: 12.5415026457. Average loss: 5.675256\n",
      "\n",
      "[[-18.60795212]\n",
      " [ 26.6753788 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-32.80892563]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 44. Moving avg of loss: 12.3255788431. Average loss: 5.723503\n",
      "\n",
      "[[-41.02352142]\n",
      " [ 24.16365051]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-38.49201965]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 45. Moving avg of loss: 12.1681220878. Average loss: 5.789473\n",
      "\n",
      "[[-41.35968399]\n",
      " [ 31.75270653]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-48.19820786]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 46. Moving avg of loss: 12.0405022637. Average loss: 5.863913\n",
      "\n",
      "[[-21.1117115 ]\n",
      " [ -6.94366026]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-25.79593849]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 47. Moving avg of loss: 11.8083213108. Average loss: 5.896908\n",
      "\n",
      "[[-21.39763832]\n",
      " [-15.53918552]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-31.66382599]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 48. Moving avg of loss: 11.5930557291. Average loss: 5.932607\n",
      "\n",
      "[[-22.98881912]\n",
      " [ -6.71624708]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-31.63322449]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 49. Moving avg of loss: 11.3914330642. Average loss: 5.970130\n",
      "\n",
      "[[-24.68037987]\n",
      " [ 10.01343346]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-32.36473846]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 50. Moving avg of loss: 11.2093485664. Average loss: 6.012122\n",
      "\n",
      "[[-61.33953094]\n",
      " [ 10.95752621]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-51.05064774]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 51. Moving avg of loss: 11.119573644. Average loss: 6.088211\n",
      "\n",
      "[[-26.96738434]\n",
      " [  9.24496841]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-39.65345001]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 52. Moving avg of loss: 10.9595316294. Average loss: 6.134265\n",
      "\n",
      "[[-60.9977684 ]\n",
      " [  3.66039681]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-33.48508072]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 53. Moving avg of loss: 10.8302688833. Average loss: 6.190479\n",
      "\n",
      "[[ -2.4532156 ]\n",
      " [ 10.40916824]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-24.66330338]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 54. Moving avg of loss: 10.6222994739. Average loss: 6.211677\n",
      "\n",
      "[[-13.55294132]\n",
      " [ -4.14840651]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-12.24188805]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 55. Moving avg of loss: 10.4056575039. Average loss: 6.225904\n",
      "\n",
      "[[-19.80442429]\n",
      " [  2.68361592]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-26.47136116]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 56. Moving avg of loss: 10.2299900052. Average loss: 6.254354\n",
      "\n",
      "[[-19.21599388]\n",
      " [ 35.27774811]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-33.69582367]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 57. Moving avg of loss: 10.0851454846. Average loss: 6.293488\n",
      "\n",
      "[[-1.96507335]\n",
      " [ 0.17713617]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-21.21032524]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 58. Moving avg of loss: 9.89149522023. Average loss: 6.308797\n",
      "\n",
      "[[-28.2992382 ]\n",
      " [  2.69716859]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-28.5912056]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 59. Moving avg of loss: 9.74219863964. Average loss: 6.340930\n",
      "\n",
      "[[-13.61947346]\n",
      " [  6.03560925]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-24.92542458]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 60. Moving avg of loss: 9.57818361701. Average loss: 6.364078\n",
      "\n",
      "[[-6.61636209]\n",
      " [ 6.83440924]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-22.60595703]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 61. Moving avg of loss: 9.40953343771. Average loss: 6.382565\n",
      "\n",
      "[[ -5.52352905]\n",
      " [ 12.70284081]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-26.08085632]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 62. Moving avg of loss: 9.25189351642. Average loss: 6.403558\n",
      "\n",
      "[[-19.36759758]\n",
      " [ -2.17968297]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-25.34483337]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 63. Moving avg of loss: 9.10440579169. Average loss: 6.426891\n",
      "\n",
      "[[-17.08003998]\n",
      " [ 24.06496239]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-29.16176605]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 64. Moving avg of loss: 8.97272782542. Average loss: 6.455467\n",
      "\n",
      "[[-20.76458549]\n",
      " [ -0.48812726]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-20.16223907]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 65. Moving avg of loss: 8.82741542907. Average loss: 6.475494\n",
      "\n",
      "[[ -5.4357686 ]\n",
      " [ 11.73509026]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-18.0039196]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 66. Moving avg of loss: 8.67498206086. Average loss: 6.489859\n",
      "\n",
      "[[-58.55801392]\n",
      " [ 21.1789875 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-34.60891724]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 67. Moving avg of loss: 8.59043712724. Average loss: 6.535180\n",
      "\n",
      "[[-18.24164963]\n",
      " [ 16.90667343]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-23.20837212]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 68. Moving avg of loss: 8.46075364745. Average loss: 6.556876\n",
      "\n",
      "[[-5.89645815]\n",
      " [ 5.3909483 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-16.33629036]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 69. Moving avg of loss: 8.31482725955. Average loss: 6.568497\n",
      "\n",
      "[[-6.85423374]\n",
      " [ 8.73011398]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-21.68304634]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 70. Moving avg of loss: 8.18004326094. Average loss: 6.583558\n",
      "\n",
      "[[-22.12404633]\n",
      " [  0.63857824]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-24.57422447]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 71. Moving avg of loss: 8.05948769245. Average loss: 6.603861\n",
      "\n",
      "[[ 1.53095555]\n",
      " [-3.15829372]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-11.51480865]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 72. Moving avg of loss: 7.91264220092. Average loss: 6.608830\n",
      "\n",
      "[[-11.67456818]\n",
      " [ 10.51521683]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-22.59751511]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 73. Moving avg of loss: 7.79225244275. Average loss: 6.625370\n",
      "\n",
      "[[-9.44301033]\n",
      " [ 0.65354723]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-18.72324944]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 74. Moving avg of loss: 7.66732908795. Average loss: 6.637751\n",
      "\n",
      "[[-30.24349022]\n",
      " [ 13.66320419]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-28.47846031]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 75. Moving avg of loss: 7.5698299457. Average loss: 6.662807\n",
      "\n",
      "[[-18.49229431]\n",
      " [ -5.27791834]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-24.97917747]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 76. Moving avg of loss: 7.45946898463. Average loss: 6.679559\n",
      "\n",
      "[[-9.40618229]\n",
      " [-8.52514076]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-16.86635399]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 77. Moving avg of loss: 7.33864013956. Average loss: 6.689054\n",
      "\n",
      "[[-11.2852087 ]\n",
      " [ 17.86820793]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-23.04702187]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 78. Moving avg of loss: 7.23259899797. Average loss: 6.704818\n",
      "\n",
      "[[-12.67850876]\n",
      " [  4.24219418]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-17.74689674]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 79. Moving avg of loss: 7.12219977253. Average loss: 6.716650\n",
      "\n",
      "[[-17.87194443]\n",
      " [  0.39352888]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-20.88387489]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 80. Moving avg of loss: 7.01864480625. Average loss: 6.730661\n",
      "\n",
      "[[-13.83780289]\n",
      " [ -7.34682465]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-17.96583748]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 81. Moving avg of loss: 6.91137541736. Average loss: 6.741103\n",
      "\n",
      "[[-6.63434792]\n",
      " [ 7.16221619]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-11.5386982]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 82. Moving avg of loss: 6.80115594939. Average loss: 6.748342\n",
      "\n",
      "[[-15.34654236]\n",
      " [  6.57809305]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-20.61557007]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 83. Moving avg of loss: 6.7037607765. Average loss: 6.761250\n",
      "\n",
      "[[-12.61236382]\n",
      " [ -0.24083813]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-17.66194725]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 84. Moving avg of loss: 6.60375002161. Average loss: 6.771271\n",
      "\n",
      "[[ 1.38841939]\n",
      " [ 0.44734114]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-5.52173805]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 85. Moving avg of loss: 6.49200975496. Average loss: 6.773123\n",
      "\n",
      "[[ 2.80429578]\n",
      " [ 7.79211092]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-12.64215851]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 86. Moving avg of loss: 6.38838004337. Average loss: 6.778076\n",
      "\n",
      "[[-10.32955551]\n",
      " [  2.2417419 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-13.80714607]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 87. Moving avg of loss: 6.2924636867. Average loss: 6.786053\n",
      "\n",
      "[[-8.90709209]\n",
      " [-0.387806  ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-15.77697277]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 88. Moving avg of loss: 6.19871225957. Average loss: 6.793940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-6.83863688]\n",
      " [-1.41900814]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-10.70462799]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 89. Moving avg of loss: 6.10291329714. Average loss: 6.799293\n",
      "\n",
      "[[-6.87938356]\n",
      " [ 7.3746562 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-15.10361385]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 90. Moving avg of loss: 6.01337865071. Average loss: 6.807025\n",
      "\n",
      "[[-13.50397682]\n",
      " [  0.94775277]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-12.84844398]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 91. Moving avg of loss: 5.92573262158. Average loss: 6.814631\n",
      "\n",
      "[[-7.20704651]\n",
      " [-4.41138411]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-11.55259323]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 92. Moving avg of loss: 5.83604230407. Average loss: 6.819777\n",
      "\n",
      "[[ -3.24943256]\n",
      " [ 10.44032001]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-11.25367737]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 93. Moving avg of loss: 5.74900084061. Average loss: 6.825278\n",
      "\n",
      "[[-1.09642029]\n",
      " [-5.33475637]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-8.79905415]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 94. Moving avg of loss: 5.65928094794. Average loss: 6.827929\n",
      "\n",
      "[[-15.97141933]\n",
      " [  5.3114872 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-16.59479141]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 95. Moving avg of loss: 5.58247308036. Average loss: 6.837277\n",
      "\n",
      "[[-4.20217371]\n",
      " [ 3.69470334]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-11.53994179]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 96. Moving avg of loss: 5.5001646015. Average loss: 6.842157\n",
      "\n",
      "[[-1.85832012]\n",
      " [ 2.4555757 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-10.18420982]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 97. Moving avg of loss: 5.41790216564. Average loss: 6.845929\n",
      "\n",
      "[[-12.54972172]\n",
      " [  7.26586533]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-13.2154026]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 98. Moving avg of loss: 5.34306273638. Average loss: 6.853218\n",
      "\n",
      "[[-3.72422051]\n",
      " [-0.15727432]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-8.09207916]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 99. Moving avg of loss: 5.26332351677. Average loss: 6.856391\n",
      "\n",
      "[[-0.47249392]\n",
      " [-4.46138954]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-8.38216591]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 0, batch 100. Moving avg of loss: 5.18376715849. Average loss: 6.858589\n",
      "\n",
      "[[-1.65915501]\n",
      " [-1.84264624]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-8.16057205]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 101. Moving avg of loss: 5.10650078206. Average loss: 0.002570\n",
      "\n",
      "[[-6.57820511]\n",
      " [ 3.76369143]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-12.17839909]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 102. Moving avg of loss: 5.03487326179. Average loss: 0.007704\n",
      "\n",
      "[[-10.46508026]\n",
      " [ -5.73232031]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-11.47729111]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 103. Moving avg of loss: 4.96413248091. Average loss: 0.012437\n",
      "\n",
      "[[-3.2311058 ]\n",
      " [ 7.42413473]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-13.08437634]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 104. Moving avg of loss: 4.8951494094. Average loss: 0.017350\n",
      "\n",
      "[[-7.88491058]\n",
      " [-0.70748317]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-9.50277138]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 105. Moving avg of loss: 4.82612121947. Average loss: 0.021301\n",
      "\n",
      "[[-8.34412766]\n",
      " [ 7.52375412]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-10.44481373]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 106. Moving avg of loss: 4.76006005938. Average loss: 0.026267\n",
      "\n",
      "[[-8.44024372]\n",
      " [-1.7146771 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-11.16597939]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 107. Moving avg of loss: 4.69427260535. Average loss: 0.030525\n",
      "\n",
      "[[-12.52937698]\n",
      " [  0.56633198]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-12.62985325]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 108. Moving avg of loss: 4.6315063431. Average loss: 0.035901\n",
      "\n",
      "[[-3.78447461]\n",
      " [ 1.76256669]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-9.57838154]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 109. Moving avg of loss: 4.56661330562. Average loss: 0.039021\n",
      "\n",
      "[[-4.80501652]\n",
      " [ 1.73950207]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-9.7894268]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 110. Moving avg of loss: 4.5033356983. Average loss: 0.042357\n",
      "\n",
      "[[-11.10540295]\n",
      " [  7.84765387]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-12.08325005]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 111. Moving avg of loss: 4.44434400518. Average loss: 0.047731\n",
      "\n",
      "[[-1.36535323]\n",
      " [ 4.61783934]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-7.49268818]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 112. Moving avg of loss: 4.38198978727. Average loss: 0.050051\n",
      "\n",
      "[[ 0.81487495]\n",
      " [ 2.54212332]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-8.12123489]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 113. Moving avg of loss: 4.3202943874. Average loss: 0.051992\n",
      "\n",
      "[[-4.73794651]\n",
      " [ 2.08136034]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-8.98481369]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 114. Moving avg of loss: 4.26118263804. Average loss: 0.054880\n",
      "\n",
      "[[-17.9451313 ]\n",
      " [ 11.53351021]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-15.18204594]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 115. Moving avg of loss: 4.2091654383. Average loss: 0.061851\n",
      "\n",
      "[[-7.62156963]\n",
      " [-3.22577858]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-7.92343521]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 116. Moving avg of loss: 4.15171367947. Average loss: 0.064396\n",
      "\n",
      "[[-5.28074121]\n",
      " [ 2.51159525]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-9.4695425]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 117. Moving avg of loss: 4.09588010647. Average loss: 0.067307\n",
      "\n",
      "[[-3.91750717]\n",
      " [ 1.48302019]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-6.8718462]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 118. Moving avg of loss: 4.03984448081. Average loss: 0.069346\n",
      "\n",
      "[[-5.61002398]\n",
      " [-1.49025595]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-6.30311012]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 119. Moving avg of loss: 3.98467269435. Average loss: 0.071257\n",
      "\n",
      "[[-1.62317729]\n",
      " [-0.05497214]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-6.55733585]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 120. Moving avg of loss: 3.92994938948. Average loss: 0.072764\n",
      "\n",
      "[[-1.51835895]\n",
      " [ 2.86204529]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-4.56641006]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 121. Moving avg of loss: 3.8759003186. Average loss: 0.074034\n",
      "\n",
      "[[-6.27313042]\n",
      " [ 8.50696182]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-8.09878826]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 122. Moving avg of loss: 3.82524311895. Average loss: 0.076999\n",
      "\n",
      "[[-4.20036459]\n",
      " [-1.80826628]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-7.06464386]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 123. Moving avg of loss: 3.77379827737. Average loss: 0.078751\n",
      "\n",
      "[[-1.34566271]\n",
      " [ 3.56123614]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-6.83524942]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 124. Moving avg of loss: 3.72318521453. Average loss: 0.080431\n",
      "\n",
      "[[-13.27791595]\n",
      " [  2.24891663]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-9.62907219]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 125. Moving avg of loss: 3.67607324003. Average loss: 0.083964\n",
      "\n",
      "[[-3.03547597]\n",
      " [ 4.97053576]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-8.53848934]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 126. Moving avg of loss: 3.62789520567. Average loss: 0.086127\n",
      "\n",
      "[[-0.86634749]\n",
      " [ 3.89846873]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-6.12794685]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 127. Moving avg of loss: 3.57950418565. Average loss: 0.087518\n",
      "\n",
      "[[ -9.82088566]\n",
      " [ 10.2619648 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-9.15612984]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 128. Moving avg of loss: 3.53453244146. Average loss: 0.090765\n",
      "\n",
      "[[-2.74292994]\n",
      " [-0.25803342]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-5.84416533]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 129. Moving avg of loss: 3.48760521025. Average loss: 0.092017\n",
      "\n",
      "[[-2.82459641]\n",
      " [-1.59386313]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-4.81944895]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 130. Moving avg of loss: 3.44116429893. Average loss: 0.093026\n",
      "\n",
      "[[-1.87612653]\n",
      " [ 4.049716  ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-6.19441223]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 131. Moving avg of loss: 3.39606736721. Average loss: 0.094429\n",
      "\n",
      "[[-2.40833259]\n",
      " [ 7.97147655]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-7.08115864]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 132. Moving avg of loss: 3.35224776384. Average loss: 0.096198\n",
      "\n",
      "[[-2.13862896]\n",
      " [-1.24132252]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-5.14542532]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 133. Moving avg of loss: 3.30806920006. Average loss: 0.097148\n",
      "\n",
      "[[-4.90586805]\n",
      " [ 3.86106491]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-4.63188362]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 134. Moving avg of loss: 3.26524097527. Average loss: 0.098540\n",
      "\n",
      "[[-4.16496372]\n",
      " [-0.63280004]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-4.97462797]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 135. Moving avg of loss: 3.22277747689. Average loss: 0.099662\n",
      "\n",
      "[[-2.98091698]\n",
      " [-0.3246949 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-6.01944494]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 136. Moving avg of loss: 3.18104578264. Average loss: 0.100796\n",
      "\n",
      "[[-5.24612284]\n",
      " [ 1.83772397]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-5.57202911]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 137. Moving avg of loss: 3.14034584948. Average loss: 0.102178\n",
      "\n",
      "[[-4.54842138]\n",
      " [ 1.25427711]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-4.09507227]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 138. Moving avg of loss: 3.09987751064. Average loss: 0.103223\n",
      "\n",
      "[[-0.88017362]\n",
      " [ 3.42780948]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-5.61165237]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 139. Moving avg of loss: 3.06001746656. Average loss: 0.104221\n",
      "\n",
      "[[-2.50705123]\n",
      " [ 2.69073486]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-5.45791101]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 140. Moving avg of loss: 3.02093253406. Average loss: 0.105307\n",
      "\n",
      "[[-4.37327862]\n",
      " [-0.6495527 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-5.86244345]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 141. Moving avg of loss: 2.9825762128. Average loss: 0.106458\n",
      "\n",
      "[[-0.80863589]\n",
      " [ 1.98780179]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-3.12386703]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 142. Moving avg of loss: 2.94408003303. Average loss: 0.107026\n",
      "\n",
      "[[-2.98778939]\n",
      " [-0.27877083]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-4.15611792]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 143. Moving avg of loss: 2.90649104672. Average loss: 0.107809\n",
      "\n",
      "[[-0.725537  ]\n",
      " [ 2.00102997]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-3.80610013]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 144. Moving avg of loss: 2.86928859063. Average loss: 0.108422\n",
      "\n",
      "[[-4.47749233]\n",
      " [ 0.83336371]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-6.54421663]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 145. Moving avg of loss: 2.8334761978. Average loss: 0.109642\n",
      "\n",
      "[[-3.93636608]\n",
      " [ 4.83512449]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-6.22116184]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 146. Moving avg of loss: 2.79829731514. Average loss: 0.110908\n",
      "\n",
      "[[-4.82905579]\n",
      " [ 2.1097188 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-5.03526354]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 147. Moving avg of loss: 2.76341707784. Average loss: 0.111971\n",
      "\n",
      "[[-6.55770302]\n",
      " [-0.38683781]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-5.87170315]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 148. Moving avg of loss: 2.72927382201. Average loss: 0.113177\n",
      "\n",
      "[[-3.38771129]\n",
      " [ 1.99509835]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-4.30110788]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 149. Moving avg of loss: 2.69516154818. Average loss: 0.113988\n",
      "\n",
      "[[-5.60812664]\n",
      " [ 2.2972436 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-2.66358089]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 150. Moving avg of loss: 2.66158913928. Average loss: 0.114802\n",
      "\n",
      "[[-3.53653288]\n",
      " [ 0.56190461]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-5.21051884]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 151. Moving avg of loss: 2.62858238231. Average loss: 0.115647\n",
      "\n",
      "[[-3.67174864]\n",
      " [-0.98880541]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-4.02095079]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 152. Moving avg of loss: 2.59584289016. Average loss: 0.116300\n",
      "\n",
      "[[-3.52031565]\n",
      " [ 0.54047388]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-4.66721678]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 153. Moving avg of loss: 2.56375063864. Average loss: 0.117062\n",
      "\n",
      "[[-5.69235659]\n",
      " [ 4.01618004]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-5.32337284]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 154. Moving avg of loss: 2.53255436135. Average loss: 0.118139\n",
      "\n",
      "[[-4.54624987]\n",
      " [ 3.57850552]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-5.24839211]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 155. Moving avg of loss: 2.50167229879. Average loss: 0.119086\n",
      "\n",
      "[[-1.94375765]\n",
      " [-0.99769169]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-2.34243751]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 156. Moving avg of loss: 2.47046993858. Average loss: 0.119406\n",
      "\n",
      "[[-1.03317761]\n",
      " [-0.92020839]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-2.08416772]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 157. Moving avg of loss: 2.4396326714. Average loss: 0.119639\n",
      "\n",
      "[[-1.22087657]\n",
      " [ 3.74393296]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-3.61728048]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 158. Moving avg of loss: 2.40963968674. Average loss: 0.120171\n",
      "\n",
      "[[-0.87138247]\n",
      " [ 1.14514244]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-3.37650919]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 159. Moving avg of loss: 2.37992552369. Average loss: 0.120564\n",
      "\n",
      "[[-4.23092413]\n",
      " [-0.08119305]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-3.73821664]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 160. Moving avg of loss: 2.35090741863. Average loss: 0.121157\n",
      "\n",
      "[[-1.59098232]\n",
      " [-1.56289768]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.85528123]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 161. Moving avg of loss: 2.32185551616. Average loss: 0.121374\n",
      "\n",
      "[[-2.29642081]\n",
      " [ 1.99964082]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-3.44490099]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 162. Moving avg of loss: 2.29357318213. Average loss: 0.121862\n",
      "\n",
      "[[-2.85729432]\n",
      " [ 0.47900343]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-3.05594659]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 163. Moving avg of loss: 2.26564895607. Average loss: 0.122300\n",
      "\n",
      "[[-4.05715609]\n",
      " [ 2.25162935]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-4.62255383]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 164. Moving avg of loss: 2.23842847671. Average loss: 0.122973\n",
      "\n",
      "[[-1.76164377]\n",
      " [ 0.59721148]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-3.49615693]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 165. Moving avg of loss: 2.2112611598. Average loss: 0.123364\n",
      "\n",
      "[[-2.04722476]\n",
      " [ 0.25711074]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-3.26738834]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 166. Moving avg of loss: 2.18448138199. Average loss: 0.123747\n",
      "\n",
      "[[-2.2760911 ]\n",
      " [ 1.84283996]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-3.94944715]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 167. Moving avg of loss: 2.15820915984. Average loss: 0.124224\n",
      "\n",
      "[[ 0.07601026]\n",
      " [ 3.59637523]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-2.73994017]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 168. Moving avg of loss: 2.13209754763. Average loss: 0.124520\n",
      "\n",
      "[[-2.36651444]\n",
      " [ 1.2055968 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-2.80838394]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 169. Moving avg of loss: 2.10643183577. Average loss: 0.124871\n",
      "\n",
      "[[-4.11755228]\n",
      " [ 1.12321615]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-4.34328985]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 170. Moving avg of loss: 2.08137710323. Average loss: 0.125418\n",
      "\n",
      "[[-0.59021205]\n",
      " [ 0.30706584]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.81551385]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 171. Moving avg of loss: 2.05612441587. Average loss: 0.125507\n",
      "\n",
      "[[-1.51690352]\n",
      " [-0.43263194]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.80832195]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 172. Moving avg of loss: 2.0313551283. Average loss: 0.125697\n",
      "\n",
      "[[-3.42788696]\n",
      " [ 0.37717566]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-2.95056581]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 173. Moving avg of loss: 2.00715447695. Average loss: 0.126063\n",
      "\n",
      "[[-1.36142898]\n",
      " [-0.47725692]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-2.15617895]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 174. Moving avg of loss: 1.98308906007. Average loss: 0.126256\n",
      "\n",
      "[[-0.84856969]\n",
      " [ 0.11084889]]\n",
      "<NDArray 2x1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[-2.18241739]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 175. Moving avg of loss: 1.95935558987. Average loss: 0.126441\n",
      "\n",
      "[[-1.4398365 ]\n",
      " [ 1.03541827]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-2.64254332]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 176. Moving avg of loss: 1.93605690659. Average loss: 0.126709\n",
      "\n",
      "[[-1.63668394]\n",
      " [ 1.01974809]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.60837173]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 177. Moving avg of loss: 1.91300600235. Average loss: 0.126911\n",
      "\n",
      "[[-0.92052376]\n",
      " [-0.75205553]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-2.25534248]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 178. Moving avg of loss: 1.89023993157. Average loss: 0.127079\n",
      "\n",
      "[[-1.23351264]\n",
      " [ 1.58835554]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-3.06284285]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 179. Moving avg of loss: 1.86793545004. Average loss: 0.127368\n",
      "\n",
      "[[-1.60876119]\n",
      " [ 1.68806767]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.77966046]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 180. Moving avg of loss: 1.84584653679. Average loss: 0.127577\n",
      "\n",
      "[[-0.87671155]\n",
      " [-0.65311271]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.30339503]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 181. Moving avg of loss: 1.82393875697. Average loss: 0.127680\n",
      "\n",
      "[[-0.69167906]\n",
      " [ 0.38001406]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.3387289]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 182. Moving avg of loss: 1.80235358605. Average loss: 0.127800\n",
      "\n",
      "[[-1.06256962]\n",
      " [ 0.32250282]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-2.11526823]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 183. Moving avg of loss: 1.78112752867. Average loss: 0.127971\n",
      "\n",
      "[[-2.50078797]\n",
      " [ 2.74918938]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-3.07930303]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 184. Moving avg of loss: 1.76039384949. Average loss: 0.128311\n",
      "\n",
      "[[-2.80440426]\n",
      " [ 0.43510705]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-2.42764997]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 185. Moving avg of loss: 1.73983416952. Average loss: 0.128558\n",
      "\n",
      "[[ 0.34733015]\n",
      " [ 2.13195491]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-2.1342864]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 186. Moving avg of loss: 1.71945114232. Average loss: 0.128717\n",
      "\n",
      "[[-1.25829637]\n",
      " [ 0.216235  ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-2.23671722]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 187. Moving avg of loss: 1.69936119786. Average loss: 0.128889\n",
      "\n",
      "[[-2.51628065]\n",
      " [ 0.30450132]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-2.01361537]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 188. Moving avg of loss: 1.67958570527. Average loss: 0.129096\n",
      "\n",
      "[[-1.35560024]\n",
      " [-0.03332584]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.7697525]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 189. Moving avg of loss: 1.66000603818. Average loss: 0.129242\n",
      "\n",
      "[[ 0.51364177]\n",
      " [ 1.24197984]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.31554747]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 190. Moving avg of loss: 1.64060255971. Average loss: 0.129313\n",
      "\n",
      "[[-1.64035523]\n",
      " [ 2.55170488]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-2.40719724]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 191. Moving avg of loss: 1.62164001096. Average loss: 0.129538\n",
      "\n",
      "[[-2.1901381]\n",
      " [ 0.6934706]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-2.81831408]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 192. Moving avg of loss: 1.60293680013. Average loss: 0.129767\n",
      "\n",
      "[[-2.05162191]\n",
      " [ 1.31376624]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.83511651]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 193. Moving avg of loss: 1.58442360554. Average loss: 0.129944\n",
      "\n",
      "[[-2.08312964]\n",
      " [ 2.31991577]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-2.44024396]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 194. Moving avg of loss: 1.56620362104. Average loss: 0.130161\n",
      "\n",
      "[[-0.12492307]\n",
      " [ 0.06731236]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.59722865]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 195. Moving avg of loss: 1.54807043409. Average loss: 0.130245\n",
      "\n",
      "[[-2.22466135]\n",
      " [-0.28196388]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.70448947]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 196. Moving avg of loss: 1.53024504117. Average loss: 0.130386\n",
      "\n",
      "[[-1.14132833]\n",
      " [ 2.29469013]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.74544299]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 197. Moving avg of loss: 1.51265418523. Average loss: 0.130527\n",
      "\n",
      "[[-1.55935812]\n",
      " [-0.19552438]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.74240017]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 198. Moving avg of loss: 1.49526244205. Average loss: 0.130639\n",
      "\n",
      "[[-1.31770229]\n",
      " [ 0.0266911 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.56180787]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 199. Moving avg of loss: 1.47809051222. Average loss: 0.130744\n",
      "\n",
      "[[-0.91917455]\n",
      " [ 0.50924313]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.7510066]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 1, batch 200. Moving avg of loss: 1.46114929409. Average loss: 0.130853\n",
      "\n",
      "[[ 0.34409833]\n",
      " [ 0.54919893]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.88242739]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 201. Moving avg of loss: 1.44434352785. Average loss: 0.000035\n",
      "\n",
      "[[-0.38840285]\n",
      " [ 0.11741072]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.89430696]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 202. Moving avg of loss: 1.42777309467. Average loss: 0.000084\n",
      "\n",
      "[[-0.3319577]\n",
      " [ 0.9850021]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.59815431]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 203. Moving avg of loss: 1.4114594105. Average loss: 0.000169\n",
      "\n",
      "[[-2.90328693]\n",
      " [ 0.99415493]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-2.02808905]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 204. Moving avg of loss: 1.3954493513. Average loss: 0.000334\n",
      "\n",
      "[[-1.52471578]\n",
      " [ 0.72658402]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-2.02992129]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 205. Moving avg of loss: 1.37960619741. Average loss: 0.000464\n",
      "\n",
      "[[-1.75812972]\n",
      " [ 0.41145566]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.86853266]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 206. Moving avg of loss: 1.36394723504. Average loss: 0.000576\n",
      "\n",
      "[[-1.15519357]\n",
      " [ 1.51596344]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.59941959]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 207. Moving avg of loss: 1.34848173179. Average loss: 0.000681\n",
      "\n",
      "[[-0.03302601]\n",
      " [ 0.75257784]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.20811915]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 208. Moving avg of loss: 1.33315899226. Average loss: 0.000738\n",
      "\n",
      "[[-1.17161584]\n",
      " [ 0.15469716]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.42259216]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 209. Moving avg of loss: 1.31806353055. Average loss: 0.000821\n",
      "\n",
      "[[-0.55071843]\n",
      " [ 0.40190747]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.33003783]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 210. Moving avg of loss: 1.30313335766. Average loss: 0.000881\n",
      "\n",
      "[[-1.09226918]\n",
      " [ 0.32326463]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.46186483]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 211. Moving avg of loss: 1.28841295217. Average loss: 0.000958\n",
      "\n",
      "[[-1.52311945]\n",
      " [-0.04792613]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.51003027]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 212. Moving avg of loss: 1.27388149486. Average loss: 0.001036\n",
      "\n",
      "[[ 0.08113138]\n",
      " [ 0.1893803 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.05467296]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 213. Moving avg of loss: 1.25948618027. Average loss: 0.001072\n",
      "\n",
      "[[-0.23033281]\n",
      " [ 0.07571375]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.91375542]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 214. Moving avg of loss: 1.2452698276. Average loss: 0.001105\n",
      "\n",
      "[[-0.24060394]\n",
      " [-0.08540487]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.94029808]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 215. Moving avg of loss: 1.23123625049. Average loss: 0.001141\n",
      "\n",
      "[[-1.3790946 ]\n",
      " [-0.24320433]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.52526104]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 216. Moving avg of loss: 1.21742148717. Average loss: 0.001215\n",
      "\n",
      "[[-0.61033088]\n",
      " [-0.26059365]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.80740982]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 217. Moving avg of loss: 1.20373290857. Average loss: 0.001246\n",
      "\n",
      "[[-1.03585362]\n",
      " [ 0.40283051]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.47793889]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 218. Moving avg of loss: 1.1902548384. Average loss: 0.001313\n",
      "\n",
      "[[-1.12707829]\n",
      " [ 1.06577384]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.37769699]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 219. Moving avg of loss: 1.17695311775. Average loss: 0.001386\n",
      "\n",
      "[[-0.45975724]\n",
      " [ 0.45028183]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.96848428]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 220. Moving avg of loss: 1.16378330292. Average loss: 0.001429\n",
      "\n",
      "[[-0.5761981 ]\n",
      " [ 0.43427244]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.28593135]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 221. Moving avg of loss: 1.1507844205. Average loss: 0.001478\n",
      "\n",
      "[[-0.09400211]\n",
      " [ 0.277989  ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.82215023]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 222. Moving avg of loss: 1.1379219703. Average loss: 0.001505\n",
      "\n",
      "[[ 0.16949682]\n",
      " [ 0.07080226]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.66263247]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 223. Moving avg of loss: 1.12520485576. Average loss: 0.001519\n",
      "\n",
      "[[-1.80670297]\n",
      " [ 0.24579391]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.51512074]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 224. Moving avg of loss: 1.11271597396. Average loss: 0.001597\n",
      "\n",
      "[[-1.03957868]\n",
      " [ 0.28140205]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.23312879]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 225. Moving avg of loss: 1.10034919281. Average loss: 0.001646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-0.04838927]\n",
      " [ 0.49004459]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.86509085]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 226. Moving avg of loss: 1.0881107344. Average loss: 0.001674\n",
      "\n",
      "[[-0.79412323]\n",
      " [ 1.07166982]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.12745309]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 227. Moving avg of loss: 1.07605035515. Average loss: 0.001726\n",
      "\n",
      "[[-0.29040596]\n",
      " [ 0.77581543]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.94608021]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 228. Moving avg of loss: 1.06411615994. Average loss: 0.001759\n",
      "\n",
      "[[-0.84846079]\n",
      " [ 0.64748681]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.83037019]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 229. Moving avg of loss: 1.05232936566. Average loss: 0.001794\n",
      "\n",
      "[[-0.76848781]\n",
      " [ 0.1073729 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.06014371]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 230. Moving avg of loss: 1.04069233119. Average loss: 0.001833\n",
      "\n",
      "[[-0.30307606]\n",
      " [-0.16597264]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.88010085]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 231. Moving avg of loss: 1.02917874583. Average loss: 0.001856\n",
      "\n",
      "[[-0.84158057]\n",
      " [ 1.13957703]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.09681511]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 232. Moving avg of loss: 1.01782679812. Average loss: 0.001899\n",
      "\n",
      "[[-0.24485652]\n",
      " [ 0.21353461]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.69401157]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 233. Moving avg of loss: 1.00658712451. Average loss: 0.001918\n",
      "\n",
      "[[-0.64336467]\n",
      " [ 0.0959441 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.75766504]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 234. Moving avg of loss: 0.99549299466. Average loss: 0.001946\n",
      "\n",
      "[[-1.21966338]\n",
      " [ 0.17359456]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.04363847]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 235. Moving avg of loss: 0.98454533193. Average loss: 0.001985\n",
      "\n",
      "[[-0.7428019 ]\n",
      " [ 0.12695993]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.75688696]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 236. Moving avg of loss: 0.973716580374. Average loss: 0.002012\n",
      "\n",
      "[[-0.99963492]\n",
      " [-0.0456489 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-1.02242601]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 237. Moving avg of loss: 0.963024829629. Average loss: 0.002045\n",
      "\n",
      "[[-0.18620552]\n",
      " [ 0.27899435]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.70594352]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 238. Moving avg of loss: 0.952445169506. Average loss: 0.002063\n",
      "\n",
      "[[-1.31808424]\n",
      " [ 0.3578046 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.89910883]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 239. Moving avg of loss: 0.942013226798. Average loss: 0.002100\n",
      "\n",
      "[[-0.51877981]\n",
      " [ 0.1245486 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.68599176]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 240. Moving avg of loss: 0.931684546302. Average loss: 0.002117\n",
      "\n",
      "[[-0.26012313]\n",
      " [-0.28577712]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.69005829]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 241. Moving avg of loss: 0.921475630775. Average loss: 0.002131\n",
      "\n",
      "[[ 0.04757576]\n",
      " [ 0.2834416 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.57142621]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 242. Moving avg of loss: 0.911387516946. Average loss: 0.002144\n",
      "\n",
      "[[-0.27585199]\n",
      " [-0.23076436]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.47218978]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 243. Moving avg of loss: 0.901414334171. Average loss: 0.002152\n",
      "\n",
      "[[-0.44234377]\n",
      " [ 0.16244902]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.43581638]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 244. Moving avg of loss: 0.891563649232. Average loss: 0.002164\n",
      "\n",
      "[[-0.6027624 ]\n",
      " [ 0.22308108]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.74202198]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 245. Moving avg of loss: 0.881840024363. Average loss: 0.002184\n",
      "\n",
      "[[-0.65756929]\n",
      " [ 0.21569614]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.73852682]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 246. Moving avg of loss: 0.872231272351. Average loss: 0.002205\n",
      "\n",
      "[[-0.63515604]\n",
      " [-0.25700372]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.77459025]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 247. Moving avg of loss: 0.862736519731. Average loss: 0.002226\n",
      "\n",
      "[[-0.26944146]\n",
      " [ 0.9385345 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.82864136]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 248. Moving avg of loss: 0.853353323031. Average loss: 0.002246\n",
      "\n",
      "[[-0.2976644 ]\n",
      " [-0.13137285]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.59112048]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 249. Moving avg of loss: 0.844070858038. Average loss: 0.002257\n",
      "\n",
      "[[-0.3563073 ]\n",
      " [ 0.20336133]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.62236261]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 250. Moving avg of loss: 0.834901956944. Average loss: 0.002272\n",
      "\n",
      "[[-0.59656847]\n",
      " [-0.38535208]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.54778111]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 251. Moving avg of loss: 0.825837960184. Average loss: 0.002284\n",
      "\n",
      "[[ 0.10042366]\n",
      " [-0.01647379]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.49043375]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 252. Moving avg of loss: 0.816875904724. Average loss: 0.002293\n",
      "\n",
      "[[ 0.03403416]\n",
      " [ 0.00444847]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.45630905]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 253. Moving avg of loss: 0.808018234819. Average loss: 0.002300\n",
      "\n",
      "[[-0.41763958]\n",
      " [ 0.03688202]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.68651956]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 254. Moving avg of loss: 0.799271759481. Average loss: 0.002315\n",
      "\n",
      "[[-0.26024878]\n",
      " [ 0.09269015]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.43250829]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 255. Moving avg of loss: 0.790621615204. Average loss: 0.002325\n",
      "\n",
      "[[-0.0895877 ]\n",
      " [-0.08553839]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.37869501]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 256. Moving avg of loss: 0.782067696415. Average loss: 0.002330\n",
      "\n",
      "[[-0.32819042]\n",
      " [ 0.03604855]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.58057523]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 257. Moving avg of loss: 0.773618772398. Average loss: 0.002340\n",
      "\n",
      "[[-0.35533792]\n",
      " [ 0.00866648]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.48901558]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 258. Moving avg of loss: 0.765269125919. Average loss: 0.002351\n",
      "\n",
      "[[-0.09464807]\n",
      " [-0.05667734]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.45885253]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 259. Moving avg of loss: 0.757012705492. Average loss: 0.002358\n",
      "\n",
      "[[-0.0212693 ]\n",
      " [ 0.36969286]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.49877405]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 260. Moving avg of loss: 0.748853808296. Average loss: 0.002368\n",
      "\n",
      "[[-0.21041074]\n",
      " [-0.00410721]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.30522978]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 261. Moving avg of loss: 0.740785495454. Average loss: 0.002374\n",
      "\n",
      "[[-0.0298778 ]\n",
      " [ 0.03296342]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.52467763]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 262. Moving avg of loss: 0.732815146855. Average loss: 0.002384\n",
      "\n",
      "[[-0.06023144]\n",
      " [ 0.07450733]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.49936783]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 263. Moving avg of loss: 0.724934053575. Average loss: 0.002391\n",
      "\n",
      "[[-0.32160458]\n",
      " [ 0.42110175]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.34270239]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 264. Moving avg of loss: 0.717145816735. Average loss: 0.002401\n",
      "\n",
      "[[-0.96925712]\n",
      " [ 0.40809917]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.37470978]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 265. Moving avg of loss: 0.709453076718. Average loss: 0.002416\n",
      "\n",
      "[[ 0.06260076]\n",
      " [-0.07238699]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.32663149]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 266. Moving avg of loss: 0.701837638571. Average loss: 0.002421\n",
      "\n",
      "[[-0.48738939]\n",
      " [ 0.24996792]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.61823463]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 267. Moving avg of loss: 0.694320209265. Average loss: 0.002435\n",
      "\n",
      "[[ 0.12427568]\n",
      " [ 0.35197681]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.52706838]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 268. Moving avg of loss: 0.686881343517. Average loss: 0.002443\n",
      "\n",
      "[[-0.2573013 ]\n",
      " [ 0.48400587]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.48909295]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 269. Moving avg of loss: 0.67952903677. Average loss: 0.002452\n",
      "\n",
      "[[-0.28786138]\n",
      " [ 0.23442063]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.20910722]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 270. Moving avg of loss: 0.672256414983. Average loss: 0.002456\n",
      "\n",
      "[[-0.23923336]\n",
      " [ 0.30724153]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.40997523]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 271. Moving avg of loss: 0.665071326899. Average loss: 0.002466\n",
      "\n",
      "[[-0.18660814]\n",
      " [ 0.25319105]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.18449998]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 272. Moving avg of loss: 0.657963085156. Average loss: 0.002470\n",
      "\n",
      "[[-0.31906751]\n",
      " [ 0.07545087]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.33835912]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 273. Moving avg of loss: 0.650937208064. Average loss: 0.002476\n",
      "\n",
      "[[-0.12453842]\n",
      " [ 0.07317074]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.34492087]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 274. Moving avg of loss: 0.643990093226. Average loss: 0.002480\n",
      "\n",
      "[[-0.45840016]\n",
      " [-0.02375845]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.39792466]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 275. Moving avg of loss: 0.637125375019. Average loss: 0.002488\n",
      "\n",
      "[[-0.34776133]\n",
      " [ 0.08435508]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.50525904]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 276. Moving avg of loss: 0.630339340092. Average loss: 0.002497\n",
      "\n",
      "[[-0.25308684]\n",
      " [ 0.03411706]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.30978322]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 277. Moving avg of loss: 0.623626669781. Average loss: 0.002503\n",
      "\n",
      "[[ 0.02132174]\n",
      " [ 0.13671494]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.21467733]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 278. Moving avg of loss: 0.616986061609. Average loss: 0.002505\n",
      "\n",
      "[[-0.18209329]\n",
      " [ 0.25287747]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.28338546]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 279. Moving avg of loss: 0.610423726479. Average loss: 0.002509\n",
      "\n",
      "[[-0.25695837]\n",
      " [ 0.08791704]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.14315259]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 280. Moving avg of loss: 0.603933956117. Average loss: 0.002513\n",
      "\n",
      "[[-0.29368672]\n",
      " [ 0.18601362]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.40966022]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 281. Moving avg of loss: 0.597520216957. Average loss: 0.002519\n",
      "\n",
      "[[-0.13483463]\n",
      " [-0.04387877]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.29057932]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 282. Moving avg of loss: 0.591175756146. Average loss: 0.002523\n",
      "\n",
      "[[-0.08665913]\n",
      " [ 0.41932788]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.3944222]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 283. Moving avg of loss: 0.584906122187. Average loss: 0.002530\n",
      "\n",
      "[[-0.05929729]\n",
      " [-0.09845103]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.26620471]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 284. Moving avg of loss: 0.578703128115. Average loss: 0.002533\n",
      "\n",
      "[[-0.2349139 ]\n",
      " [-0.01127628]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.38134113]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 285. Moving avg of loss: 0.572573030448. Average loss: 0.002540\n",
      "\n",
      "[[-0.23961455]\n",
      " [ 0.10398716]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.31025195]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 286. Moving avg of loss: 0.566509096742. Average loss: 0.002544\n",
      "\n",
      "[[-0.31748635]\n",
      " [ 0.40520263]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.38676691]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 287. Moving avg of loss: 0.560516207922. Average loss: 0.002551\n",
      "\n",
      "[[-0.36734655]\n",
      " [ 0.31378189]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.50797725]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 288. Moving avg of loss: 0.554591180273. Average loss: 0.002559\n",
      "\n",
      "[[-0.22208074]\n",
      " [ 0.60778403]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.46246034]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 289. Moving avg of loss: 0.548732180379. Average loss: 0.002567\n",
      "\n",
      "[[-0.26637167]\n",
      " [-0.01093993]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.22104198]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 290. Moving avg of loss: 0.542933722148. Average loss: 0.002570\n",
      "\n",
      "[[-0.16854487]\n",
      " [-0.04900216]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.25479329]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 291. Moving avg of loss: 0.537199515476. Average loss: 0.002573\n",
      "\n",
      "[[ 0.00533549]\n",
      " [ 0.01943148]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.26934457]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 292. Moving avg of loss: 0.531528495551. Average loss: 0.002575\n",
      "\n",
      "[[-0.12743422]\n",
      " [ 0.05705525]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.30521655]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 293. Moving avg of loss: 0.525922113333. Average loss: 0.002579\n",
      "\n",
      "[[-0.02909067]\n",
      " [ 0.02017473]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.15602124]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 294. Moving avg of loss: 0.520375660758. Average loss: 0.002581\n",
      "\n",
      "[[-0.14614064]\n",
      " [ 0.1077184 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.14874363]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 295. Moving avg of loss: 0.51489169395. Average loss: 0.002583\n",
      "\n",
      "[[-0.34186935]\n",
      " [-0.09327683]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.28743774]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 296. Moving avg of loss: 0.509470610823. Average loss: 0.002588\n",
      "\n",
      "[[-0.04376045]\n",
      " [ 0.08614071]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.13020372]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 297. Moving avg of loss: 0.504106852808. Average loss: 0.002590\n",
      "\n",
      "[[-0.12016524]\n",
      " [-0.02873196]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.10562722]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 298. Moving avg of loss: 0.498801966785. Average loss: 0.002592\n",
      "\n",
      "[[-0.52862132]\n",
      " [-0.00451847]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.34253788]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 299. Moving avg of loss: 0.493559816852. Average loss: 0.002597\n",
      "\n",
      "[[-0.22847086]\n",
      " [ 0.13607383]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.31395406]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 2, batch 300. Moving avg of loss: 0.48837377947. Average loss: 0.002601\n",
      "\n",
      "[[ 0.05633099]\n",
      " [-0.01541156]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.22674596]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 301. Moving avg of loss: 0.483242610345. Average loss: 0.000002\n",
      "\n",
      "[[-0.00259142]\n",
      " [ 0.15545018]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.11884695]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 302. Moving avg of loss: 0.478167726275. Average loss: 0.000003\n",
      "\n",
      "[[-0.08036575]\n",
      " [-0.17424743]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.21968418]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 303. Moving avg of loss: 0.473149052609. Average loss: 0.000005\n",
      "\n",
      "[[-0.20831917]\n",
      " [-0.00334997]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.29949093]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 304. Moving avg of loss: 0.468187461185. Average loss: 0.000009\n",
      "\n",
      "[[-0.00307933]\n",
      " [-0.07532017]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.35927087]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 305. Moving avg of loss: 0.46328095586. Average loss: 0.000013\n",
      "\n",
      "[[-0.07130159]\n",
      " [ 0.13547535]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.0854398]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 306. Moving avg of loss: 0.458425333835. Average loss: 0.000014\n",
      "\n",
      "[[-0.32162973]\n",
      " [ 0.02841035]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.35562316]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 307. Moving avg of loss: 0.453626320269. Average loss: 0.000019\n",
      "\n",
      "[[-0.08263416]\n",
      " [ 0.10337385]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.18534055]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 308. Moving avg of loss: 0.448877265643. Average loss: 0.000021\n",
      "\n",
      "[[-0.04663345]\n",
      " [ 0.10828324]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.13610768]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 309. Moving avg of loss: 0.44417987486. Average loss: 0.000023\n",
      "\n",
      "[[ 0.13449575]\n",
      " [ 0.24265255]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.29842818]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 310. Moving avg of loss: 0.439535163084. Average loss: 0.000026\n",
      "\n",
      "[[-0.17371011]\n",
      " [ 0.11487669]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.34159923]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 311. Moving avg of loss: 0.434941505431. Average loss: 0.000029\n",
      "\n",
      "[[-0.19608559]\n",
      " [ 0.08392058]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.21601582]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 312. Moving avg of loss: 0.430397414429. Average loss: 0.000032\n",
      "\n",
      "[[-0.05494091]\n",
      " [ 0.1064734 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.02125692]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 313. Moving avg of loss: 0.425901577331. Average loss: 0.000034\n",
      "\n",
      "[[-0.10012728]\n",
      " [ 0.04576745]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.03575253]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, batch 314. Moving avg of loss: 0.421454326599. Average loss: 0.000035\n",
      "\n",
      "[[-0.10192062]\n",
      " [ 0.17395131]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.17926705]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 315. Moving avg of loss: 0.417057590014. Average loss: 0.000038\n",
      "\n",
      "[[-0.0826266 ]\n",
      " [ 0.02016478]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.21425015]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 316. Moving avg of loss: 0.412707954426. Average loss: 0.000041\n",
      "\n",
      "[[-0.06566124]\n",
      " [-0.00932315]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.07227898]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 317. Moving avg of loss: 0.408403720744. Average loss: 0.000041\n",
      "\n",
      "[[-0.1614745 ]\n",
      " [ 0.14212467]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.10791898]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 318. Moving avg of loss: 0.404148053125. Average loss: 0.000044\n",
      "\n",
      "[[-0.25825328]\n",
      " [-0.06155666]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.08206433]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 319. Moving avg of loss: 0.399937572593. Average loss: 0.000046\n",
      "\n",
      "[[-0.04064113]\n",
      " [ 0.07834741]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.11463869]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 320. Moving avg of loss: 0.395772657537. Average loss: 0.000047\n",
      "\n",
      "[[-0.1436256 ]\n",
      " [ 0.01879237]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.01787782]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 321. Moving avg of loss: 0.391652678372. Average loss: 0.000048\n",
      "\n",
      "[[ -2.16842964e-05]\n",
      " [  5.16531952e-02]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.04037693]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 322. Moving avg of loss: 0.38757743728. Average loss: 0.000050\n",
      "\n",
      "[[ 0.03055435]\n",
      " [-0.01702252]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.17918479]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 323. Moving avg of loss: 0.383546274542. Average loss: 0.000051\n",
      "\n",
      "[[-0.11178315]\n",
      " [-0.00386338]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.06069279]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 324. Moving avg of loss: 0.379558558895. Average loss: 0.000053\n",
      "\n",
      "[[-0.11968364]\n",
      " [-0.12821543]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.14748299]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 325. Moving avg of loss: 0.375613814461. Average loss: 0.000054\n",
      "\n",
      "[[-0.20561795]\n",
      " [ 0.13195942]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.18130493]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 326. Moving avg of loss: 0.371713069538. Average loss: 0.000057\n",
      "\n",
      "[[-0.11959792]\n",
      " [ 0.13328162]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.18690586]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 327. Moving avg of loss: 0.367853510786. Average loss: 0.000059\n",
      "\n",
      "[[-0.13413236]\n",
      " [-0.05218653]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.19206738]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 328. Moving avg of loss: 0.364036211131. Average loss: 0.000061\n",
      "\n",
      "[[-0.01448361]\n",
      " [ 0.00636243]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.24078232]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 329. Moving avg of loss: 0.360259748901. Average loss: 0.000064\n",
      "\n",
      "[[ 0.01687013]\n",
      " [ 0.03650616]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.09056564]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 330. Moving avg of loss: 0.356522683575. Average loss: 0.000065\n",
      "\n",
      "[[-0.00029062]\n",
      " [ 0.05989397]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.19438308]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 331. Moving avg of loss: 0.352826367408. Average loss: 0.000066\n",
      "\n",
      "[[-0.05203151]\n",
      " [-0.03869812]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.07885873]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 332. Moving avg of loss: 0.349168514812. Average loss: 0.000067\n",
      "\n",
      "[[-0.13057937]\n",
      " [-0.09122317]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.17008126]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 333. Moving avg of loss: 0.345550789089. Average loss: 0.000068\n",
      "\n",
      "[[-0.09827645]\n",
      " [ 0.15192513]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.13990086]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 334. Moving avg of loss: 0.341972665902. Average loss: 0.000070\n",
      "\n",
      "[[-0.04343604]\n",
      " [ 0.07793346]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.20936143]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 335. Moving avg of loss: 0.338433019405. Average loss: 0.000072\n",
      "\n",
      "[[-0.02808337]\n",
      " [ 0.08131107]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.13581479]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 336. Moving avg of loss: 0.334930574214. Average loss: 0.000074\n",
      "\n",
      "[[-0.10959833]\n",
      " [ 0.11539727]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.01588559]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 337. Moving avg of loss: 0.331464923642. Average loss: 0.000075\n",
      "\n",
      "[[-0.16714434]\n",
      " [ 0.10690822]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.14888287]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 338. Moving avg of loss: 0.328037581947. Average loss: 0.000077\n",
      "\n",
      "[[-0.17363866]\n",
      " [ 0.0950181 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.0002465]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 339. Moving avg of loss: 0.324646536207. Average loss: 0.000078\n",
      "\n",
      "[[-0.05620381]\n",
      " [ 0.14737381]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.15357511]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 340. Moving avg of loss: 0.321292422383. Average loss: 0.000081\n",
      "\n",
      "[[-0.03661474]\n",
      " [-0.06354377]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.05801034]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 341. Moving avg of loss: 0.317972312238. Average loss: 0.000081\n",
      "\n",
      "[[-0.00456606]\n",
      " [-0.05683165]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.05855715]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 342. Moving avg of loss: 0.31468773319. Average loss: 0.000082\n",
      "\n",
      "[[-0.04831534]\n",
      " [ 0.01086015]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.06527168]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 343. Moving avg of loss: 0.311438312544. Average loss: 0.000083\n",
      "\n",
      "[[-0.05350202]\n",
      " [-0.01039304]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.07266402]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 344. Moving avg of loss: 0.308224076841. Average loss: 0.000085\n",
      "\n",
      "[[-0.10254677]\n",
      " [ 0.04030848]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.04452619]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 345. Moving avg of loss: 0.305043508874. Average loss: 0.000085\n",
      "\n",
      "[[ -4.08001579e-05]\n",
      " [ -2.66181361e-02]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.02024245]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 346. Moving avg of loss: 0.3018977751. Average loss: 0.000087\n",
      "\n",
      "[[ 0.0015674 ]\n",
      " [ 0.01068937]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.11638224]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 347. Moving avg of loss: 0.298784860303. Average loss: 0.000089\n",
      "\n",
      "[[ 0.02801192]\n",
      " [ 0.19762771]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.09105182]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 348. Moving avg of loss: 0.295704490713. Average loss: 0.000089\n",
      "\n",
      "[[-0.02503986]\n",
      " [-0.04012084]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.12607133]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 349. Moving avg of loss: 0.292657312025. Average loss: 0.000090\n",
      "\n",
      "[[-0.06618246]\n",
      " [ 0.11429495]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.08525771]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 350. Moving avg of loss: 0.289643490666. Average loss: 0.000093\n",
      "\n",
      "[[-0.16154733]\n",
      " [-0.04341649]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.18111503]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 351. Moving avg of loss: 0.286661421643. Average loss: 0.000095\n",
      "\n",
      "[[-0.05258577]\n",
      " [ 0.18730764]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.12208509]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 352. Moving avg of loss: 0.283710664943. Average loss: 0.000096\n",
      "\n",
      "[[-0.05905583]\n",
      " [ 0.09636531]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.07834172]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 353. Moving avg of loss: 0.280790537063. Average loss: 0.000097\n",
      "\n",
      "[[-0.05733824]\n",
      " [-0.03591646]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.1169939]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 354. Moving avg of loss: 0.27790163254. Average loss: 0.000099\n",
      "\n",
      "[[ 0.03300807]\n",
      " [ 0.04358415]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.00467244]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 355. Moving avg of loss: 0.275042619342. Average loss: 0.000099\n",
      "\n",
      "[[-0.02795498]\n",
      " [ 0.12703617]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.15627259]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 356. Moving avg of loss: 0.272214691591. Average loss: 0.000101\n",
      "\n",
      "[[-0.07347771]\n",
      " [ 0.01628867]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.00310594]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 357. Moving avg of loss: 0.269416263273. Average loss: 0.000102\n",
      "\n",
      "[[-0.12641728]\n",
      " [ 0.08630124]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.06785131]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 358. Moving avg of loss: 0.266647399821. Average loss: 0.000103\n",
      "\n",
      "[[-0.18817991]\n",
      " [-0.01118568]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.17365432]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 359. Moving avg of loss: 0.263908810284. Average loss: 0.000105\n",
      "\n",
      "[[-0.09853982]\n",
      " [ 0.08712939]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.10571742]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 360. Moving avg of loss: 0.261198196141. Average loss: 0.000106\n",
      "\n",
      "[[ 0.00228914]\n",
      " [ 0.02239146]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.10286427]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 361. Moving avg of loss: 0.258516623218. Average loss: 0.000108\n",
      "\n",
      "[[-0.028728  ]\n",
      " [-0.00214383]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.12181508]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 362. Moving avg of loss: 0.255862964705. Average loss: 0.000109\n",
      "\n",
      "[[ 0.05857484]\n",
      " [-0.06663834]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.0259397]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 363. Moving avg of loss: 0.253236836263. Average loss: 0.000110\n",
      "\n",
      "[[ 0.0575029 ]\n",
      " [ 0.03408389]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.11604345]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 364. Moving avg of loss: 0.250638531992. Average loss: 0.000111\n",
      "\n",
      "[[ 0.028318  ]\n",
      " [-0.00041083]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.08942908]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 365. Moving avg of loss: 0.248067269799. Average loss: 0.000112\n",
      "\n",
      "[[-0.07835267]\n",
      " [-0.0157415 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.06263414]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 366. Moving avg of loss: 0.245523177024. Average loss: 0.000113\n",
      "\n",
      "[[-0.06251989]\n",
      " [ 0.08841944]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.16174746]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 367. Moving avg of loss: 0.243006293804. Average loss: 0.000114\n",
      "\n",
      "[[-0.01095925]\n",
      " [ 0.03685828]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.11309385]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 368. Moving avg of loss: 0.24051549115. Average loss: 0.000115\n",
      "\n",
      "[[-0.07899314]\n",
      " [ 0.04601102]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.04022995]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 369. Moving avg of loss: 0.238050581089. Average loss: 0.000116\n",
      "\n",
      "[[ 0.05922633]\n",
      " [-0.05640361]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.0957178]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 370. Moving avg of loss: 0.235612626508. Average loss: 0.000117\n",
      "\n",
      "[[ 0.05705238]\n",
      " [-0.00033742]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.11844969]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 371. Moving avg of loss: 0.233199719996. Average loss: 0.000119\n",
      "\n",
      "[[ 0.10795993]\n",
      " [ 0.02417426]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.136747]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 372. Moving avg of loss: 0.230812154657. Average loss: 0.000120\n",
      "\n",
      "[[-0.08386236]\n",
      " [-0.07480364]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.10413807]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 373. Moving avg of loss: 0.228449059759. Average loss: 0.000120\n",
      "\n",
      "[[-0.0532577 ]\n",
      " [ 0.05289838]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.03853011]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 374. Moving avg of loss: 0.226111247503. Average loss: 0.000122\n",
      "\n",
      "[[-0.03919635]\n",
      " [ 0.02769278]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.0015626]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 375. Moving avg of loss: 0.223797684529. Average loss: 0.000123\n",
      "\n",
      "[[-0.09447502]\n",
      " [ 0.0730581 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.13458085]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 376. Moving avg of loss: 0.221508863708. Average loss: 0.000124\n",
      "\n",
      "[[-0.03482191]\n",
      " [ 0.08362938]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.01029909]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 377. Moving avg of loss: 0.219243864867. Average loss: 0.000125\n",
      "\n",
      "[[ 0.06601392]\n",
      " [ 0.01899099]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.16248488]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 378. Moving avg of loss: 0.217002404663. Average loss: 0.000127\n",
      "\n",
      "[[-0.08100402]\n",
      " [-0.04874056]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.10751837]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 379. Moving avg of loss: 0.214784248207. Average loss: 0.000128\n",
      "\n",
      "[[-0.00495917]\n",
      " [ 0.00549856]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.10357499]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 380. Moving avg of loss: 0.212588757754. Average loss: 0.000128\n",
      "\n",
      "[[ 0.01537162]\n",
      " [ 0.03419558]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.06870759]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 381. Moving avg of loss: 0.210416267821. Average loss: 0.000129\n",
      "\n",
      "[[-0.11914762]\n",
      " [ 0.11875245]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.14634681]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 382. Moving avg of loss: 0.208268060183. Average loss: 0.000131\n",
      "\n",
      "[[-0.06992753]\n",
      " [-0.08586006]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.03822988]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 383. Moving avg of loss: 0.206141186639. Average loss: 0.000132\n",
      "\n",
      "[[-0.21982175]\n",
      " [ 0.06334791]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.10435466]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 384. Moving avg of loss: 0.204037885718. Average loss: 0.000134\n",
      "\n",
      "[[-0.1034207 ]\n",
      " [-0.00866073]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.02534109]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 385. Moving avg of loss: 0.201954758769. Average loss: 0.000135\n",
      "\n",
      "[[-0.02697389]\n",
      " [ 0.11548796]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.04905933]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 386. Moving avg of loss: 0.199893190485. Average loss: 0.000136\n",
      "\n",
      "[[ 0.01158938]\n",
      " [-0.03523646]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.10572404]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 387. Moving avg of loss: 0.19785358676. Average loss: 0.000137\n",
      "\n",
      "[[ 0.08108084]\n",
      " [-0.03132529]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.01469254]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 388. Moving avg of loss: 0.195835280793. Average loss: 0.000138\n",
      "\n",
      "[[-0.03157546]\n",
      " [-0.04765198]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.01446941]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 389. Moving avg of loss: 0.193837657572. Average loss: 0.000139\n",
      "\n",
      "[[ 0.0030598 ]\n",
      " [-0.16175134]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.02308261]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 390. Moving avg of loss: 0.191860926588. Average loss: 0.000140\n",
      "\n",
      "[[-0.01745369]\n",
      " [ 0.04467184]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.02932119]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 391. Moving avg of loss: 0.189904860925. Average loss: 0.000141\n",
      "\n",
      "[[-0.00854815]\n",
      " [ 0.15362714]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.08269835]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 392. Moving avg of loss: 0.187969076011. Average loss: 0.000141\n",
      "\n",
      "[[-0.01797204]\n",
      " [-0.10747379]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.10250711]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 393. Moving avg of loss: 0.186053620237. Average loss: 0.000143\n",
      "\n",
      "[[-0.24150465]\n",
      " [ 0.02830431]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.07190239]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 394. Moving avg of loss: 0.184158458701. Average loss: 0.000144\n",
      "\n",
      "[[-0.00420829]\n",
      " [ 0.09504362]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.01261538]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 395. Moving avg of loss: 0.182283665992. Average loss: 0.000146\n",
      "\n",
      "[[-0.11425472]\n",
      " [-0.01349783]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.02645445]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 396. Moving avg of loss: 0.180427142718. Average loss: 0.000147\n",
      "\n",
      "[[ 0.00118204]\n",
      " [-0.0697555 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.02335852]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 397. Moving avg of loss: 0.178589150865. Average loss: 0.000148\n",
      "\n",
      "[[-0.01137796]\n",
      " [-0.10388619]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.00090301]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 398. Moving avg of loss: 0.176770852564. Average loss: 0.000148\n",
      "\n",
      "[[-0.0308344 ]\n",
      " [ 0.02045215]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.05341315]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 399. Moving avg of loss: 0.174970906719. Average loss: 0.000149\n",
      "\n",
      "[[-0.09277857]\n",
      " [ 0.01403984]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.09017766]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 3, batch 400. Moving avg of loss: 0.173190213494. Average loss: 0.000150\n",
      "\n",
      "[[-0.03267302]\n",
      " [-0.18418819]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.07997593]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 401. Moving avg of loss: 0.171427889401. Average loss: 0.000001\n",
      "\n",
      "[[-0.06230612]\n",
      " [-0.06435279]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.02045989]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 402. Moving avg of loss: 0.169683950992. Average loss: 0.000002\n",
      "\n",
      "[[-0.03928403]\n",
      " [ 0.05041705]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.05913973]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 403. Moving avg of loss: 0.167957637825. Average loss: 0.000003\n",
      "\n",
      "[[-0.12416655]\n",
      " [-0.10787849]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.07149851]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 404. Moving avg of loss: 0.166249628553. Average loss: 0.000004\n",
      "\n",
      "[[ 0.0354694 ]\n",
      " [ 0.07648558]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.04193151]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 405. Moving avg of loss: 0.16455944372. Average loss: 0.000005\n",
      "\n",
      "[[ 0.02890016]\n",
      " [-0.00298632]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.03580087]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 406. Moving avg of loss: 0.162885727397. Average loss: 0.000005\n",
      "\n",
      "[[-0.04663694]\n",
      " [ 0.17978206]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.05994034]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, batch 407. Moving avg of loss: 0.161231262536. Average loss: 0.000007\n",
      "\n",
      "[[-0.03727381]\n",
      " [-0.00313147]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.02892798]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 408. Moving avg of loss: 0.159592434119. Average loss: 0.000008\n",
      "\n",
      "[[ 0.01059139]\n",
      " [ 0.11302754]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.03003025]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 409. Moving avg of loss: 0.157970738317. Average loss: 0.000008\n",
      "\n",
      "[[-0.04872931]\n",
      " [ 0.00780601]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.07808375]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 410. Moving avg of loss: 0.156365809124. Average loss: 0.000009\n",
      "\n",
      "[[ 0.02920918]\n",
      " [ 0.05379996]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.07070804]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 411. Moving avg of loss: 0.154777226041. Average loss: 0.000010\n",
      "\n",
      "[[ 0.03753606]\n",
      " [ 0.10143987]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.10430276]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 412. Moving avg of loss: 0.153205345711. Average loss: 0.000011\n",
      "\n",
      "[[ 0.1193375 ]\n",
      " [ 0.02177973]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.06017709]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 413. Moving avg of loss: 0.151650108776. Average loss: 0.000012\n",
      "\n",
      "[[ 0.01526364]\n",
      " [ 0.01036676]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.00089359]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 414. Moving avg of loss: 0.150110145305. Average loss: 0.000013\n",
      "\n",
      "[[-0.03030499]\n",
      " [-0.03237494]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.13426164]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 415. Moving avg of loss: 0.148587111857. Average loss: 0.000014\n",
      "\n",
      "[[ 0.05643476]\n",
      " [ 0.01109827]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.0747875]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 416. Moving avg of loss: 0.147078839373. Average loss: 0.000015\n",
      "\n",
      "[[-0.09845706]\n",
      " [-0.00399725]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.04443657]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 417. Moving avg of loss: 0.145586453849. Average loss: 0.000016\n",
      "\n",
      "[[ 0.0133252 ]\n",
      " [-0.09858049]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.01114833]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 418. Moving avg of loss: 0.144109075155. Average loss: 0.000016\n",
      "\n",
      "[[-0.0555558 ]\n",
      " [-0.01993084]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.04916096]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 419. Moving avg of loss: 0.14264704249. Average loss: 0.000017\n",
      "\n",
      "[[-0.1099277 ]\n",
      " [-0.02341823]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.02199471]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 420. Moving avg of loss: 0.141200200195. Average loss: 0.000018\n",
      "\n",
      "[[-0.0428449 ]\n",
      " [-0.01359464]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.02008137]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 421. Moving avg of loss: 0.139768294356. Average loss: 0.000019\n",
      "\n",
      "[[ 0.05515766]\n",
      " [-0.0196632 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.08046585]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 422. Moving avg of loss: 0.138351109169. Average loss: 0.000020\n",
      "\n",
      "[[-0.08646152]\n",
      " [-0.00419899]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.09079951]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 423. Moving avg of loss: 0.136948878883. Average loss: 0.000021\n",
      "\n",
      "[[-0.05478987]\n",
      " [-0.05577232]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.03114858]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 424. Moving avg of loss: 0.135560956522. Average loss: 0.000022\n",
      "\n",
      "[[-0.05915405]\n",
      " [ 0.03932098]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.0236392]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 425. Moving avg of loss: 0.134186802149. Average loss: 0.000023\n",
      "\n",
      "[[-0.00902228]\n",
      " [ 0.12302112]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.11403275]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 426. Moving avg of loss: 0.132827572362. Average loss: 0.000024\n",
      "\n",
      "[[-0.02901079]\n",
      " [-0.02340268]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.10334754]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 427. Moving avg of loss: 0.131482020438. Average loss: 0.000025\n",
      "\n",
      "[[ 0.00018287]\n",
      " [ 0.09022009]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.0908165]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 428. Moving avg of loss: 0.130149771744. Average loss: 0.000026\n",
      "\n",
      "[[-0.1320232 ]\n",
      " [-0.10803777]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.05144143]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 429. Moving avg of loss: 0.128831556609. Average loss: 0.000027\n",
      "\n",
      "[[ 0.04804322]\n",
      " [-0.03540235]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.03151107]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 430. Moving avg of loss: 0.12752731292. Average loss: 0.000028\n",
      "\n",
      "[[ 0.00127267]\n",
      " [ 0.07698512]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.00621431]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 431. Moving avg of loss: 0.126236165152. Average loss: 0.000030\n",
      "\n",
      "[[ 0.01395544]\n",
      " [-0.06530872]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.00835699]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 432. Moving avg of loss: 0.124957847857. Average loss: 0.000030\n",
      "\n",
      "[[-0.0867215 ]\n",
      " [-0.01144377]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.00434661]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 433. Moving avg of loss: 0.12369349466. Average loss: 0.000032\n",
      "\n",
      "[[-0.03681997]\n",
      " [ 0.00494347]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.04115313]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 434. Moving avg of loss: 0.122441374392. Average loss: 0.000032\n",
      "\n",
      "[[ 0.01922988]\n",
      " [-0.01176121]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.01795838]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 435. Moving avg of loss: 0.121203167041. Average loss: 0.000034\n",
      "\n",
      "[[ 0.01110525]\n",
      " [-0.03277195]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.00573003]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 436. Moving avg of loss: 0.119976644495. Average loss: 0.000035\n",
      "\n",
      "[[ 0.06300882]\n",
      " [ 0.03276654]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.04226577]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 437. Moving avg of loss: 0.118763175651. Average loss: 0.000036\n",
      "\n",
      "[[-0.00814296]\n",
      " [-0.00779042]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.13133349]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 438. Moving avg of loss: 0.117561519424. Average loss: 0.000037\n",
      "\n",
      "[[ 0.00115123]\n",
      " [ 0.02947616]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.04287231]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 439. Moving avg of loss: 0.116372323707. Average loss: 0.000038\n",
      "\n",
      "[[-0.05861334]\n",
      " [-0.03422727]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.00229245]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 440. Moving avg of loss: 0.115195489591. Average loss: 0.000039\n",
      "\n",
      "[[-0.01206267]\n",
      " [ 0.0807584 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.00578654]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 441. Moving avg of loss: 0.114030596104. Average loss: 0.000040\n",
      "\n",
      "[[-0.03729314]\n",
      " [-0.01185347]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.12480414]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 442. Moving avg of loss: 0.112877370526. Average loss: 0.000041\n",
      "\n",
      "[[-0.01001167]\n",
      " [-0.01363502]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.02372086]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 443. Moving avg of loss: 0.11173611752. Average loss: 0.000041\n",
      "\n",
      "[[ 0.05843313]\n",
      " [ 0.08558768]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.10147927]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 444. Moving avg of loss: 0.110606706941. Average loss: 0.000042\n",
      "\n",
      "[[ 0.04337589]\n",
      " [ 0.04189001]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.03348482]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 445. Moving avg of loss: 0.109489455643. Average loss: 0.000044\n",
      "\n",
      "[[ 0.0363539 ]\n",
      " [ 0.02018824]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.0588522]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 446. Moving avg of loss: 0.108383159196. Average loss: 0.000045\n",
      "\n",
      "[[-0.04947449]\n",
      " [ 0.11731394]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.03102714]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 447. Moving avg of loss: 0.107288213852. Average loss: 0.000046\n",
      "\n",
      "[[ 0.24177051]\n",
      " [ 0.12560433]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.0950563]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 448. Moving avg of loss: 0.106205634138. Average loss: 0.000049\n",
      "\n",
      "[[ 0.08236699]\n",
      " [ 0.04981918]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.06285089]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 449. Moving avg of loss: 0.105132704567. Average loss: 0.000049\n",
      "\n",
      "[[ 0.02206676]\n",
      " [-0.04323045]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.01171756]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 450. Moving avg of loss: 0.104072120933. Average loss: 0.000052\n",
      "\n",
      "[[ 0.03001642]\n",
      " [ 0.00172675]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.00426686]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 451. Moving avg of loss: 0.10302074882. Average loss: 0.000052\n",
      "\n",
      "[[ 0.09877598]\n",
      " [ 0.02376897]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.04889033]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 452. Moving avg of loss: 0.101980511913. Average loss: 0.000053\n",
      "\n",
      "[[-0.1111583 ]\n",
      " [ 0.03624506]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.01435328]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 453. Moving avg of loss: 0.100950888174. Average loss: 0.000054\n",
      "\n",
      "[[-0.08579316]\n",
      " [ 0.01181686]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.06397843]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 454. Moving avg of loss: 0.099931253412. Average loss: 0.000055\n",
      "\n",
      "[[ 0.02111347]\n",
      " [ 0.00859502]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.00064659]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 455. Moving avg of loss: 0.098922178408. Average loss: 0.000056\n",
      "\n",
      "[[ 0.07383577]\n",
      " [-0.01021537]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.00936937]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 456. Moving avg of loss: 0.0979238478781. Average loss: 0.000057\n",
      "\n",
      "[[ 0.01021549]\n",
      " [ 0.02568585]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.08658576]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 457. Moving avg of loss: 0.0969359033147. Average loss: 0.000058\n",
      "\n",
      "[[-0.01295786]\n",
      " [ 0.01105031]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.02177473]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 458. Moving avg of loss: 0.0959581215069. Average loss: 0.000059\n",
      "\n",
      "[[ 0.02123552]\n",
      " [ 0.04227645]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.02061272]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 459. Moving avg of loss: 0.0949898398418. Average loss: 0.000060\n",
      "\n",
      "[[-0.0881298 ]\n",
      " [ 0.04343645]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.01801658]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 460. Moving avg of loss: 0.0940313417223. Average loss: 0.000061\n",
      "\n",
      "[[-0.07778326]\n",
      " [-0.17148714]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.00502777]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 461. Moving avg of loss: 0.0930832442061. Average loss: 0.000063\n",
      "\n",
      "[[-0.14228407]\n",
      " [-0.02405245]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.0103811]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 462. Moving avg of loss: 0.0921445168147. Average loss: 0.000064\n",
      "\n",
      "[[-0.07844549]\n",
      " [-0.0630318 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.13580444]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 463. Moving avg of loss: 0.0912158997827. Average loss: 0.000065\n",
      "\n",
      "[[-0.04771394]\n",
      " [-0.02738285]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.03182483]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 464. Moving avg of loss: 0.0902956524567. Average loss: 0.000066\n",
      "\n",
      "[[-0.00469764]\n",
      " [ 0.01299928]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.05509169]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 465. Moving avg of loss: 0.0893843890244. Average loss: 0.000066\n",
      "\n",
      "[[ 0.05790757]\n",
      " [ 0.03833022]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.07262278]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 466. Moving avg of loss: 0.0884831324498. Average loss: 0.000067\n",
      "\n",
      "[[-0.01118896]\n",
      " [ 0.05176757]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.06824386]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 467. Moving avg of loss: 0.0875903813851. Average loss: 0.000067\n",
      "\n",
      "[[ 0.06276994]\n",
      " [-0.07119226]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.12648201]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 468. Moving avg of loss: 0.0867081231758. Average loss: 0.000069\n",
      "\n",
      "[[ 0.07026329]\n",
      " [ 0.06143397]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.12147462]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 469. Moving avg of loss: 0.0858345347778. Average loss: 0.000070\n",
      "\n",
      "[[-0.00765364]\n",
      " [-0.05202163]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.09139207]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 470. Moving avg of loss: 0.0849697257098. Average loss: 0.000071\n",
      "\n",
      "[[ 0.04797092]\n",
      " [ 0.03067323]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.05148715]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 471. Moving avg of loss: 0.0841131268821. Average loss: 0.000072\n",
      "\n",
      "[[-0.00552555]\n",
      " [-0.00350104]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.01831961]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 472. Moving avg of loss: 0.0832650429488. Average loss: 0.000073\n",
      "\n",
      "[[-0.07444138]\n",
      " [ 0.0998533 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.06145579]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 473. Moving avg of loss: 0.0824270482981. Average loss: 0.000074\n",
      "\n",
      "[[ 0.12183955]\n",
      " [-0.05307332]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.06674322]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 474. Moving avg of loss: 0.0815971563731. Average loss: 0.000076\n",
      "\n",
      "[[-0.01797829]\n",
      " [ 0.01695883]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.02088845]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 475. Moving avg of loss: 0.0807751420568. Average loss: 0.000077\n",
      "\n",
      "[[-0.03439013]\n",
      " [ 0.02624116]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.03553689]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 476. Moving avg of loss: 0.0799617381771. Average loss: 0.000078\n",
      "\n",
      "[[-0.10140188]\n",
      " [-0.0003466 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.05042759]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 477. Moving avg of loss: 0.0791563229231. Average loss: 0.000079\n",
      "\n",
      "[[ 0.00472788]\n",
      " [ 0.00812458]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.01414132]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 478. Moving avg of loss: 0.0783585911884. Average loss: 0.000079\n",
      "\n",
      "[[-0.01339601]\n",
      " [-0.02191266]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.01840192]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 479. Moving avg of loss: 0.0775700942339. Average loss: 0.000081\n",
      "\n",
      "[[ 0.00225074]\n",
      " [ 0.21106973]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.02686381]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 480. Moving avg of loss: 0.0767892145642. Average loss: 0.000082\n",
      "\n",
      "[[-0.01491372]\n",
      " [-0.08414792]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.01682305]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 481. Moving avg of loss: 0.0760165409174. Average loss: 0.000083\n",
      "\n",
      "[[-0.06930357]\n",
      " [ 0.05802727]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.05244704]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 482. Moving avg of loss: 0.0752509695968. Average loss: 0.000084\n",
      "\n",
      "[[ 0.02163252]\n",
      " [ 0.02337575]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.00937474]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 483. Moving avg of loss: 0.0744932165626. Average loss: 0.000084\n",
      "\n",
      "[[-0.01943272]\n",
      " [ 0.03291251]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.1198349]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 484. Moving avg of loss: 0.0737437602861. Average loss: 0.000086\n",
      "\n",
      "[[ 0.04988365]\n",
      " [-0.07096013]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.01503732]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 485. Moving avg of loss: 0.0730013419306. Average loss: 0.000086\n",
      "\n",
      "[[-0.00724229]\n",
      " [-0.05302077]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.05499315]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 486. Moving avg of loss: 0.0722666543099. Average loss: 0.000087\n",
      "\n",
      "[[ 0.01254054]\n",
      " [-0.07071429]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.08541131]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 487. Moving avg of loss: 0.0715402769894. Average loss: 0.000089\n",
      "\n",
      "[[-0.03920518]\n",
      " [ 0.09452429]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.07062721]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 488. Moving avg of loss: 0.0708200937747. Average loss: 0.000090\n",
      "\n",
      "[[ 0.08753425]\n",
      " [ 0.00183458]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.06318152]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 489. Moving avg of loss: 0.0701077558746. Average loss: 0.000091\n",
      "\n",
      "[[-0.03202043]\n",
      " [-0.08656199]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.00149655]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 490. Moving avg of loss: 0.0694027160146. Average loss: 0.000092\n",
      "\n",
      "[[ 0.14804298]\n",
      " [-0.06360113]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.05629474]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 491. Moving avg of loss: 0.0687047404641. Average loss: 0.000093\n",
      "\n",
      "[[-0.05669124]\n",
      " [-0.03580036]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.12577695]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 492. Moving avg of loss: 0.0680140539305. Average loss: 0.000094\n",
      "\n",
      "[[ 0.01007741]\n",
      " [-0.00081872]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.03243726]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 493. Moving avg of loss: 0.0673296254466. Average loss: 0.000095\n",
      "\n",
      "[[-0.04481351]\n",
      " [ 0.02165939]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.00306894]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 494. Moving avg of loss: 0.0666523570866. Average loss: 0.000095\n",
      "\n",
      "[[-0.02580303]\n",
      " [ 0.07832988]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.03466189]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 495. Moving avg of loss: 0.0659819577303. Average loss: 0.000096\n",
      "\n",
      "[[ 0.00677989]\n",
      " [ 0.1874647 ]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[-0.00856471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 496. Moving avg of loss: 0.0653195379391. Average loss: 0.000098\n",
      "\n",
      "[[ 0.01843735]\n",
      " [ 0.10124548]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.0681119]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 497. Moving avg of loss: 0.064663098074. Average loss: 0.000099\n",
      "\n",
      "[[-0.04052725]\n",
      " [-0.00720349]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.06049187]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 498. Moving avg of loss: 0.0640127147354. Average loss: 0.000100\n",
      "\n",
      "[[-0.08137797]\n",
      " [-0.13946931]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.02301455]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 499. Moving avg of loss: 0.0633695414948. Average loss: 0.000101\n",
      "\n",
      "[[-0.02867285]\n",
      " [ 0.00968594]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "\n",
      "[ 0.05206299]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Epoch 4, batch 500. Moving avg of loss: 0.0627324403704. Average loss: 0.000102\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "learning_rate = .001\n",
    "niter = 0\n",
    "losses = []\n",
    "moving_loss = 0\n",
    "smoothing_constant = .01\n",
    "\n",
    "# 训练\n",
    "for e in range(epochs):    \n",
    "    total_loss = 0\n",
    "\n",
    "    for data, label in data_iter():\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = square_loss(output, label)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        total_loss += nd.sum(loss).asscalar()\n",
    "\n",
    "        # 记录每读取一个数据点后，损失的移动平均值的变化；\n",
    "        niter +=1\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss\n",
    "\n",
    "        # correct the bias from the moving averages\n",
    "        est_loss = moving_loss/(1-(1-smoothing_constant)**niter)\n",
    "        print(\"Epoch %s, batch %s. Moving avg of loss: %s. Average loss: %f\" % (e, niter, est_loss, total_loss/num_examples))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完成后，我们可以比较学得的参数和真实参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, -3.4], \n",
       " [[ 1.99975812]\n",
       "  [-3.3999877 ]]\n",
       " <NDArray 2x1 @cpu(0)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_w, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.2, \n",
       " [ 4.19976664]\n",
       " <NDArray 1 @cpu(0)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_b, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "我们现在看到，仅仅是使用NDArray和autograd就可以很容易实现的一个模型。在接下来的教程里，我们会在此基础上，介绍更多现代神经网络的知识，以及怎样使用少量的MXNet代码实现各种复杂的模型。\n",
    "\n",
    "## 练习\n",
    "\n",
    "尝试用不同的学习率查看误差下降速度（收敛率）\n",
    "\n",
    "## 讨论\n",
    "\n",
    "欢迎扫码直达[本节内容讨论区](https://discuss.gluon.ai/t/topic/743)："
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
