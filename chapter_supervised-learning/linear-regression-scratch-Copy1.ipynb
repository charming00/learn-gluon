{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归——从零开始\n",
    "\n",
    "尽管强大的深度学习框架可以减少大量重复性工作，但若过于依赖它提供的便利，你就会很难深入理解深度学习是如何工作的。因此，我们的第一个教程是如何只利用ndarray和autograd来实现一个线性回归的训练。\n",
    "\n",
    "## 线性回归\n",
    "\n",
    "给定一个数据点集合`X`和对应的目标值`y`，线性模型的目标就是找到一条使用向量`w`和位移`b`描述的线，来尽可能地近似每个样本`X[i]`和`y[i]`。用数学符号来表示就是：\n",
    "\n",
    "$$\\boldsymbol{\\hat{y}} = X \\boldsymbol{w} + b$$\n",
    "\n",
    "并最小化所有数据点上的平方误差\n",
    "\n",
    "$$\\sum_{i=1}^n (\\hat{y}_i-y_i)^2.$$\n",
    "\n",
    "你可能会对我们把古老的线性回归作为深度学习的一个样例表示奇怪。实际上线性模型是最简单、但也是最有用的神经网络。一个神经网络就是一个由节点（神经元）和有向边组成的集合。我们一般把一些节点组成层，每一层先从下面一层的节点获取输入，然后输出给上面的层使用。要计算一个节点值，我们需要将输入节点值做加权和（权数值即 `w`），然后再加上一个**激活函数（activation function）**。对于线性回归而言，它是一个两层神经网络，其中第一层是（下图橙色点）输入，每个节点对应输入数据点的一个维度，第二层是单输出节点（下图绿色点），它使用身份函数（$f(x)=x$）作为激活函数。\n",
    "\n",
    "![](../img/onelayer.png)\n",
    "\n",
    "## 创建数据集\n",
    "\n",
    "这里我们使用一个数据集来尽量简单地解释清楚，真实的模型是什么样的。具体来说，我们使用如下方法来生成数据；随机数值 `X[i]`，其相应的标注为 `y[i]`：\n",
    "\n",
    "`y[i] = 2 * X[i][0] - 3.4 * X[i][1] + 4.2 + noise`\n",
    "\n",
    "使用数学符号表示：\n",
    "\n",
    "$$y = X \\cdot w + b + \\eta, \\quad \\text{for } \\eta \\sim \\mathcal{N}(0,\\sigma^2)$$\n",
    "\n",
    "这里噪音服从均值0和标准差为0.01的正态分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 13.32999992  12.63000011  13.05000019]\n",
      " [ 13.81000042  13.19999981  13.63000011]\n",
      " [ 14.60000038  14.06000042  14.47999954]\n",
      " [ 14.68000031  14.18000031  14.48999977]\n",
      " [ 13.23999977  12.77999973  13.09000015]\n",
      " [ 13.88000011  13.44999981  13.68000031]\n",
      " [ 13.44999981  12.86999989  13.25      ]\n",
      " [ 13.67000008  13.18000031  13.53999996]\n",
      " [ 13.06999969  12.72000027  12.93999958]\n",
      " [ 13.65999985  13.31999969  13.59000015]\n",
      " [ 12.68999958  11.97999954  12.43999958]\n",
      " [ 16.20000076  15.81000042  16.04000092]\n",
      " [ 12.77999973  12.34000015  12.64000034]\n",
      " [ 12.44999981  11.97999954  12.28999996]\n",
      " [ 12.92000008  12.44999981  12.76000023]\n",
      " [ 14.35000038  13.93000031  14.21000004]\n",
      " [ 14.10999966  13.57999992  13.89000034]\n",
      " [ 12.48999977  11.93999958  12.26000023]\n",
      " [ 13.17000008  12.89000034  13.06999969]\n",
      " [ 13.13000011  12.52999973  12.94999981]\n",
      " [ 11.86999989  11.15999985  11.77999973]\n",
      " [ 14.34000015  13.89000034  14.23999977]\n",
      " [ 15.52999973  14.77999973  15.27999973]\n",
      " [ 13.31999969  12.71000004  13.10000038]\n",
      " [ 11.15999985  10.46000004  10.85999966]\n",
      " [ 15.02999973  14.35000038  14.80000019]\n",
      " [ 14.10999966  13.69999981  13.97000027]\n",
      " [ 14.47999954  13.93999958  14.30000019]\n",
      " [ 13.84000015  13.13000011  13.59000015]\n",
      " [ 12.69999981  12.14999962  12.51000023]\n",
      " [ 14.76000023  14.27999973  14.64000034]\n",
      " [ 13.13000011  12.63000011  13.02000046]\n",
      " [ 14.07999992  13.43000031  13.86999989]\n",
      " [ 14.93999958  14.42000008  14.76000023]\n",
      " [ 13.22999954  12.60999966  13.02000046]\n",
      " [ 12.59000015  12.03999996  12.39000034]\n",
      " [ 13.89999962  13.43000031  13.73999977]\n",
      " [ 12.19999981  11.52000046  11.93000031]\n",
      " [ 14.10000038  13.28999996  13.90999985]\n",
      " [ 12.38000011  11.78999996  12.18000031]\n",
      " [ 15.10000038  14.53999996  15.01000023]\n",
      " [ 13.27000046  12.78999996  13.11999989]\n",
      " [ 13.53999996  13.03999996  13.36999989]\n",
      " [ 12.57999992  12.11999989  12.43000031]\n",
      " [ 12.89999962  12.40999985  12.69999981]\n",
      " [ 13.06999969  12.47999954  12.88000011]\n",
      " [ 12.96000004  12.63000011  12.89000034]\n",
      " [ 13.23999977  12.82999992  13.14999962]\n",
      " [ 13.43000031  12.94999981  13.27000046]\n",
      " [ 14.80000019  14.06999969  14.56000042]\n",
      " [ 13.60999966  13.06999969  13.43999958]\n",
      " [ 15.27999973  14.64000034  15.10999966]\n",
      " [ 13.72999954  13.25        13.60000038]\n",
      " [ 13.65999985  13.02999973  13.43999958]\n",
      " [ 14.          13.51000023  13.85999966]\n",
      " [ 11.65999985  11.05000019  11.43999958]\n",
      " [ 12.93999958  12.42000008  12.76000023]\n",
      " [ 12.22000027  11.63000011  12.02000046]\n",
      " [ 13.21000004  12.48999977  12.97999954]\n",
      " [ 13.46000004  12.93999958  13.27999973]\n",
      " [ 14.60000038  14.02000046  14.43000031]\n",
      " [ 13.26000023  12.59000015  13.02999973]\n",
      " [ 13.96000004  13.27000046  13.71000004]\n",
      " [ 13.14999962  12.47000027  12.92000008]\n",
      " [ 13.93999958  13.31000042  13.75      ]\n",
      " [ 13.22000027  12.69999981  13.06999969]\n",
      " [ 14.43000031  13.89999962  14.23999977]\n",
      " [ 15.43999958  14.52999973  15.17000008]\n",
      " [ 12.14999962  11.57999992  11.93000031]\n",
      " [ 12.27999973  11.77000046  12.11999989]\n",
      " [ 12.90999985  12.61999989  12.86999989]\n",
      " [ 13.43000031  12.81999969  13.22000027]\n",
      " [ 13.51000023  12.81000042  13.27000046]\n",
      " [ 14.77000046  14.02999973  14.51000023]\n",
      " [ 12.80000019  12.34000015  12.68000031]\n",
      " [ 14.02999973  13.32999992  13.75      ]\n",
      " [ 13.82999992  13.17000008  13.64999962]\n",
      " [ 14.53999996  14.          14.32999992]\n",
      " [ 14.85999966  14.48999977  14.77999973]\n",
      " [ 11.90999985  11.60999966  11.84000015]\n",
      " [ 12.88000011  12.22999954  12.67000008]\n",
      " [ 13.56999969  12.97999954  13.35000038]\n",
      " [ 12.90999985  12.39999962  12.77000046]\n",
      " [ 11.93999958  11.40999985  11.68999958]\n",
      " [ 11.84000015  11.19999981  11.59000015]\n",
      " [ 15.05000019  14.35999966  14.85000038]\n",
      " [ 13.40999985  12.82999992  13.30000019]\n",
      " [ 12.61999989  12.02999973  12.36999989]\n",
      " [ 15.02999973  14.52999973  14.89999962]\n",
      " [ 11.92000008  11.22999954  11.67000008]\n",
      " [ 13.48999977  12.82999992  13.27000046]\n",
      " [ 14.31000042  13.81000042  14.17000008]\n",
      " [ 13.27000046  12.47000027  13.02000046]\n",
      " [ 14.38000011  13.65999985  14.14000034]\n",
      " [ 14.05000019  13.36999989  13.84000015]\n",
      " [ 12.56000042  11.92000008  12.36999989]\n",
      " [ 14.53999996  13.97000027  14.36999989]\n",
      " [ 14.03999996  13.38000011  13.81999969]\n",
      " [ 14.          13.61999989  13.89999962]\n",
      " [ 12.76000023  12.38000011  12.68999958]\n",
      " [ 13.59000015  12.97000027  13.46000004]\n",
      " [ 13.25        12.64999962  13.02000046]\n",
      " [ 13.02999973  12.40999985  12.84000015]\n",
      " [ 13.97000027  13.35999966  13.75      ]\n",
      " [ 12.31000042  11.64000034  12.07999992]\n",
      " [ 15.26000023  14.35000038  15.        ]\n",
      " [ 13.72999954  13.34000015  13.68999958]\n",
      " [ 14.06999969  13.42000008  13.89000034]\n",
      " [ 12.14999962  11.56999969  11.92000008]\n",
      " [ 15.09000015  14.60999966  14.97000027]\n",
      " [ 13.34000015  12.71000004  13.14999962]\n",
      " [ 14.05000019  13.43000031  13.86999989]\n",
      " [ 13.5         12.92000008  13.32999992]\n",
      " [ 13.68000031  13.14000034  13.52999973]\n",
      " [ 13.77999973  13.10000038  13.60000038]\n",
      " [ 15.43999958  14.61999989  15.18999958]\n",
      " [ 13.47999954  13.06999969  13.32999992]\n",
      " [ 11.81000042  11.28999996  11.61999989]\n",
      " [ 13.06999969  12.73999977  12.94999981]\n",
      " [ 14.27000046  13.75        14.07999992]\n",
      " [ 13.10999966  12.60999966  12.93999958]\n",
      " [ 11.25        10.72999954  11.06000042]\n",
      " [ 12.97000027  12.46000004  12.82999992]\n",
      " [ 13.82999992  13.15999985  13.56999969]\n",
      " [ 13.14000034  12.47000027  12.92000008]\n",
      " [ 14.67000008  14.15999985  14.52000046]\n",
      " [ 14.19999981  13.73999977  14.06999969]\n",
      " [ 13.09000015  12.48999977  12.90999985]\n",
      " [ 13.56999969  13.18000031  13.43999958]\n",
      " [ 15.19999981  14.68000031  15.05000019]\n",
      " [ 12.52000046  11.92000008  12.28999996]\n",
      " [ 13.47999954  13.02000046  13.35000038]\n",
      " [ 12.88000011  12.43999958  12.77000046]\n",
      " [ 12.35999966  11.89000034  12.19999981]\n",
      " [ 14.77000046  14.34000015  14.63000011]\n",
      " [ 14.10999966  13.42000008  13.86999989]\n",
      " [ 15.22000027  14.68999958  15.03999996]\n",
      " [ 12.89000034  12.56999969  12.81000042]\n",
      " [ 13.          12.47000027  12.84000015]\n",
      " [ 12.86999989  12.35000038  12.71000004]\n",
      " [ 13.27999973  12.75        13.06000042]\n",
      " [ 12.89000034  12.34000015  12.75      ]\n",
      " [ 14.34000015  13.96000004  14.26000023]\n",
      " [ 14.13000011  13.60000038  13.98999977]\n",
      " [ 14.69999981  14.27000046  14.55000019]\n",
      " [ 11.78999996  11.06000042  11.46000004]\n",
      " [ 12.44999981  11.97000027  12.28999996]\n",
      " [ 12.06999969  11.43999958  11.84000015]\n",
      " [ 14.30000019  13.80000019  14.14999962]\n",
      " [ 11.96000004  11.38000011  11.76000023]]\n",
      "<NDArray 150x3 @cpu(0)>\n",
      "\n",
      "[ 13.81999969  14.59000015  15.26000023  15.26000023  13.80000019\n",
      "  14.72999954  13.78999996  14.36999989  14.21000004  14.44999981\n",
      "  13.14000034  16.93000031  13.56000042  13.11999989  13.65999985\n",
      "  14.94999981  14.71000004  13.34000015  13.96000004  13.65999985\n",
      "  12.73999977  15.02000046  16.38999939  14.01000023  12.06000042\n",
      "  15.92000008  14.93999958  15.18000031  14.39999962  13.89999962\n",
      "  15.61999989  13.90999985  14.67000008  15.22999954  14.02999973\n",
      "  13.27999973  14.69999981  12.60000038  15.14999962  12.89999962\n",
      "  15.81999969  14.27000046  14.07999992  13.34000015  13.84000015\n",
      "  13.85999966  13.80000019  14.30000019  14.10000038  15.68999958\n",
      "  14.32999992  16.44000053  14.65999985  14.60000038  14.73999977\n",
      "  12.56000042  13.64000034  13.25        13.84000015  14.26000023\n",
      "  15.64000034  13.68999958  14.81000042  13.97999954  14.52000046\n",
      "  13.60000038  14.97000027  16.53000069  13.26000023  13.22999954\n",
      "  14.32999992  14.27999973  14.53999996  15.69999981  13.53999996\n",
      "  15.02000046  15.18999958  15.22999954  15.60000038  13.02000046\n",
      "  13.05000019  14.17000008  13.44999981  12.46000004  12.30000019\n",
      "  15.53999996  14.13000011  13.14999962  15.85000038  12.72000027\n",
      "  14.35999966  14.86999989  14.28999996  14.97000027  15.25        13.36999989\n",
      "  15.26000023  14.77000046  14.53999996  13.75        14.31000042\n",
      "  14.14999962  13.78999996  14.86999989  12.73999977  16.18000031\n",
      "  14.35000038  14.82999992  13.05000019  15.81999969  14.01000023\n",
      "  14.46000004  14.56000042  14.36999989  14.47000027  16.06999969\n",
      "  14.43000031  12.55000019  13.78999996  15.21000004  13.88000011  11.75\n",
      "  14.06999969  13.61999989  13.72000027  15.30000019  14.97999954\n",
      "  13.94999981  14.36999989  15.68999958  13.22999954  14.31000042\n",
      "  13.78999996  13.13000011  15.44999981  14.64999962  15.71000004\n",
      "  13.63000011  13.89000034  13.10999966  14.21000004  13.60000038\n",
      "  15.44999981  15.03999996  15.06999969  12.31000042  13.35000038\n",
      "  12.81000042  14.73999977  12.76000023]\n",
      "<NDArray 150 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "\n",
    "data_root = '../data'\n",
    "txt_root = data_root + '/CephalometricLandmark/AnnotationsByMD'\n",
    "landmark_index = 15\n",
    "num_examples = 150\n",
    "num_inputs = 3\n",
    "rate = 100\n",
    "X = nd.zeros(shape=(num_examples, num_inputs))\n",
    "y = nd.zeros(shape=(num_examples,))\n",
    "for i in range(num_examples):\n",
    "    txt_filename1 = txt_root + '/400_senior' + \"/%03d.txt\" % (i+1)\n",
    "    with open(txt_filename1, 'r') as f:\n",
    "        txts = f.read().split()\n",
    "    x7 = int(txts[6].split(',')[0])\n",
    "    X[i][0] = x7/rate\n",
    "    \n",
    "    x8 = int(txts[7].split(',')[0])\n",
    "    X[i][1] = x8/rate\n",
    "    \n",
    "    x9 = int(txts[8].split(',')[0])\n",
    "    X[i][2] = x9/rate\n",
    "    \n",
    "    x16 = int(txts[15].split(',')[0])\n",
    "    y[i] = x16/rate\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到`X`的每一行是一个长度为2的向量，而`y`的每一行是一个长度为1的向量（标量）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 13.32999992  12.63000011  13.05000019]\n",
      "<NDArray 3 @cpu(0)> \n",
      "[ 13.81999969]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(X[0], y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果有兴趣，可以使用安装包中已包括的 Python 绘图包 `matplotlib`，生成第二个特征值 (`X[:, 1]`) 和目标值 `Y` 的散点图，更直观地观察两者间的关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取\n",
    "\n",
    "当我们开始训练神经网络的时候，我们需要不断读取数据块。这里我们定义一个函数它每次返回`batch_size`个随机的样本和对应的目标。我们通过python的`yield`来构造一个迭代器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "batch_size = 15\n",
    "def data_iter():\n",
    "    # 产生一个随机索引\n",
    "    idx = list(range(num_examples))\n",
    "    random.shuffle(idx)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = nd.array(idx[i:min(i+batch_size,num_examples)])\n",
    "        yield nd.take(X, j), nd.take(y, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面代码读取第一个随机数据块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 13.60999966  13.06999969  13.43999958]\n",
      " [ 15.02999973  14.52999973  14.89999962]\n",
      " [ 12.61999989  12.02999973  12.36999989]\n",
      " [ 12.89000034  12.34000015  12.75      ]\n",
      " [ 12.92000008  12.44999981  12.76000023]\n",
      " [ 11.84000015  11.19999981  11.59000015]\n",
      " [ 15.19999981  14.68000031  15.05000019]\n",
      " [ 12.14999962  11.56999969  11.92000008]\n",
      " [ 13.          12.47000027  12.84000015]\n",
      " [ 13.27000046  12.47000027  13.02000046]\n",
      " [ 11.81000042  11.28999996  11.61999989]\n",
      " [ 13.47999954  13.06999969  13.32999992]\n",
      " [ 14.35000038  13.93000031  14.21000004]\n",
      " [ 12.90999985  12.39999962  12.77000046]\n",
      " [ 13.13000011  12.63000011  13.02000046]]\n",
      "<NDArray 15x3 @cpu(0)> \n",
      "[ 14.32999992  15.85000038  13.14999962  13.60000038  13.65999985\n",
      "  12.30000019  15.68999958  13.05000019  13.89000034  14.28999996\n",
      "  12.55000019  14.43000031  14.94999981  13.44999981  13.90999985]\n",
      "<NDArray 15 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "for data, label in data_iter():\n",
    "    print(data, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型参数\n",
    "\n",
    "下面我们随机初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "w = nd.random_normal(shape=(num_inputs, 1))\n",
    "b = nd.zeros((1,))\n",
    "params = [w, b]\n",
    "print(w.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后训练时我们需要对这些参数求导来更新它们的值，使损失尽量减小；因此我们需要创建它们的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "线性模型就是将输入和模型的权重（`w`）相乘，再加上偏移（`b`）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return nd.dot(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "我们使用常见的平方误差来衡量预测目标和真实目标之间的差距。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square_loss(yhat, y):\n",
    "    # 注意这里我们把y变形成yhat的形状来避免矩阵形状的自动转换\n",
    "#     print(yhat.shape,y.shape)\n",
    "    return nd.abs(yhat - y.reshape(yhat.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化\n",
    "\n",
    "虽然线性回归有显式解，但绝大部分模型并没有。所以我们这里通过随机梯度下降来求解。每一步，我们将模型参数沿着梯度的反方向走特定距离，这个距离一般叫**学习率（learning rate）** `lr`。（我们会之后一直使用这个函数，我们将其保存在[utils.py](../utils.py)。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "#         print(param.grad)\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "现在我们可以开始训练了。训练通常需要迭代数据数次，在这里使用`epochs`表示迭代总次数；一次迭代中，我们每次随机读取固定数个数据点，计算梯度并更新模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 模型函数\n",
    "def real_fn(X):\n",
    "    return true_w[0] * X[:, 0] + true_w[1] * X[:, 1] + true_b\n",
    "# 绘制损失随训练次数降低的折线图，以及预测值和真实值的散点图\n",
    "def plot(losses, X, sample_size=100):\n",
    "    xs = list(range(len(losses)))\n",
    "    f, (fg1, fg2) = plt.subplots(1, 2)\n",
    "    fg1.set_title('Loss during training')\n",
    "    fg1.plot(xs, losses, '-r')\n",
    "    fg2.set_title('Estimated vs real function')\n",
    "    fg2.plot(X[:sample_size, 1].asnumpy(),\n",
    "             net(X[:sample_size, :]).asnumpy(), 'or', label='Estimated')\n",
    "    fg2.plot(X[:sample_size, 1].asnumpy(),\n",
    "             real_fn(X[:sample_size, :]).asnumpy(), '*g', label='Real')\n",
    "    fg2.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 10. Moving avg of loss: 4.01661671144. Average loss: 4.007568\n",
      "Epoch 1, batch 20. Moving avg of loss: 4.00612407663. Average loss: 3.989294\n",
      "Epoch 2, batch 30. Moving avg of loss: 3.99487756756. Average loss: 3.968955\n",
      "Epoch 3, batch 40. Moving avg of loss: 3.9849026891. Average loss: 3.953565\n",
      "Epoch 4, batch 50. Moving avg of loss: 3.98205239113. Average loss: 3.964510\n",
      "Epoch 5, batch 60. Moving avg of loss: 3.98177751503. Average loss: 3.974949\n",
      "Epoch 6, batch 70. Moving avg of loss: 3.98301235055. Average loss: 3.980396\n",
      "Epoch 7, batch 80. Moving avg of loss: 3.98525789958. Average loss: 3.983772\n",
      "Epoch 8, batch 90. Moving avg of loss: 3.98651615434. Average loss: 3.980146\n",
      "Epoch 9, batch 100. Moving avg of loss: 3.9855182539. Average loss: 3.970090\n",
      "Epoch 10, batch 110. Moving avg of loss: 3.98468845267. Average loss: 3.966613\n",
      "Epoch 11, batch 120. Moving avg of loss: 3.98744180037. Average loss: 3.991291\n",
      "Epoch 12, batch 130. Moving avg of loss: 3.99032747547. Average loss: 3.998045\n",
      "Epoch 13, batch 140. Moving avg of loss: 3.98575206409. Average loss: 3.941684\n",
      "Epoch 14, batch 150. Moving avg of loss: 3.9870767634. Average loss: 3.986073\n",
      "Epoch 15, batch 160. Moving avg of loss: 3.98666210647. Average loss: 3.974420\n",
      "Epoch 16, batch 170. Moving avg of loss: 3.98599816788. Average loss: 3.966596\n",
      "Epoch 17, batch 180. Moving avg of loss: 3.9714675145. Average loss: 3.840601\n",
      "Epoch 18, batch 190. Moving avg of loss: 3.97066771466. Average loss: 3.949255\n",
      "Epoch 19, batch 200. Moving avg of loss: 3.95881424493. Average loss: 3.846789\n",
      "Epoch 20, batch 210. Moving avg of loss: 3.95134162758. Average loss: 3.880389\n",
      "Epoch 21, batch 220. Moving avg of loss: 3.95487969308. Average loss: 3.972896\n",
      "Epoch 22, batch 230. Moving avg of loss: 3.96016690973. Average loss: 3.991074\n",
      "Epoch 23, batch 240. Moving avg of loss: 3.96335681517. Average loss: 3.975838\n",
      "Epoch 24, batch 250. Moving avg of loss: 3.95432295236. Average loss: 3.863414\n",
      "Epoch 25, batch 260. Moving avg of loss: 3.95825618986. Average loss: 3.982303\n",
      "Epoch 26, batch 270. Moving avg of loss: 3.9615258573. Average loss: 3.980459\n",
      "Epoch 27, batch 280. Moving avg of loss: 3.963070331. Average loss: 3.966610\n",
      "Epoch 28, batch 290. Moving avg of loss: 3.96822689278. Average loss: 4.008349\n",
      "Epoch 29, batch 300. Moving avg of loss: 3.96960400017. Average loss: 3.971530\n",
      "Epoch 30, batch 310. Moving avg of loss: 3.96974292963. Average loss: 3.963111\n",
      "Epoch 31, batch 320. Moving avg of loss: 3.97248196457. Average loss: 3.991512\n",
      "Epoch 32, batch 330. Moving avg of loss: 3.97491392696. Average loss: 3.989679\n",
      "Epoch 33, batch 340. Moving avg of loss: 3.97431856803. Average loss: 3.960428\n",
      "Epoch 34, batch 350. Moving avg of loss: 3.97433296409. Average loss: 3.964419\n",
      "Epoch 35, batch 360. Moving avg of loss: 3.97522334961. Average loss: 3.976451\n",
      "Epoch 36, batch 370. Moving avg of loss: 3.97646927714. Average loss: 3.977724\n",
      "Epoch 37, batch 380. Moving avg of loss: 3.97487226508. Average loss: 3.949851\n",
      "Epoch 38, batch 390. Moving avg of loss: 3.97997127721. Average loss: 4.014287\n",
      "Epoch 39, batch 400. Moving avg of loss: 3.98014219716. Average loss: 3.971872\n",
      "Epoch 40, batch 410. Moving avg of loss: 3.97874706687. Average loss: 3.960270\n",
      "Epoch 41, batch 420. Moving avg of loss: 3.98025852826. Average loss: 3.988347\n",
      "Epoch 42, batch 430. Moving avg of loss: 3.97966877227. Average loss: 3.968643\n",
      "Epoch 43, batch 440. Moving avg of loss: 3.97880528044. Average loss: 3.965411\n",
      "Epoch 44, batch 450. Moving avg of loss: 3.98058283461. Average loss: 3.992741\n",
      "Epoch 45, batch 460. Moving avg of loss: 3.97938479872. Average loss: 3.958826\n",
      "Epoch 46, batch 470. Moving avg of loss: 3.98089767971. Average loss: 3.987931\n",
      "Epoch 47, batch 480. Moving avg of loss: 3.98275318268. Average loss: 3.993371\n",
      "Epoch 48, batch 490. Moving avg of loss: 3.98654895825. Average loss: 4.009111\n",
      "Epoch 49, batch 500. Moving avg of loss: 3.98956069699. Average loss: 4.003769\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "learning_rate = .001\n",
    "niter = 0\n",
    "losses = []\n",
    "moving_loss = 0\n",
    "smoothing_constant = .01\n",
    "\n",
    "# 训练\n",
    "for e in range(epochs):    \n",
    "    total_loss = 0\n",
    "\n",
    "    for data, label in data_iter():\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = square_loss(output, label)\n",
    "#         print(loss)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        total_loss += nd.sum(loss).asscalar()\n",
    "\n",
    "        # 记录每读取一个数据点后，损失的移动平均值的变化；\n",
    "        niter +=1\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss\n",
    "\n",
    "        # correct the bias from the moving averages\n",
    "        est_loss = moving_loss/(1-(1-smoothing_constant)**niter)\n",
    "    print(\"Epoch %s, batch %s. Moving avg of loss: %s. Average loss: %f\" % (e, niter, est_loss, total_loss/num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完成后，我们可以比较学得的参数和真实参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 1306.29125977]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1382.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1351.49914551]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1459.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1427.32739258]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1526.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1434.21875]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1526.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1292.82739258]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1380.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1354.64648438]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1473.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1315.69787598]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1379.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1335.46508789]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1437.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1274.01977539]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1421.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1331.53405762]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1445.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1244.03442383]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1314.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1580.484375]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1693.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1247.50964355]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1356.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1215.85791016]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1312.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1261.76147461]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1366.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1400.44091797]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1495.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1379.11083984]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1471.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1221.28076172]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1334.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1282.41101074]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1396.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1284.88220215]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1366.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1164.19763184]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1274.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1400.13635254]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1502.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1522.22216797]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1639.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1303.57946777]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1401.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1094.32226562]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1206.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1471.99743652]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1592.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1376.79736328]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1494.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1415.51367188]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1518.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1356.35168457]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1440.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1241.85351562]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1390.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1441.73510742]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1562.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1282.95947266]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1391.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1378.63525391]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1467.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1460.03393555]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1523.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1295.00842285]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1403.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1231.09448242]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1328.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1357.4753418]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1470.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1195.53649902]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1260.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1383.87133789]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1515.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1211.3972168]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1290.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1476.61450195]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1582.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1296.16394043]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1427.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1322.90917969]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1408.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1228.36694336]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1334.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1260.15197754]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1384.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1278.80310059]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1386.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1262.96386719]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1380.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1291.90515137]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1430.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1311.77502441]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1410.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1450.53442383]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1569.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1330.55871582]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1433.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1495.69506836]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1644.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1341.12194824]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1466.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1337.19287109]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1460.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1367.67993164]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1474.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1141.4519043]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1256.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1264.69958496]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1364.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1195.77050781]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1325.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1295.0559082]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1384.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1315.48657227]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1426.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1428.06213379]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1564.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1298.9230957]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1369.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1367.66516113]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1481.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1288.38293457]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1398.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1364.58654785]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1452.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1292.09338379]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1360.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1410.41137695]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1497.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1516.65234375]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1653.00012207]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1188.49597168]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1326.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1200.06726074]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1323.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1257.3145752]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1433.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1314.33862305]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1428.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1323.93395996]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1454.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1447.77636719]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1570.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1249.90075684]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1354.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1374.6583252]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1502.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1354.46838379]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1519.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1421.32678223]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1523.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1449.32885742]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1560.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1159.80371094]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1302.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1261.43444824]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1305.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1327.58984375]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1417.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1261.62890625]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1345.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1167.12609863]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1246.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1159.59484863]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1230.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1474.20092773]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1554.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1311.93188477]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1413.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1234.7590332]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1315.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1468.49597168]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1585.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1168.42431641]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1272.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1321.19897461]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1436.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1398.15991211]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1487.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1302.51049805]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1429.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1409.31091309]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1497.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1376.31494141]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1525.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1230.0090332]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1337.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1421.99890137]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1526.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1374.9161377]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1477.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1365.50708008]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1454.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1244.4465332]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1375.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1330.29370117]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1431.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1296.52392578]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1415.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1275.50610352]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1379.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1367.06347656]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1487.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1206.1394043]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1274.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1499.08789062]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1618.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1339.43371582]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1435.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1377.70532227]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1483.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1188.68359375]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1305.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1473.96520996]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1582.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1305.98632812]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1401.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1375.14245605]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1446.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1320.62817383]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1456.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1337.42675781]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1437.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1349.99145508]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1447.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1514.85461426]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1607.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1315.25134277]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1443.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1154.32006836]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1255.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1273.62902832]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1379.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1394.5814209]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1521.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1280.91223145]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1388.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1099.62646484]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1175.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1267.48901367]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1407.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1354.54650879]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1362.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1287.21875]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1372.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1433.50756836]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1530.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1386.6192627]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1498.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1280.97546387]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1395.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1323.66625977]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1437.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1485.4744873]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1569.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1225.22692871]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1323.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1316.29870605]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1431.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1257.32324219]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1379.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1207.06774902]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1313.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1441.66442871]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1545.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1382.33093262]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1465.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1487.58410645]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1571.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1255.90844727]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1363.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1270.79406738]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1389.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1257.89416504]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1311.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1298.04711914]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1421.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1260.48852539]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1360.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1398.74511719]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1545.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1381.1895752]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1504.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1434.81188965]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1507.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1156.41516113]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1231.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1216.06115723]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1335.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1181.88623047]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1281.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1397.16760254]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1474.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1170.17370605]\n",
      "<NDArray 1 @cpu(0)> \n",
      "[ 1276.]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "test_X = nd.zeros(shape=(150, num_inputs))\n",
    "test_y = nd.zeros(shape=(150,))\n",
    "for i in range(151,301):\n",
    "    txt_filename1 = txt_root + '/400_senior' + \"/%03d.txt\" % i\n",
    "    with open(txt_filename1, 'r') as f:\n",
    "        txts = f.read().split()\n",
    "    x7 = int(txts[6].split(',')[0])\n",
    "    test_X[i-151][0] = x7/1000\n",
    "    \n",
    "    x8 = int(txts[7].split(',')[0])\n",
    "    test_X[i-151][1] = x8/1000\n",
    "    \n",
    "    x9 = int(txts[8].split(',')[0])\n",
    "    test_X[i-151][2] = x9/1000\n",
    "    \n",
    "    x16 = int(txts[15].split(',')[0])\n",
    "    test_y[i-151] = x16/1000\n",
    "py = net(X)\n",
    "for i in range(150):\n",
    "    print(rate*py[i],rate*y[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "我们现在看到，仅仅是使用NDArray和autograd就可以很容易实现的一个模型。在接下来的教程里，我们会在此基础上，介绍更多现代神经网络的知识，以及怎样使用少量的MXNet代码实现各种复杂的模型。\n",
    "\n",
    "## 练习\n",
    "\n",
    "尝试用不同的学习率查看误差下降速度（收敛率）\n",
    "\n",
    "## 讨论\n",
    "\n",
    "欢迎扫码直达[本节内容讨论区](https://discuss.gluon.ai/t/topic/743)："
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
