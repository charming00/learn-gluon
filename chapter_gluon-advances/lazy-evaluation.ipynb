{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 惰性计算\n",
    "\n",
    "MXNet使用惰性计算（lazy evaluation）来提升计算性能。理解它的工作原理既有助于开发更高效的程序，又有助于在内存资源有限的情况下主动降低计算性能从而减小内存消耗。\n",
    "\n",
    "我们先导入本节中实验需要的包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd, gluon, nd\n",
    "from mxnet.gluon import nn\n",
    "import os\n",
    "import subprocess\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "惰性计算是指程序中定义的计算在结果真正被取用的时候才执行。我们先看下面这个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "a = 1 + 1\n",
    "a = 2 + 2\n",
    "a = 3 + 3\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中，前三句都在对变量`a`赋值，最后一句打印变量`a`的计算结果。事实上，我们可以把前三句赋值语句延迟到需要执行最后的打印语句时才计算。这样的主要好处是系统在计算变量`a`时已经看到了全部有关计算`a`的指令，从而有更多空间优化计算。例如，这里我们并不需要对前两句赋值语句做计算就可以在最后一句中得到正确的`a`的值。\n",
    "\n",
    "\n",
    "## MXNet中的惰性计算\n",
    "\n",
    "广义上，MXNet包括用户直接用来交互的前端和系统用来执行计算的后端。例如，用户可以使用不同的前端语言编写MXNet程序，例如Python、R、Scala和C++。无论使用何种前端编程语言，MXNet程序的执行主要都发生在C++后端。换句话说，用户写好的前端MXNet程序会传给后端执行计算。后端有自己的线程来不断收集任务，构造、优化并执行计算图。\n",
    "\n",
    "\n",
    "本章我们介绍后端优化之一：延迟执行。\n",
    "\n",
    "考虑下图的样例，我们在前端调用四条语句，它们被后端的线程分析依赖并构建成计算图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 3.  3.]]\n",
      "<NDArray 1x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "a = nd.ones((1, 2))\n",
    "b = nd.ones((1, 2))\n",
    "c = a * b + 2\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/frontend-backend.svg)\n",
    "\n",
    "在延迟执行中，前端执行前三个语句的时候，它仅仅是把任务放进后端的队列里就返回了。当在需要打印结果时，前端会等待后端线程把`c`的结果计算完。\n",
    "\n",
    "这个设计的一个好处是前端，就是Python线程，不需要做实际计算工作，从而不管Python的性能如何，它对整个程序的影响会很小。只需要C++后端足够高效，那么不管前端语言性能如何，都可以提供一致的性能。\n",
    "\n",
    "下面的例子通过计时来展示了延后执行的效果。可以看到，当`y=...`返回的时候并没有等待它真的被计算完。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workloads are queued:\t0.000450 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 501.15838623  508.29724121  495.65237427 ...,  492.8470459   492.69091797\n",
      "   490.0480957 ]\n",
      " [ 508.81057739  507.18218994  495.17428589 ...,  503.10525513\n",
      "   497.29315186  493.6791687 ]\n",
      " [ 489.565979    499.47015381  490.17721558 ...,  490.99945068\n",
      "   488.05007935  483.2883606 ]\n",
      " ..., \n",
      " [ 484.00189209  495.71789551  479.92141724 ...,  493.69952393\n",
      "   478.89193726  487.20739746]\n",
      " [ 499.64932251  507.65093994  497.59381104 ...,  493.0473938   500.74511719\n",
      "   495.82711792]\n",
      " [ 516.01428223  519.17150879  506.35400391 ...,  510.08877563  496.3560791\n",
      "   495.42523193]]\n",
      "<NDArray 2000x2000 @cpu(0)>\n",
      "workloads are finished:\t0.413521 sec\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "x = nd.random.uniform(shape=(2000, 2000))\n",
    "y = nd.dot(x, x)\n",
    "print('workloads are queued:\\t%f sec' % (time() - start))\n",
    "print(y)\n",
    "print('workloads are finished:\\t%f sec' % (time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "延迟执行大部分情况是对用户透明的。因为除非我们需要打印或者保存结果外，我们基本不需要关心目前是不是结果在内存里面已经计算好了。\n",
    "\n",
    "事实上，只要数据是保存在NDArray里，而且使用MXNet提供的运算子，后端将默认使用延迟执行来获取最大的性能。\n",
    "\n",
    "## 立即获取结果\n",
    "\n",
    "除了前面介绍的`print`外，我们还有别的方法可以让前端线程等待直到结果完成。我们可以使用`nd.NDArray.wait_to_read()`等待直到特定结果完成，或者`nd.waitall()`等待所有前面结果完成。后者是测试性能常用方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20882558822631836"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "y = nd.dot(x, x)\n",
    "y.wait_to_read()\n",
    "time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34374237060546875"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "y = nd.dot(x, x)\n",
    "z = nd.dot(x, x)\n",
    "nd.waitall()\n",
    "time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任何方法将内容从NDArray搬运到其他不支持延迟执行的数据结构里都会触发等待，例如`asnumpy()`, `asscalar()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32565784454345703"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "y = nd.dot(x, x)\n",
    "y.asnumpy()\n",
    "time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16484570503234863"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "y = nd.dot(x, x)\n",
    "y.norm().asscalar()\n",
    "time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 延迟执行带来的便利\n",
    "\n",
    "下面例子中，我们不断的对`y`进行赋值。如果每次我们需要等到`y`的值，那么我们必须要要计算它。而在延迟执行里，系统有可能省略掉一些执行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lazy evaluation: 1.196231 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With evaluation: 0.778286 sec\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "for i in range(1000):\n",
    "    y = x + 1\n",
    "    y.wait_to_read()\n",
    "\n",
    "print('No lazy evaluation: %f sec' % (time()-start))\n",
    "\n",
    "start = time()\n",
    "for i in range(1000):\n",
    "    y = x + 1\n",
    "nd.waitall()\n",
    "print('With evaluation: %f sec' % (time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 延迟执行带来的影响\n",
    "\n",
    "在延迟执行里，只要最终结果是一致的，系统可能使用跟代码不一样的顺序来执行，例如假设我们写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1\n",
    "b = 2\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一句和第二句之间没有依赖，所以把`b=2`提前到`a=1`前执行也是可以的。但这样可能会导致内存使用的变化。\n",
    "\n",
    "下面我们列举几个在训练和预测中常见的现象。一般每个批量我们都会评测一下，例如计算损失或者精度，其中会用到`asscalar`或者`asnumpy`。这样我们会每次仅仅将一个批量的任务放进后端系统执行。但如果我们去掉这些同步函数，会导致我们将大量的批量任务同时放进系统，从而可能导致系统占用过多资源。\n",
    "\n",
    "为了演示这种情况，我们定义一个数据获取函数，它会打印什么数据是什么时候被请求的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    start = time()\n",
    "    batch_size = 1024\n",
    "    for i in range(60):\n",
    "        if i % 10 == 0:\n",
    "            print('batch %d, time %f sec' %(i, time()-start))\n",
    "        x = nd.ones((batch_size, 1024))\n",
    "        y = nd.ones((batch_size,))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用两层网络和和L2损失函数作为样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(\n",
    "        nn.Dense(1024, activation='relu'),\n",
    "        nn.Dense(1024, activation='relu'),\n",
    "        nn.Dense(1),\n",
    "    )\n",
    "net.initialize()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {})\n",
    "loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们定义辅助函数来监测内存的使用（只能在Linux运行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "def get_mem():\n",
    "    \"\"\"get memory usage in MB\"\"\"\n",
    "    res = subprocess.check_output(['ps', 'u', '-p', str(os.getpid())])\n",
    "    return int(str(res).split()[15])/1e3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以做测试了。我们先试运行一次让系统把`net`的参数初始化（回忆[延后初始化](../chapter_gluon-basics/parameters.md)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, time 0.000002 sec\n"
     ]
    }
   ],
   "source": [
    "for x, y in get_data():\n",
    "    break\n",
    "loss(y, net(x)).wait_to_read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们用`net`来做预测，正常情况下对每个批量的结果我们把它复制出NDArray，例如打印或者保存在磁盘上。这里我们简单使用`wait_to_read`来模拟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, time 0.000003 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 10, time 0.445737 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 20, time 0.806322 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 30, time 1.166639 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 40, time 1.526855 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 50, time 1.888729 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increased memory 12.304000 MB\n"
     ]
    }
   ],
   "source": [
    "mem = get_mem()\n",
    "\n",
    "for x, y in get_data():\n",
    "    loss(y, net(x)).wait_to_read()\n",
    "nd.waitall()\n",
    "\n",
    "print('Increased memory %f MB' % (get_mem() - mem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们不使用`wait_to_read()`， 那么前端会将所有批量的计算一次性的添加进后端。可以看到每个批量的数据都会在很短的时间内生成，同时在接下来的数秒钟内，我们看到了内存的增长（包括了在内存中保存所有`x`和`y`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, time 0.000004 sec\n",
      "batch 10, time 0.006569 sec\n",
      "batch 20, time 0.012304 sec\n",
      "batch 30, time 0.017973 sec\n",
      "batch 40, time 0.023712 sec\n",
      "batch 50, time 0.029343 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increased memory 233.724000 MB\n"
     ]
    }
   ],
   "source": [
    "mem = get_mem()\n",
    "\n",
    "for x, y in get_data():\n",
    "    loss(y, net(x))\n",
    "\n",
    "nd.waitall()\n",
    "print('Increased memory %f MB' % (get_mem() - mem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样对于训练，如果我们每次计算损失，那么就加入了同步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, time 0.000003 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 10, time 0.988995 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 20, time 1.997673 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 30, time 3.006102 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 40, time 4.013228 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 50, time 5.020585 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increased memory -56.300000 MB\n"
     ]
    }
   ],
   "source": [
    "mem = get_mem()\n",
    "\n",
    "total_loss = 0\n",
    "for x, y in get_data():\n",
    "    with autograd.record():\n",
    "        L = loss(y, net(x))\n",
    "    total_loss += L.sum().asscalar()\n",
    "    L.backward()\n",
    "    trainer.step(x.shape[0])\n",
    "\n",
    "nd.waitall()\n",
    "print('Increased memory %f MB' % (get_mem() - mem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但如果不去掉同步，同样会首先把数据全部生成好，导致占用大量内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, time 0.000004 sec\n",
      "batch 10, time 0.015873 sec\n",
      "batch 20, time 0.030742 sec\n",
      "batch 30, time 0.045688 sec\n",
      "batch 40, time 0.061229 sec\n",
      "batch 50, time 0.076878 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increased memory 94.084000 MB\n"
     ]
    }
   ],
   "source": [
    "mem = get_mem()\n",
    "\n",
    "total_loss = 0\n",
    "for x, y in get_data():\n",
    "    with autograd.record():\n",
    "        L = loss(y, net(x))\n",
    "    L.backward()\n",
    "    trainer.step(x.shape[0])\n",
    "\n",
    "nd.waitall()\n",
    "print('Increased memory %f MB' % (get_mem() - mem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 延后执行使得系统有更多空间来做性能优化。但我们推荐每个批量里至少有一个同步函数，例如对损失函数进行评估，来避免将过多任务同时丢进后端系统。\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 为什么同步版本的训练中，我们看到了内存使用的大量下降？\n",
    "\n",
    "## 扫码直达[讨论区](https://discuss.gluon.ai/t/topic/1881)\n",
    "\n",
    "![](../img/qr_lazy-evaluation.svg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}